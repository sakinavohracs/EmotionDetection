{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EmotionRecognition.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "11DW3jRffS14Vp_PLRmKup7Qd-3EJVGHw",
      "authorship_tag": "ABX9TyOuh8BODA+9hAHXlV2ywNn+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakinavohracs/EmotionDetection/blob/master/EmotionRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmb9JVKjR8JQ",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive Folder from Github"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OhUQHGQa-yqy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "94fdee2c-a59d-43f6-9a69-578f7420fd9f"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/your_project_folder/' \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WUoCAxybBEtE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4eba8f70-41aa-4367-87b5-73249c02393c"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/conv-emotion-master/TL-ERC/\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sueyc1pOCvgk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "870f4f14-7e95-49a9-afab-91879cef3a35"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_model\t\t   environment.yml     iemocap_preprocess.py\n",
            "dailydialog_preprocess.py  generative_weights  setup.py\n",
            "datasets\t\t   glove.txt\t       utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBcOoXwSDPex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATASET_DIRECTORY_PATH = \"./datasets/\"\n",
        "GENERATIVE_WEIGHTS_DIRECTORY_PATH = \"./generative_weights/\"\n",
        "\n",
        "if not os.path.exists(DATASET_DIRECTORY_PATH):\n",
        "    os.makedirs(DATASET_DIRECTORY_PATH)\n",
        "\n",
        "\n",
        "if not os.path.exists(GENERATIVE_WEIGHTS_DIRECTORY_PATH):\n",
        "    os.makedirs(GENERATIVE_WEIGHTS_DIRECTORY_PATH)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXrU6j13DS2l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fdfac234-2b17-4b26-dcad-796e2f9f409e"
      },
      "source": [
        "%cd bert_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/bert_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UxXT60BDfxx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "36289bfa-0573-44f0-ed16-c2d749e0681e"
      },
      "source": [
        "!python train.py --load_checkpoint=../generative_weights/cornell_weights.pkl --data=dailydialog"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dailydialog\n",
            "dailydialog\n",
            "dailydialog\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/conversation_length.pkl'),\n",
            " 'data': 'dailydialog',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 7,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/word_emb.pkl')}\n",
            "100% 231508/231508 [00:00<00:00, 319644.23B/s]\n",
            "Build Graph\n",
            "100% 407873900/407873900 [00:29<00:00, 13822616.48B/s]\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [7, 256]\n",
            "\tdecoder2output.linears.0.bias\t [7]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 4.2e+01 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.9266706705093384\n",
            "Epoch: 1, iter 100: loss = 0.6967078447341919\n",
            "Epoch: 1, iter 200: loss = 0.5852368474006653\n",
            "Epoch: 1, iter 300: loss = 1.4945261478424072\n",
            "Epoch: 1, iter 400: loss = 0.39390507340431213\n",
            "Epoch: 1, iter 500: loss = 0.43595147132873535\n",
            "Epoch: 1, iter 600: loss = 0.12806425988674164\n",
            "Epoch: 1, iter 700: loss = 0.676632821559906\n",
            "Epoch: 1, iter 800: loss = 0.08969517797231674\n",
            "Epoch: 1, iter 900: loss = 0.230367511510849\n",
            "Epoch: 1, iter 1000: loss = 0.14135876297950745\n",
            "Epoch: 1, iter 1100: loss = 0.20804201066493988\n",
            "Epoch: 1, iter 1200: loss = 1.5313172340393066\n",
            "Epoch: 1, iter 1300: loss = 0.013807732611894608\n",
            "Epoch: 1, iter 1400: loss = 0.03593487665057182\n",
            "Epoch: 1, iter 1500: loss = 0.1719484180212021\n",
            "Epoch: 1, iter 1600: loss = 0.5616558790206909\n",
            "Epoch: 1, iter 1700: loss = 0.9286719560623169\n",
            "Epoch: 1, iter 1800: loss = 0.0226606335490942\n",
            "Epoch: 1, iter 1900: loss = 0.26568084955215454\n",
            "Epoch: 1, iter 2000: loss = 1.1060802936553955\n",
            "Epoch: 1, iter 2100: loss = 0.04505009576678276\n",
            "Epoch: 1, iter 2200: loss = 0.5759457945823669\n",
            "Epoch: 1, iter 2300: loss = 0.2693024277687073\n",
            "Epoch: 1, iter 2400: loss = 0.2874022126197815\n",
            "Epoch: 1, iter 2500: loss = 0.1350185126066208\n",
            "Epoch: 1, iter 2600: loss = 1.078153133392334\n",
            "Epoch: 1, iter 2700: loss = 0.28278663754463196\n",
            "Epoch: 1, iter 2800: loss = 0.3386690318584442\n",
            "Epoch: 1, iter 2900: loss = 0.9975738525390625\n",
            "Epoch: 1, iter 3000: loss = 0.545141339302063\n",
            "Epoch: 1, iter 3100: loss = 1.6925212144851685\n",
            "Epoch: 1, iter 3200: loss = 0.20566020905971527\n",
            "Epoch: 1, iter 3300: loss = 0.04713226109743118\n",
            "Epoch: 1, iter 3400: loss = 1.0478748083114624\n",
            "Epoch: 1, iter 3500: loss = 0.4673961102962494\n",
            "Epoch: 1, iter 3600: loss = 0.3025790750980377\n",
            "Epoch: 1, iter 3700: loss = 0.5459728240966797\n",
            "Epoch: 1, iter 3800: loss = 1.9628281593322754\n",
            "Epoch: 1, iter 3900: loss = 1.7366760969161987\n",
            "Epoch: 1, iter 4000: loss = 0.12003481388092041\n",
            "Epoch: 1, iter 4100: loss = 0.018425019457936287\n",
            "Epoch: 1, iter 4200: loss = 0.4187125861644745\n",
            "Epoch: 1, iter 4300: loss = 0.028562301769852638\n",
            "Epoch: 1, iter 4400: loss = 0.07147690653800964\n",
            "Epoch: 1, iter 4500: loss = 0.8658066987991333\n",
            "Epoch: 1, iter 4600: loss = 0.5393916964530945\n",
            "Epoch: 1, iter 4700: loss = 0.3556378483772278\n",
            "Epoch: 1, iter 4800: loss = 0.15214265882968903\n",
            "Epoch: 1, iter 4900: loss = 1.3638966083526611\n",
            "Epoch: 1, iter 5000: loss = 0.443551242351532\n",
            "Epoch: 1, iter 5100: loss = 0.6790952682495117\n",
            "Epoch: 1, iter 5200: loss = 2.107647657394409\n",
            "Epoch: 1, iter 5300: loss = 0.35228231549263\n",
            "Epoch: 1, iter 5400: loss = 0.06559062004089355\n",
            "Epoch: 1, iter 5500: loss = 0.17531391978263855\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:38<00:00,  5.92it/s]\n",
            "Epoch 1 loss average: 0.484\n",
            "train\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6336    0.3887    0.4818     11182\n",
            "           2     0.0000    0.0000    0.0000       969\n",
            "           3     0.5926    0.2719    0.3728      1600\n",
            "           4     0.6250    0.0060    0.0120       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.0000    0.0000    0.0000       303\n",
            "\n",
            "   micro avg     0.6292    0.3185    0.4229     15027\n",
            "   macro avg     0.3085    0.1111    0.1444     15027\n",
            "weighted avg     0.5690    0.3185    0.3989     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7247    0.3307    0.4542      1019\n",
            "           2     1.0000    0.0196    0.0385       102\n",
            "           3     0.5517    0.4138    0.4729       116\n",
            "           4     0.2143    0.0254    0.0455       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6866    0.2748    0.3926      1419\n",
            "   macro avg     0.4151    0.1316    0.1685      1419\n",
            "weighted avg     0.6552    0.2748    0.3714      1419\n",
            "\n",
            "0.4843913557695302 0.3988618814833294 0.44727601638734305 0.371353864532691\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 0.06095708906650543\n",
            "Epoch: 2, iter 100: loss = 0.06575604528188705\n",
            "Epoch: 2, iter 200: loss = 0.1917794644832611\n",
            "Epoch: 2, iter 300: loss = 0.7530670166015625\n",
            "Epoch: 2, iter 400: loss = 0.49690595269203186\n",
            "Epoch: 2, iter 500: loss = 0.011229799129068851\n",
            "Epoch: 2, iter 600: loss = 0.16723643243312836\n",
            "Epoch: 2, iter 700: loss = 0.4072531759738922\n",
            "Epoch: 2, iter 800: loss = 0.072844497859478\n",
            "Epoch: 2, iter 900: loss = 0.5991166830062866\n",
            "Epoch: 2, iter 1000: loss = 1.5042918920516968\n",
            "Epoch: 2, iter 1100: loss = 0.2829184830188751\n",
            "Epoch: 2, iter 1200: loss = 0.08949843794107437\n",
            "Epoch: 2, iter 1300: loss = 1.5628149509429932\n",
            "Epoch: 2, iter 1400: loss = 2.02268123626709\n",
            "Epoch: 2, iter 1500: loss = 0.012786919251084328\n",
            "Epoch: 2, iter 1600: loss = 0.8765574097633362\n",
            "Epoch: 2, iter 1700: loss = 0.007514787372201681\n",
            "Epoch: 2, iter 1800: loss = 0.5110598206520081\n",
            "Epoch: 2, iter 1900: loss = 0.08374691009521484\n",
            "Epoch: 2, iter 2000: loss = 0.3433307111263275\n",
            "Epoch: 2, iter 2100: loss = 0.26927217841148376\n",
            "Epoch: 2, iter 2200: loss = 0.7601827383041382\n",
            "Epoch: 2, iter 2300: loss = 0.04358157142996788\n",
            "Epoch: 2, iter 2400: loss = 0.9728591442108154\n",
            "Epoch: 2, iter 2500: loss = 0.047630757093429565\n",
            "Epoch: 2, iter 2600: loss = 0.45887207984924316\n",
            "Epoch: 2, iter 2700: loss = 0.010884132236242294\n",
            "Epoch: 2, iter 2800: loss = 1.310545563697815\n",
            "Epoch: 2, iter 2900: loss = 0.7957673668861389\n",
            "Epoch: 2, iter 3000: loss = 0.4234425127506256\n",
            "Epoch: 2, iter 3100: loss = 0.4116290807723999\n",
            "Epoch: 2, iter 3200: loss = 0.7569011449813843\n",
            "Epoch: 2, iter 3300: loss = 0.45663678646087646\n",
            "Epoch: 2, iter 3400: loss = 0.5820203423500061\n",
            "Epoch: 2, iter 3500: loss = 0.040569428354501724\n",
            "Epoch: 2, iter 3600: loss = 0.14589053392410278\n",
            "Epoch: 2, iter 3700: loss = 0.018866047263145447\n",
            "Epoch: 2, iter 3800: loss = 0.6221022009849548\n",
            "Epoch: 2, iter 3900: loss = 0.08770962804555893\n",
            "Epoch: 2, iter 4000: loss = 0.07536782324314117\n",
            "Epoch: 2, iter 4100: loss = 0.08159288018941879\n",
            "Epoch: 2, iter 4200: loss = 0.023509394377470016\n",
            "Epoch: 2, iter 4300: loss = 0.2288016974925995\n",
            "Epoch: 2, iter 4400: loss = 0.7509083151817322\n",
            "Epoch: 2, iter 4500: loss = 0.07294229418039322\n",
            "Epoch: 2, iter 4600: loss = 0.12973663210868835\n",
            "Epoch: 2, iter 4700: loss = 0.24031361937522888\n",
            "Epoch: 2, iter 4800: loss = 0.8033797144889832\n",
            "Epoch: 2, iter 4900: loss = 0.24366596341133118\n",
            "Epoch: 2, iter 5000: loss = 1.0317834615707397\n",
            "Epoch: 2, iter 5100: loss = 0.07787997275590897\n",
            "Epoch: 2, iter 5200: loss = 0.40052834153175354\n",
            "Epoch: 2, iter 5300: loss = 0.0826651006937027\n",
            "Epoch: 2, iter 5400: loss = 0.3783033788204193\n",
            "Epoch: 2, iter 5500: loss = 0.06414718180894852\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 2 loss average: 0.405\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6936    0.5033    0.5833     11182\n",
            "           2     0.3548    0.0114    0.0220       969\n",
            "           3     0.6231    0.4525    0.5243      1600\n",
            "           4     0.4848    0.0387    0.0717       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.0000    0.0000    0.0000       303\n",
            "\n",
            "   micro avg     0.6823    0.4256    0.5242     15027\n",
            "   macro avg     0.3594    0.1676    0.2002     15027\n",
            "weighted avg     0.6320    0.4256    0.4953     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7036    0.4053    0.5143      1019\n",
            "           2     0.2105    0.0392    0.0661       102\n",
            "           3     0.4049    0.5690    0.4731       116\n",
            "           4     0.0000    0.0000    0.0000       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6265    0.3404    0.4411      1419\n",
            "   macro avg     0.2198    0.1689    0.1756      1419\n",
            "weighted avg     0.5535    0.3404    0.4128      1419\n",
            "\n",
            "0.40453936563355586 0.4952574475874082 0.45437393817039184 0.4127687962184173\n",
            "Patience counter: 1\n",
            "Epoch: 3, iter 0: loss = 0.5921329259872437\n",
            "Epoch: 3, iter 100: loss = 0.2224878966808319\n",
            "Epoch: 3, iter 200: loss = 0.03529345989227295\n",
            "Epoch: 3, iter 300: loss = 0.23407217860221863\n",
            "Epoch: 3, iter 400: loss = 0.7452691197395325\n",
            "Epoch: 3, iter 500: loss = 0.1462963968515396\n",
            "Epoch: 3, iter 600: loss = 0.19138295948505402\n",
            "Epoch: 3, iter 700: loss = 0.5724356770515442\n",
            "Epoch: 3, iter 800: loss = 0.19928309321403503\n",
            "Epoch: 3, iter 900: loss = 0.5695611238479614\n",
            "Epoch: 3, iter 1000: loss = 0.15061146020889282\n",
            "Epoch: 3, iter 1100: loss = 0.19547417759895325\n",
            "Epoch: 3, iter 1200: loss = 1.5868521928787231\n",
            "Epoch: 3, iter 1300: loss = 0.15168167650699615\n",
            "Epoch: 3, iter 1400: loss = 0.0831652358174324\n",
            "Epoch: 3, iter 1500: loss = 1.5425567626953125\n",
            "Epoch: 3, iter 1600: loss = 0.04487752914428711\n",
            "Epoch: 3, iter 1700: loss = 0.09113200008869171\n",
            "Epoch: 3, iter 1800: loss = 0.7341113090515137\n",
            "Epoch: 3, iter 1900: loss = 0.18598918616771698\n",
            "Epoch: 3, iter 2000: loss = 0.10620834678411484\n",
            "Epoch: 3, iter 2100: loss = 0.49017009139060974\n",
            "Epoch: 3, iter 2200: loss = 0.2961694300174713\n",
            "Epoch: 3, iter 2300: loss = 0.08168673515319824\n",
            "Epoch: 3, iter 2400: loss = 0.16619929671287537\n",
            "Epoch: 3, iter 2500: loss = 0.032164331525564194\n",
            "Epoch: 3, iter 2600: loss = 0.5630601644515991\n",
            "Epoch: 3, iter 2700: loss = 0.1080307811498642\n",
            "Epoch: 3, iter 2800: loss = 0.06947150826454163\n",
            "Epoch: 3, iter 2900: loss = 0.5881158709526062\n",
            "Epoch: 3, iter 3000: loss = 0.28466951847076416\n",
            "Epoch: 3, iter 3100: loss = 0.1293795257806778\n",
            "Epoch: 3, iter 3200: loss = 0.07746572047472\n",
            "Epoch: 3, iter 3300: loss = 0.0254923515021801\n",
            "Epoch: 3, iter 3400: loss = 0.4009382426738739\n",
            "Epoch: 3, iter 3500: loss = 0.0037480450700968504\n",
            "Epoch: 3, iter 3600: loss = 0.0979766845703125\n",
            "Epoch: 3, iter 3700: loss = 0.29591670632362366\n",
            "Epoch: 3, iter 3800: loss = 1.0474635362625122\n",
            "Epoch: 3, iter 3900: loss = 0.22626659274101257\n",
            "Epoch: 3, iter 4000: loss = 0.09335941076278687\n",
            "Epoch: 3, iter 4100: loss = 0.37275415658950806\n",
            "Epoch: 3, iter 4200: loss = 0.06732255220413208\n",
            "Epoch: 3, iter 4300: loss = 0.3440995514392853\n",
            "Epoch: 3, iter 4400: loss = 0.6420554518699646\n",
            "Epoch: 3, iter 4500: loss = 0.0320691280066967\n",
            "Epoch: 3, iter 4600: loss = 1.9621002674102783\n",
            "Epoch: 3, iter 4700: loss = 0.6391022205352783\n",
            "Epoch: 3, iter 4800: loss = 0.6245877742767334\n",
            "Epoch: 3, iter 4900: loss = 0.43272528052330017\n",
            "Epoch: 3, iter 5000: loss = 0.21871699392795563\n",
            "Epoch: 3, iter 5100: loss = 0.30048173666000366\n",
            "Epoch: 3, iter 5200: loss = 0.3077458143234253\n",
            "Epoch: 3, iter 5300: loss = 0.03347017616033554\n",
            "Epoch: 3, iter 5400: loss = 0.2275860756635666\n",
            "Epoch: 3, iter 5500: loss = 0.007527682464569807\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 3 loss average: 0.362\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7207    0.5619    0.6315     11182\n",
            "           2     0.3667    0.0454    0.0808       969\n",
            "           3     0.6661    0.5200    0.5841      1600\n",
            "           4     0.4597    0.1657    0.2436       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2381    0.0165    0.0309       303\n",
            "\n",
            "   micro avg     0.7016    0.4859    0.5741     15027\n",
            "   macro avg     0.4086    0.2182    0.2618     15027\n",
            "weighted avg     0.6610    0.4859    0.5513     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7114    0.3749    0.4910      1019\n",
            "           2     0.2857    0.0392    0.0690       102\n",
            "           3     0.5158    0.4224    0.4645       116\n",
            "           4     0.4286    0.1017    0.1644       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6622    0.3150    0.4269      1419\n",
            "   macro avg     0.3236    0.1564    0.1981      1419\n",
            "weighted avg     0.6092    0.3150    0.4092      1419\n",
            "\n",
            "0.3618059187022965 0.5513099113586932 0.41841884134839324 0.40918966865472395\n",
            "Patience counter: 2\n",
            "Epoch: 4, iter 0: loss = 0.11660878360271454\n",
            "Epoch: 4, iter 100: loss = 0.2545831501483917\n",
            "Epoch: 4, iter 200: loss = 0.25080442428588867\n",
            "Epoch: 4, iter 300: loss = 0.2636833190917969\n",
            "Epoch: 4, iter 400: loss = 0.20704792439937592\n",
            "Epoch: 4, iter 500: loss = 0.12271054089069366\n",
            "Epoch: 4, iter 600: loss = 1.236861228942871\n",
            "Epoch: 4, iter 700: loss = 1.3244361877441406\n",
            "Epoch: 4, iter 800: loss = 0.16552458703517914\n",
            "Epoch: 4, iter 900: loss = 0.03904375061392784\n",
            "Epoch: 4, iter 1000: loss = 0.04631996899843216\n",
            "Epoch: 4, iter 1100: loss = 0.33925917744636536\n",
            "Epoch: 4, iter 1200: loss = 0.030261430889368057\n",
            "Epoch: 4, iter 1300: loss = 0.1727035492658615\n",
            "Epoch: 4, iter 1400: loss = 0.6399121284484863\n",
            "Epoch: 4, iter 1500: loss = 0.41486501693725586\n",
            "Epoch: 4, iter 1600: loss = 0.007604329846799374\n",
            "Epoch: 4, iter 1700: loss = 0.6174777746200562\n",
            "Epoch: 4, iter 1800: loss = 0.004416173789650202\n",
            "Epoch: 4, iter 1900: loss = 0.03966271132230759\n",
            "Epoch: 4, iter 2000: loss = 0.013860492035746574\n",
            "Epoch: 4, iter 2100: loss = 0.28862807154655457\n",
            "Epoch: 4, iter 2200: loss = 0.031089356169104576\n",
            "Epoch: 4, iter 2300: loss = 0.47220757603645325\n",
            "Epoch: 4, iter 2400: loss = 0.05305587500333786\n",
            "Epoch: 4, iter 2500: loss = 0.11232833564281464\n",
            "Epoch: 4, iter 2600: loss = 0.6096265912055969\n",
            "Epoch: 4, iter 2700: loss = 0.03582346439361572\n",
            "Epoch: 4, iter 2800: loss = 0.002853283192962408\n",
            "Epoch: 4, iter 2900: loss = 0.1137639656662941\n",
            "Epoch: 4, iter 3000: loss = 0.8712285161018372\n",
            "Epoch: 4, iter 3100: loss = 1.1065679788589478\n",
            "Epoch: 4, iter 3200: loss = 0.6968399286270142\n",
            "Epoch: 4, iter 3300: loss = 0.2247108519077301\n",
            "Epoch: 4, iter 3400: loss = 0.6829974055290222\n",
            "Epoch: 4, iter 3500: loss = 0.006690968759357929\n",
            "Epoch: 4, iter 3600: loss = 0.02301831915974617\n",
            "Epoch: 4, iter 3700: loss = 0.5116594433784485\n",
            "Epoch: 4, iter 3800: loss = 0.4670092761516571\n",
            "Epoch: 4, iter 3900: loss = 0.004668094217777252\n",
            "Epoch: 4, iter 4000: loss = 0.7367105484008789\n",
            "Epoch: 4, iter 4100: loss = 0.2879369556903839\n",
            "Epoch: 4, iter 4200: loss = 1.4025980234146118\n",
            "Epoch: 4, iter 4300: loss = 0.2992931306362152\n",
            "Epoch: 4, iter 4400: loss = 0.2211361527442932\n",
            "Epoch: 4, iter 4500: loss = 0.2927193343639374\n",
            "Epoch: 4, iter 4600: loss = 0.4605848491191864\n",
            "Epoch: 4, iter 4700: loss = 0.0691339448094368\n",
            "Epoch: 4, iter 4800: loss = 0.31487295031547546\n",
            "Epoch: 4, iter 4900: loss = 0.03756067529320717\n",
            "Epoch: 4, iter 5000: loss = 0.00805908814072609\n",
            "Epoch: 4, iter 5100: loss = 1.1720283031463623\n",
            "Epoch: 4, iter 5200: loss = 0.08872638642787933\n",
            "Epoch: 4, iter 5300: loss = 1.2450822591781616\n",
            "Epoch: 4, iter 5400: loss = 0.19591356813907623\n",
            "Epoch: 4, iter 5500: loss = 0.1159299984574318\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 4 loss average: 0.327\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7605    0.6257    0.6866     11182\n",
            "           2     0.4689    0.1476    0.2245       969\n",
            "           3     0.6774    0.5513    0.6079      1600\n",
            "           4     0.4954    0.2588    0.3400       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2794    0.0627    0.1024       303\n",
            "\n",
            "   micro avg     0.7301    0.5493    0.6269     15027\n",
            "   macro avg     0.4469    0.2743    0.3269     15027\n",
            "weighted avg     0.7012    0.5493    0.6109     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5858    0.5829    0.5844      1019\n",
            "           2     0.4091    0.1765    0.2466       102\n",
            "           3     0.4954    0.4655    0.4800       116\n",
            "           4     0.3810    0.3390    0.3587       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.5000    0.0213    0.0408        47\n",
            "\n",
            "   micro avg     0.5549    0.4982    0.5251      1419\n",
            "   macro avg     0.3952    0.2642    0.2851      1419\n",
            "weighted avg     0.5388    0.4982    0.5078      1419\n",
            "\n",
            "0.32669814010141934 0.610879337796154 0.5282372388285315 0.507781385533611\n",
            "Patience counter: 3\n",
            "Epoch: 5, iter 0: loss = 0.030205274000763893\n",
            "Epoch: 5, iter 100: loss = 0.12623290717601776\n",
            "Epoch: 5, iter 200: loss = 0.0014847632264718413\n",
            "Epoch: 5, iter 300: loss = 0.08361128717660904\n",
            "Epoch: 5, iter 400: loss = 0.08845775574445724\n",
            "Epoch: 5, iter 500: loss = 0.022788923233747482\n",
            "Epoch: 5, iter 600: loss = 0.32370877265930176\n",
            "Epoch: 5, iter 700: loss = 0.13496628403663635\n",
            "Epoch: 5, iter 800: loss = 0.5027210116386414\n",
            "Epoch: 5, iter 900: loss = 0.26787784695625305\n",
            "Epoch: 5, iter 1000: loss = 0.12553411722183228\n",
            "Epoch: 5, iter 1100: loss = 0.2633090317249298\n",
            "Epoch: 5, iter 1200: loss = 0.20014658570289612\n",
            "Epoch: 5, iter 1300: loss = 0.10709398984909058\n",
            "Epoch: 5, iter 1400: loss = 0.16526801884174347\n",
            "Epoch: 5, iter 1500: loss = 0.28591492772102356\n",
            "Epoch: 5, iter 1600: loss = 0.46485742926597595\n",
            "Epoch: 5, iter 1700: loss = 0.3743516206741333\n",
            "Epoch: 5, iter 1800: loss = 0.09675358980894089\n",
            "Epoch: 5, iter 1900: loss = 0.06415940076112747\n",
            "Epoch: 5, iter 2000: loss = 0.5816877484321594\n",
            "Epoch: 5, iter 2100: loss = 1.0067538022994995\n",
            "Epoch: 5, iter 2200: loss = 1.1412276029586792\n",
            "Epoch: 5, iter 2300: loss = 0.017180684953927994\n",
            "Epoch: 5, iter 2400: loss = 0.4777716398239136\n",
            "Epoch: 5, iter 2500: loss = 0.021526921540498734\n",
            "Epoch: 5, iter 2600: loss = 0.5545480847358704\n",
            "Epoch: 5, iter 2700: loss = 1.0891603231430054\n",
            "Epoch: 5, iter 2800: loss = 0.3068819046020508\n",
            "Epoch: 5, iter 2900: loss = 0.1132969856262207\n",
            "Epoch: 5, iter 3000: loss = 0.2521255612373352\n",
            "Epoch: 5, iter 3100: loss = 0.19467619061470032\n",
            "Epoch: 5, iter 3200: loss = 0.011955284513533115\n",
            "Epoch: 5, iter 3300: loss = 1.1422723531723022\n",
            "Epoch: 5, iter 3400: loss = 0.24659891426563263\n",
            "Epoch: 5, iter 3500: loss = 0.5102021098136902\n",
            "Epoch: 5, iter 3600: loss = 0.29504895210266113\n",
            "Epoch: 5, iter 3700: loss = 0.08094199001789093\n",
            "Epoch: 5, iter 3800: loss = 0.21980498731136322\n",
            "Epoch: 5, iter 3900: loss = 0.037090182304382324\n",
            "Epoch: 5, iter 4000: loss = 0.013608884997665882\n",
            "Epoch: 5, iter 4100: loss = 0.1753290295600891\n",
            "Epoch: 5, iter 4200: loss = 0.04739757627248764\n",
            "Epoch: 5, iter 4300: loss = 0.047007154673337936\n",
            "Epoch: 5, iter 4400: loss = 0.638480544090271\n",
            "Epoch: 5, iter 4500: loss = 0.0029277943540364504\n",
            "Epoch: 5, iter 4600: loss = 0.4368392527103424\n",
            "Epoch: 5, iter 4700: loss = 0.008008304052054882\n",
            "Epoch: 5, iter 4800: loss = 0.030034348368644714\n",
            "Epoch: 5, iter 4900: loss = 0.6663331985473633\n",
            "Epoch: 5, iter 5000: loss = 1.567396879196167\n",
            "Epoch: 5, iter 5100: loss = 0.22156913578510284\n",
            "Epoch: 5, iter 5200: loss = 0.7367452383041382\n",
            "Epoch: 5, iter 5300: loss = 0.47570228576660156\n",
            "Epoch: 5, iter 5400: loss = 0.3643794059753418\n",
            "Epoch: 5, iter 5500: loss = 0.11416619271039963\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 5 loss average: 0.300\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7796    0.6698    0.7206     11182\n",
            "           2     0.5443    0.2724    0.3631       969\n",
            "           3     0.7047    0.6325    0.6667      1600\n",
            "           4     0.5147    0.3386    0.4085       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3274    0.1221    0.1779       303\n",
            "\n",
            "   micro avg     0.7454    0.6044    0.6676     15027\n",
            "   macro avg     0.4785    0.3392    0.3895     15027\n",
            "weighted avg     0.7252    0.6044    0.6567     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5992    0.5545    0.5759      1019\n",
            "           2     0.1858    0.3333    0.2386       102\n",
            "           3     0.4911    0.4741    0.4825       116\n",
            "           4     0.4222    0.1610    0.2331       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3182    0.1489    0.2029        47\n",
            "\n",
            "   micro avg     0.5211    0.4792    0.4993      1419\n",
            "   macro avg     0.3361    0.2786    0.2888      1419\n",
            "weighted avg     0.5294    0.4792    0.4963      1419\n",
            "\n",
            "0.30023203810888777 0.6566642327082688 0.5235056329383679 0.4962882448970571\n",
            "Patience counter: 4\n",
            "Epoch: 6, iter 0: loss = 0.015897056087851524\n",
            "Epoch: 6, iter 100: loss = 0.698582112789154\n",
            "Epoch: 6, iter 200: loss = 0.0553160235285759\n",
            "Epoch: 6, iter 300: loss = 0.0014886570861563087\n",
            "Epoch: 6, iter 400: loss = 0.025220507755875587\n",
            "Epoch: 6, iter 500: loss = 0.4402615427970886\n",
            "Epoch: 6, iter 600: loss = 0.19488224387168884\n",
            "Epoch: 6, iter 700: loss = 0.00785504188388586\n",
            "Epoch: 6, iter 800: loss = 0.017759090289473534\n",
            "Epoch: 6, iter 900: loss = 0.04684155806899071\n",
            "Epoch: 6, iter 1000: loss = 0.005452562123537064\n",
            "Epoch: 6, iter 1100: loss = 0.005042072385549545\n",
            "Epoch: 6, iter 1200: loss = 0.5107566714286804\n",
            "Epoch: 6, iter 1300: loss = 0.021441787481307983\n",
            "Epoch: 6, iter 1400: loss = 0.1513231098651886\n",
            "Epoch: 6, iter 1500: loss = 0.36674100160598755\n",
            "Epoch: 6, iter 1600: loss = 0.2476235181093216\n",
            "Epoch: 6, iter 1700: loss = 0.16274602711200714\n",
            "Epoch: 6, iter 1800: loss = 0.18298321962356567\n",
            "Epoch: 6, iter 1900: loss = 0.0016466480446979403\n",
            "Epoch: 6, iter 2000: loss = 0.8370784521102905\n",
            "Epoch: 6, iter 2100: loss = 0.9416057467460632\n",
            "Epoch: 6, iter 2200: loss = 0.5195108652114868\n",
            "Epoch: 6, iter 2300: loss = 0.13648159801959991\n",
            "Epoch: 6, iter 2400: loss = 0.4473700523376465\n",
            "Epoch: 6, iter 2500: loss = 0.00464226771146059\n",
            "Epoch: 6, iter 2600: loss = 0.13377176225185394\n",
            "Epoch: 6, iter 2700: loss = 0.3292127251625061\n",
            "Epoch: 6, iter 2800: loss = 0.2165655493736267\n",
            "Epoch: 6, iter 2900: loss = 0.2272549271583557\n",
            "Epoch: 6, iter 3000: loss = 0.08129251003265381\n",
            "Epoch: 6, iter 3100: loss = 0.017147019505500793\n",
            "Epoch: 6, iter 3200: loss = 1.1510190963745117\n",
            "Epoch: 6, iter 3300: loss = 0.005231897812336683\n",
            "Epoch: 6, iter 3400: loss = 0.48288679122924805\n",
            "Epoch: 6, iter 3500: loss = 0.25482645630836487\n",
            "Epoch: 6, iter 3600: loss = 0.5883435606956482\n",
            "Epoch: 6, iter 3700: loss = 0.134112149477005\n",
            "Epoch: 6, iter 3800: loss = 0.006128879729658365\n",
            "Epoch: 6, iter 3900: loss = 0.035552214831113815\n",
            "Epoch: 6, iter 4000: loss = 0.0010255236411467195\n",
            "Epoch: 6, iter 4100: loss = 0.3035126328468323\n",
            "Epoch: 6, iter 4200: loss = 0.2497827112674713\n",
            "Epoch: 6, iter 4300: loss = 0.002677496522665024\n",
            "Epoch: 6, iter 4400: loss = 0.2811545729637146\n",
            "Epoch: 6, iter 4500: loss = 0.0074247317388653755\n",
            "Epoch: 6, iter 4600: loss = 0.1747310906648636\n",
            "Epoch: 6, iter 4700: loss = 0.9258560538291931\n",
            "Epoch: 6, iter 4800: loss = 1.2943875789642334\n",
            "Epoch: 6, iter 4900: loss = 0.08740951120853424\n",
            "Epoch: 6, iter 5000: loss = 0.026272064074873924\n",
            "Epoch: 6, iter 5100: loss = 0.008243482559919357\n",
            "Epoch: 6, iter 5200: loss = 1.855526328086853\n",
            "Epoch: 6, iter 5300: loss = 0.5376327037811279\n",
            "Epoch: 6, iter 5400: loss = 0.022208301350474358\n",
            "Epoch: 6, iter 5500: loss = 0.001099908258765936\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:40<00:00,  5.91it/s]\n",
            "Epoch 6 loss average: 0.275\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7979    0.7070    0.7497     11182\n",
            "           2     0.5911    0.3416    0.4330       969\n",
            "           3     0.7258    0.6469    0.6841      1600\n",
            "           4     0.5686    0.4111    0.4772       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3667    0.1452    0.2080       303\n",
            "\n",
            "   micro avg     0.7656    0.6426    0.6987     15027\n",
            "   macro avg     0.5083    0.3753    0.4253     15027\n",
            "weighted avg     0.7478    0.6426    0.6891     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6855    0.4279    0.5269      1019\n",
            "           2     0.3864    0.1667    0.2329       102\n",
            "           3     0.5301    0.3793    0.4422       116\n",
            "           4     0.3729    0.1864    0.2486       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.2045    0.1915    0.1978        47\n",
            "\n",
            "   micro avg     0.6097    0.3721    0.4621      1419\n",
            "   macro avg     0.3632    0.2253    0.2747      1419\n",
            "weighted avg     0.6012    0.3721    0.4585      1419\n",
            "\n",
            "0.27538259703079854 0.6890873445710853 0.49814510128744643 0.4584771228718959\n",
            "Patience counter: 5\n",
            "Epoch: 7, iter 0: loss = 0.7344706654548645\n",
            "Epoch: 7, iter 100: loss = 0.5424622297286987\n",
            "Epoch: 7, iter 200: loss = 0.18640325963497162\n",
            "Epoch: 7, iter 300: loss = 0.20425517857074738\n",
            "Epoch: 7, iter 400: loss = 0.004558548331260681\n",
            "Epoch: 7, iter 500: loss = 0.42425987124443054\n",
            "Epoch: 7, iter 600: loss = 0.5557657480239868\n",
            "Epoch: 7, iter 700: loss = 0.22880659997463226\n",
            "Epoch: 7, iter 800: loss = 0.1657858043909073\n",
            "Epoch: 7, iter 900: loss = 0.09096700698137283\n",
            "Epoch: 7, iter 1000: loss = 0.2831864655017853\n",
            "Epoch: 7, iter 1100: loss = 0.004890898242592812\n",
            "Epoch: 7, iter 1200: loss = 0.14739614725112915\n",
            "Epoch: 7, iter 1300: loss = 0.0011501249391585588\n",
            "Epoch: 7, iter 1400: loss = 0.5440335869789124\n",
            "Epoch: 7, iter 1500: loss = 0.01585547812283039\n",
            "Epoch: 7, iter 1600: loss = 0.21171046793460846\n",
            "Epoch: 7, iter 1700: loss = 0.11806675791740417\n",
            "Epoch: 7, iter 1800: loss = 0.24975106120109558\n",
            "Epoch: 7, iter 1900: loss = 0.20417822897434235\n",
            "Epoch: 7, iter 2000: loss = 0.33765730261802673\n",
            "Epoch: 7, iter 2100: loss = 0.3270217776298523\n",
            "Epoch: 7, iter 2200: loss = 0.1595647633075714\n",
            "Epoch: 7, iter 2300: loss = 0.0015149378450587392\n",
            "Epoch: 7, iter 2400: loss = 0.4039042294025421\n",
            "Epoch: 7, iter 2500: loss = 0.13814915716648102\n",
            "Epoch: 7, iter 2600: loss = 0.18540580570697784\n",
            "Epoch: 7, iter 2700: loss = 0.681376039981842\n",
            "Epoch: 7, iter 2800: loss = 0.021207187324762344\n",
            "Epoch: 7, iter 2900: loss = 0.46965473890304565\n",
            "Epoch: 7, iter 3000: loss = 0.3675568699836731\n",
            "Epoch: 7, iter 3100: loss = 0.32330524921417236\n",
            "Epoch: 7, iter 3200: loss = 0.13539040088653564\n",
            "Epoch: 7, iter 3300: loss = 0.054399289190769196\n",
            "Epoch: 7, iter 3400: loss = 0.001550588058307767\n",
            "Epoch: 7, iter 3500: loss = 0.003596743568778038\n",
            "Epoch: 7, iter 3600: loss = 0.001159211969934404\n",
            "Epoch: 7, iter 3700: loss = 0.28755590319633484\n",
            "Epoch: 7, iter 3800: loss = 0.06160793453454971\n",
            "Epoch: 7, iter 3900: loss = 0.10162289440631866\n",
            "Epoch: 7, iter 4000: loss = 0.02605891413986683\n",
            "Epoch: 7, iter 4100: loss = 0.17491818964481354\n",
            "Epoch: 7, iter 4200: loss = 0.8766512870788574\n",
            "Epoch: 7, iter 4300: loss = 0.002171922940760851\n",
            "Epoch: 7, iter 4400: loss = 0.6712291836738586\n",
            "Epoch: 7, iter 4500: loss = 0.2042028158903122\n",
            "Epoch: 7, iter 4600: loss = 0.05673152953386307\n",
            "Epoch: 7, iter 4700: loss = 0.053216684609651566\n",
            "Epoch: 7, iter 4800: loss = 0.10269234329462051\n",
            "Epoch: 7, iter 4900: loss = 0.02137119323015213\n",
            "Epoch: 7, iter 5000: loss = 0.05357637256383896\n",
            "Epoch: 7, iter 5100: loss = 0.36257222294807434\n",
            "Epoch: 7, iter 5200: loss = 0.8914849758148193\n",
            "Epoch: 7, iter 5300: loss = 1.080216646194458\n",
            "Epoch: 7, iter 5400: loss = 0.3614766299724579\n",
            "Epoch: 7, iter 5500: loss = 0.15593723952770233\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:39<00:00,  5.92it/s]\n",
            "Epoch 7 loss average: 0.258\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8174    0.7367    0.7750     11182\n",
            "           2     0.6533    0.4025    0.4981       969\n",
            "           3     0.7306    0.6544    0.6904      1600\n",
            "           4     0.5993    0.4450    0.5108       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3977    0.2310    0.2923       303\n",
            "\n",
            "   micro avg     0.7841    0.6730    0.7243     15027\n",
            "   macro avg     0.5331    0.4116    0.4611     15027\n",
            "weighted avg     0.7692    0.6730    0.7163     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6178    0.5299    0.5705      1019\n",
            "           2     0.2927    0.2353    0.2609       102\n",
            "           3     0.5435    0.4310    0.4808       116\n",
            "           4     0.4203    0.2458    0.3102       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3333    0.2766    0.3023        47\n",
            "\n",
            "   micro avg     0.5675    0.4623    0.5095      1419\n",
            "   macro avg     0.3679    0.2864    0.3208      1419\n",
            "weighted avg     0.5551    0.4623    0.5036      1419\n",
            "\n",
            "0.25790061216882887 0.716312944442884 0.5343619465616594 0.50355819255434\n",
            "Patience counter: 6\n",
            "Epoch: 8, iter 0: loss = 0.012713455595076084\n",
            "Epoch: 8, iter 100: loss = 0.28619030117988586\n",
            "Epoch: 8, iter 200: loss = 0.43709635734558105\n",
            "Epoch: 8, iter 300: loss = 0.003134363330900669\n",
            "Epoch: 8, iter 400: loss = 0.1484614610671997\n",
            "Epoch: 8, iter 500: loss = 0.07642912119626999\n",
            "Epoch: 8, iter 600: loss = 0.02140762470662594\n",
            "Epoch: 8, iter 700: loss = 0.32370227575302124\n",
            "Epoch: 8, iter 800: loss = 0.24708914756774902\n",
            "Epoch: 8, iter 900: loss = 0.16460764408111572\n",
            "Epoch: 8, iter 1000: loss = 0.0020756081212311983\n",
            "Epoch: 8, iter 1100: loss = 0.04811910167336464\n",
            "Epoch: 8, iter 1200: loss = 0.006125607993453741\n",
            "Epoch: 8, iter 1300: loss = 0.003085759002715349\n",
            "Epoch: 8, iter 1400: loss = 0.03119722194969654\n",
            "Epoch: 8, iter 1500: loss = 0.2190627008676529\n",
            "Epoch: 8, iter 1600: loss = 0.011436414904892445\n",
            "Epoch: 8, iter 1700: loss = 0.023388350382447243\n",
            "Epoch: 8, iter 1800: loss = 0.7332493662834167\n",
            "Epoch: 8, iter 1900: loss = 0.12737351655960083\n",
            "Epoch: 8, iter 2000: loss = 0.0636049285531044\n",
            "Epoch: 8, iter 2100: loss = 0.09302899241447449\n",
            "Epoch: 8, iter 2200: loss = 1.5579900741577148\n",
            "Epoch: 8, iter 2300: loss = 0.015632696449756622\n",
            "Epoch: 8, iter 2400: loss = 0.3691200613975525\n",
            "Epoch: 8, iter 2500: loss = 0.009161410853266716\n",
            "Epoch: 8, iter 2600: loss = 0.12215776741504669\n",
            "Epoch: 8, iter 2700: loss = 0.026316285133361816\n",
            "Epoch: 8, iter 2800: loss = 0.5830484628677368\n",
            "Epoch: 8, iter 2900: loss = 0.13870492577552795\n",
            "Epoch: 8, iter 3000: loss = 0.33586618304252625\n",
            "Epoch: 8, iter 3100: loss = 0.5731016993522644\n",
            "Epoch: 8, iter 3200: loss = 0.0069105420261621475\n",
            "Epoch: 8, iter 3300: loss = 0.004333932884037495\n",
            "Epoch: 8, iter 3400: loss = 0.017364636063575745\n",
            "Epoch: 8, iter 3500: loss = 0.10826396197080612\n",
            "Epoch: 8, iter 3600: loss = 0.04795535281300545\n",
            "Epoch: 8, iter 3700: loss = 0.06971425563097\n",
            "Epoch: 8, iter 3800: loss = 1.3129152059555054\n",
            "Epoch: 8, iter 3900: loss = 0.8322956562042236\n",
            "Epoch: 8, iter 4000: loss = 0.12427278608083725\n",
            "Epoch: 8, iter 4100: loss = 0.25154218077659607\n",
            "Epoch: 8, iter 4200: loss = 0.08067695796489716\n",
            "Epoch: 8, iter 4300: loss = 0.008747507818043232\n",
            "Epoch: 8, iter 4400: loss = 0.156585231423378\n",
            "Epoch: 8, iter 4500: loss = 0.3966373801231384\n",
            "Epoch: 8, iter 4600: loss = 0.014406920410692692\n",
            "Epoch: 8, iter 4700: loss = 0.2820749282836914\n",
            "Epoch: 8, iter 4800: loss = 0.04791971296072006\n",
            "Epoch: 8, iter 4900: loss = 0.6370862722396851\n",
            "Epoch: 8, iter 5000: loss = 0.06579502671957016\n",
            "Epoch: 8, iter 5100: loss = 0.27133411169052124\n",
            "Epoch: 8, iter 5200: loss = 0.03803268074989319\n",
            "Epoch: 8, iter 5300: loss = 0.042422834783792496\n",
            "Epoch: 8, iter 5400: loss = 0.3774951696395874\n",
            "Epoch: 8, iter 5500: loss = 0.0038579616229981184\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 8 loss average: 0.245\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8300    0.7517    0.7889     11182\n",
            "           2     0.7059    0.4582    0.5557       969\n",
            "           3     0.7519    0.6763    0.7121      1600\n",
            "           4     0.6346    0.5103    0.5657       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.4845    0.3102    0.3783       303\n",
            "\n",
            "   micro avg     0.8000    0.6952    0.7440     15027\n",
            "   macro avg     0.5678    0.4511    0.5001     15027\n",
            "weighted avg     0.7879    0.6952    0.7375     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6754    0.4573    0.5453      1019\n",
            "           2     0.2174    0.0490    0.0800       102\n",
            "           3     0.5256    0.3534    0.4227       116\n",
            "           4     0.4211    0.2712    0.3299       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3200    0.1702    0.2222        47\n",
            "\n",
            "   micro avg     0.6188    0.3890    0.4777      1419\n",
            "   macro avg     0.3599    0.2169    0.2667      1419\n",
            "weighted avg     0.5892    0.3890    0.4667      1419\n",
            "\n",
            "0.24452220630734492 0.7374571400708612 0.48224129440201796 0.46671809663665564\n",
            "Patience counter: 7\n",
            "Epoch: 9, iter 0: loss = 0.13454830646514893\n",
            "Epoch: 9, iter 100: loss = 0.24309861660003662\n",
            "Epoch: 9, iter 200: loss = 0.34569209814071655\n",
            "Epoch: 9, iter 300: loss = 0.044653553515672684\n",
            "Epoch: 9, iter 400: loss = 0.18282856047153473\n",
            "Epoch: 9, iter 500: loss = 0.0022064463701099157\n",
            "Epoch: 9, iter 600: loss = 0.00046970960102044046\n",
            "Epoch: 9, iter 700: loss = 0.9024180769920349\n",
            "Epoch: 9, iter 800: loss = 0.25340381264686584\n",
            "Epoch: 9, iter 900: loss = 0.000573131546843797\n",
            "Epoch: 9, iter 1000: loss = 0.01761285401880741\n",
            "Epoch: 9, iter 1100: loss = 0.06955929845571518\n",
            "Epoch: 9, iter 1200: loss = 0.20078662037849426\n",
            "Epoch: 9, iter 1300: loss = 0.7013208270072937\n",
            "Epoch: 9, iter 1400: loss = 0.13397692143917084\n",
            "Epoch: 9, iter 1500: loss = 0.01412319764494896\n",
            "Epoch: 9, iter 1600: loss = 0.029981620609760284\n",
            "Epoch: 9, iter 1700: loss = 0.0008111156639643013\n",
            "Epoch: 9, iter 1800: loss = 0.7557587623596191\n",
            "Epoch: 9, iter 1900: loss = 0.9093378782272339\n",
            "Epoch: 9, iter 2000: loss = 0.006555238738656044\n",
            "Epoch: 9, iter 2100: loss = 0.0028791872318834066\n",
            "Epoch: 9, iter 2200: loss = 0.1503165066242218\n",
            "Epoch: 9, iter 2300: loss = 0.05233585089445114\n",
            "Epoch: 9, iter 2400: loss = 1.4712189435958862\n",
            "Epoch: 9, iter 2500: loss = 0.2487754076719284\n",
            "Epoch: 9, iter 2600: loss = 0.007407060358673334\n",
            "Epoch: 9, iter 2700: loss = 0.001310769934207201\n",
            "Epoch: 9, iter 2800: loss = 0.020406855270266533\n",
            "Epoch: 9, iter 2900: loss = 0.011640147306025028\n",
            "Epoch: 9, iter 3000: loss = 0.2862510085105896\n",
            "Epoch: 9, iter 3100: loss = 0.09463227540254593\n",
            "Epoch: 9, iter 3200: loss = 0.032295111566782\n",
            "Epoch: 9, iter 3300: loss = 0.05115053802728653\n",
            "Epoch: 9, iter 3400: loss = 0.3040575385093689\n",
            "Epoch: 9, iter 3500: loss = 0.0053451210260391235\n",
            "Epoch: 9, iter 3600: loss = 0.15562380850315094\n",
            "Epoch: 9, iter 3700: loss = 0.0034857429563999176\n",
            "Epoch: 9, iter 3800: loss = 0.024986635893583298\n",
            "Epoch: 9, iter 3900: loss = 0.07699878513813019\n",
            "Epoch: 9, iter 4000: loss = 0.509011447429657\n",
            "Epoch: 9, iter 4100: loss = 0.039311930537223816\n",
            "Epoch: 9, iter 4200: loss = 0.0008600963046774268\n",
            "Epoch: 9, iter 4300: loss = 0.44803258776664734\n",
            "Epoch: 9, iter 4400: loss = 0.0020912615582346916\n",
            "Epoch: 9, iter 4500: loss = 0.01975269801914692\n",
            "Epoch: 9, iter 4600: loss = 0.1273595690727234\n",
            "Epoch: 9, iter 4700: loss = 0.4333082437515259\n",
            "Epoch: 9, iter 4800: loss = 0.04029112309217453\n",
            "Epoch: 9, iter 4900: loss = 0.31749311089515686\n",
            "Epoch: 9, iter 5000: loss = 0.028751671314239502\n",
            "Epoch: 9, iter 5100: loss = 0.1768304854631424\n",
            "Epoch: 9, iter 5200: loss = 0.008924437686800957\n",
            "Epoch: 9, iter 5300: loss = 0.10590847581624985\n",
            "Epoch: 9, iter 5400: loss = 0.4407098591327667\n",
            "Epoch: 9, iter 5500: loss = 0.06843288987874985\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 9 loss average: 0.231\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8401    0.7712    0.8042     11182\n",
            "           2     0.6963    0.5015    0.5831       969\n",
            "           3     0.7588    0.6844    0.7197      1600\n",
            "           4     0.6209    0.5248    0.5688       827\n",
            "           5     0.5000    0.0205    0.0395       146\n",
            "           6     0.5224    0.3465    0.4167       303\n",
            "\n",
            "   micro avg     0.8073    0.7152    0.7585     15027\n",
            "   macro avg     0.6564    0.4748    0.5220     15027\n",
            "weighted avg     0.8004    0.7152    0.7528     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6715    0.4514    0.5399      1019\n",
            "           2     0.3053    0.2843    0.2944       102\n",
            "           3     0.3950    0.4052    0.4000       116\n",
            "           4     0.2907    0.2119    0.2451       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.2000    0.0638    0.0968        47\n",
            "\n",
            "   micro avg     0.5634    0.3975    0.4661      1419\n",
            "   macro avg     0.3104    0.2361    0.2627      1419\n",
            "weighted avg     0.5673    0.3975    0.4652      1419\n",
            "\n",
            "0.23124855146199866 0.7527552044758684 0.5172242816324805 0.4651618969879093\n",
            "Patience counter: 8\n",
            "Epoch: 10, iter 0: loss = 0.13272960484027863\n",
            "Epoch: 10, iter 100: loss = 0.14708898961544037\n",
            "Epoch: 10, iter 200: loss = 0.011097296141088009\n",
            "Epoch: 10, iter 300: loss = 0.0002914750948548317\n",
            "Epoch: 10, iter 400: loss = 0.4001626968383789\n",
            "Epoch: 10, iter 500: loss = 0.11924774199724197\n",
            "Epoch: 10, iter 600: loss = 0.004531805869191885\n",
            "Epoch: 10, iter 700: loss = 0.35879480838775635\n",
            "Epoch: 10, iter 800: loss = 0.009487031027674675\n",
            "Epoch: 10, iter 900: loss = 0.17809253931045532\n",
            "Epoch: 10, iter 1000: loss = 0.0006430833018384874\n",
            "Epoch: 10, iter 1100: loss = 0.00040526053635403514\n",
            "Epoch: 10, iter 1200: loss = 0.555634081363678\n",
            "Epoch: 10, iter 1300: loss = 0.03505425527691841\n",
            "Epoch: 10, iter 1400: loss = 0.018126649782061577\n",
            "Epoch: 10, iter 1500: loss = 0.03204432874917984\n",
            "Epoch: 10, iter 1600: loss = 0.06175584718585014\n",
            "Epoch: 10, iter 1700: loss = 0.7621724009513855\n",
            "Epoch: 10, iter 1800: loss = 0.9010800719261169\n",
            "Epoch: 10, iter 1900: loss = 0.0234724972397089\n",
            "Epoch: 10, iter 2000: loss = 0.0015528049552813172\n",
            "Epoch: 10, iter 2100: loss = 0.008977685123682022\n",
            "Epoch: 10, iter 2200: loss = 0.0044159661047160625\n",
            "Epoch: 10, iter 2300: loss = 0.3488175570964813\n",
            "Epoch: 10, iter 2400: loss = 0.11155771464109421\n",
            "Epoch: 10, iter 2500: loss = 0.26573610305786133\n",
            "Epoch: 10, iter 2600: loss = 0.04725014045834541\n",
            "Epoch: 10, iter 2700: loss = 0.005629398860037327\n",
            "Epoch: 10, iter 2800: loss = 0.0343044176697731\n",
            "Epoch: 10, iter 2900: loss = 0.03230936825275421\n",
            "Epoch: 10, iter 3000: loss = 0.0039189960807561874\n",
            "Epoch: 10, iter 3100: loss = 0.010126504115760326\n",
            "Epoch: 10, iter 3200: loss = 0.07627527415752411\n",
            "Epoch: 10, iter 3300: loss = 0.9551177024841309\n",
            "Epoch: 10, iter 3400: loss = 0.0690581277012825\n",
            "Epoch: 10, iter 3500: loss = 0.0017693365225568414\n",
            "Epoch: 10, iter 3600: loss = 0.0030539243016391993\n",
            "Epoch: 10, iter 3700: loss = 0.015634773299098015\n",
            "Epoch: 10, iter 3800: loss = 0.0020709121599793434\n",
            "Epoch: 10, iter 3900: loss = 0.036559537053108215\n",
            "Epoch: 10, iter 4000: loss = 0.055138200521469116\n",
            "Epoch: 10, iter 4100: loss = 0.10400020331144333\n",
            "Epoch: 10, iter 4200: loss = 0.09794871509075165\n",
            "Epoch: 10, iter 4300: loss = 0.10942215472459793\n",
            "Epoch: 10, iter 4400: loss = 0.009126493707299232\n",
            "Epoch: 10, iter 4500: loss = 0.0005736338789574802\n",
            "Epoch: 10, iter 4600: loss = 0.0028129692655056715\n",
            "Epoch: 10, iter 4700: loss = 0.5645883679389954\n",
            "Epoch: 10, iter 4800: loss = 0.3986243009567261\n",
            "Epoch: 10, iter 4900: loss = 0.33294999599456787\n",
            "Epoch: 10, iter 5000: loss = 0.03978358209133148\n",
            "Epoch: 10, iter 5100: loss = 0.24605289101600647\n",
            "Epoch: 10, iter 5200: loss = 0.3644951581954956\n",
            "Epoch: 10, iter 5300: loss = 0.03673319146037102\n",
            "Epoch: 10, iter 5400: loss = 0.6658957004547119\n",
            "Epoch: 10, iter 5500: loss = 0.3042024075984955\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:36<00:00,  5.94it/s]\n",
            "Epoch 10 loss average: 0.225\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8439    0.7709    0.8058     11182\n",
            "           2     0.6934    0.5181    0.5930       969\n",
            "           3     0.7734    0.6869    0.7276      1600\n",
            "           4     0.6306    0.5284    0.5750       827\n",
            "           5     0.5000    0.0137    0.0267       146\n",
            "           6     0.5207    0.3729    0.4346       303\n",
            "\n",
            "   micro avg     0.8116    0.7169    0.7613     15027\n",
            "   macro avg     0.6603    0.4818    0.5271     15027\n",
            "weighted avg     0.8051    0.7169    0.7560     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5993    0.5329    0.5642      1019\n",
            "           2     0.4048    0.1667    0.2361       102\n",
            "           3     0.4016    0.4224    0.4118       116\n",
            "           4     0.4545    0.2966    0.3590       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.6667    0.0851    0.1509        47\n",
            "\n",
            "   micro avg     0.5620    0.4567    0.5039      1419\n",
            "   macro avg     0.4212    0.2506    0.2870      1419\n",
            "weighted avg     0.5522    0.4567    0.4906      1419\n",
            "\n",
            "0.22470178712054226 0.755963115743843 0.5243159079243831 0.4906104005582152\n",
            "Patience counter: 9\n",
            "Epoch: 11, iter 0: loss = 0.9063475131988525\n",
            "Epoch: 11, iter 100: loss = 0.3658301830291748\n",
            "Epoch: 11, iter 200: loss = 0.04130125045776367\n",
            "Epoch: 11, iter 300: loss = 0.007031859364360571\n",
            "Epoch: 11, iter 400: loss = 0.011095443740487099\n",
            "Epoch: 11, iter 500: loss = 0.04302123561501503\n",
            "Epoch: 11, iter 600: loss = 0.00042477023089304566\n",
            "Epoch: 11, iter 700: loss = 0.1340513825416565\n",
            "Epoch: 11, iter 800: loss = 0.006426476873457432\n",
            "Epoch: 11, iter 900: loss = 0.02758883871138096\n",
            "Epoch: 11, iter 1000: loss = 0.21171928942203522\n",
            "Epoch: 11, iter 1100: loss = 0.8101972937583923\n",
            "Epoch: 11, iter 1200: loss = 0.012295879423618317\n",
            "Epoch: 11, iter 1300: loss = 0.3108430504798889\n",
            "Epoch: 11, iter 1400: loss = 0.47171473503112793\n",
            "Epoch: 11, iter 1500: loss = 0.2843577563762665\n",
            "Epoch: 11, iter 1600: loss = 0.004503173287957907\n",
            "Epoch: 11, iter 1700: loss = 0.18001417815685272\n",
            "Epoch: 11, iter 1800: loss = 0.46668022871017456\n",
            "Epoch: 11, iter 1900: loss = 0.4108782112598419\n",
            "Epoch: 11, iter 2000: loss = 0.1544213891029358\n",
            "Epoch: 11, iter 2100: loss = 0.08242927491664886\n",
            "Epoch: 11, iter 2200: loss = 0.0109877223148942\n",
            "Epoch: 11, iter 2300: loss = 0.001420186017639935\n",
            "Epoch: 11, iter 2400: loss = 1.3746923208236694\n",
            "Epoch: 11, iter 2500: loss = 0.09804227948188782\n",
            "Epoch: 11, iter 2600: loss = 0.2994619309902191\n",
            "Epoch: 11, iter 2700: loss = 0.2541836202144623\n",
            "Epoch: 11, iter 2800: loss = 0.45961111783981323\n",
            "Epoch: 11, iter 2900: loss = 0.7381024956703186\n",
            "Epoch: 11, iter 3000: loss = 0.1389714628458023\n",
            "Epoch: 11, iter 3100: loss = 0.7286086082458496\n",
            "Epoch: 11, iter 3200: loss = 0.006187439896166325\n",
            "Epoch: 11, iter 3300: loss = 0.24695375561714172\n",
            "Epoch: 11, iter 3400: loss = 0.01616632007062435\n",
            "Epoch: 11, iter 3500: loss = 0.01691979356110096\n",
            "Epoch: 11, iter 3600: loss = 0.6212296485900879\n",
            "Epoch: 11, iter 3700: loss = 0.763241708278656\n",
            "Epoch: 11, iter 3800: loss = 0.3254060745239258\n",
            "Epoch: 11, iter 3900: loss = 0.13244102895259857\n",
            "Epoch: 11, iter 4000: loss = 0.03274182230234146\n",
            "Epoch: 11, iter 4100: loss = 0.004662451799958944\n",
            "Epoch: 11, iter 4200: loss = 0.002787601435557008\n",
            "Epoch: 11, iter 4300: loss = 0.0056325513869524\n",
            "Epoch: 11, iter 4400: loss = 0.06099894642829895\n",
            "Epoch: 11, iter 4500: loss = 0.2775675058364868\n",
            "Epoch: 11, iter 4600: loss = 0.014560694806277752\n",
            "Epoch: 11, iter 4700: loss = 0.023934105411171913\n",
            "Epoch: 11, iter 4800: loss = 0.04299125820398331\n",
            "Epoch: 11, iter 4900: loss = 0.0018617736641317606\n",
            "Epoch: 11, iter 5000: loss = 0.13044151663780212\n",
            "Epoch: 11, iter 5100: loss = 0.13120360672473907\n",
            "Epoch: 11, iter 5200: loss = 0.0658191591501236\n",
            "Epoch: 11, iter 5300: loss = 0.08661393076181412\n",
            "Epoch: 11, iter 5400: loss = 0.30382177233695984\n",
            "Epoch: 11, iter 5500: loss = 0.38493967056274414\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:36<00:00,  5.94it/s]\n",
            "Epoch 11 loss average: 0.216\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8450    0.7882    0.8156     11182\n",
            "           2     0.7050    0.5501    0.6180       969\n",
            "           3     0.7695    0.7031    0.7348      1600\n",
            "           4     0.6869    0.5599    0.6169       827\n",
            "           5     0.2381    0.0342    0.0599       146\n",
            "           6     0.5279    0.4059    0.4590       303\n",
            "\n",
            "   micro avg     0.8148    0.7362    0.7735     15027\n",
            "   macro avg     0.6287    0.5069    0.5507     15027\n",
            "weighted avg     0.8069    0.7362    0.7688     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6139    0.5025    0.5526      1019\n",
            "           2     0.3191    0.1471    0.2013       102\n",
            "           3     0.4949    0.4224    0.4558       116\n",
            "           4     0.4138    0.3051    0.3512       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.4091    0.1915    0.2609        47\n",
            "\n",
            "   micro avg     0.5697    0.4376    0.4950      1419\n",
            "   macro avg     0.3751    0.2614    0.3036      1419\n",
            "weighted avg     0.5522    0.4376    0.4864      1419\n",
            "\n",
            "0.21613300964483895 0.7688016203570405 0.5062142790978056 0.4864222767822317\n",
            "Patience counter: 10\n",
            "Epoch: 12, iter 0: loss = 0.02438521571457386\n",
            "Epoch: 12, iter 100: loss = 0.397398978471756\n",
            "Epoch: 12, iter 200: loss = 0.26429861783981323\n",
            "Epoch: 12, iter 300: loss = 0.727404773235321\n",
            "Epoch: 12, iter 400: loss = 0.1456511914730072\n",
            "Epoch: 12, iter 500: loss = 0.11752007156610489\n",
            "Epoch: 12, iter 600: loss = 0.44711747765541077\n",
            "Epoch: 12, iter 700: loss = 0.24869544804096222\n",
            "Epoch: 12, iter 800: loss = 0.2560496926307678\n",
            "Epoch: 12, iter 900: loss = 0.0029996878001838923\n",
            "Epoch: 12, iter 1000: loss = 0.0691094920039177\n",
            "Epoch: 12, iter 1100: loss = 0.0013169752201065421\n",
            "Epoch: 12, iter 1200: loss = 0.0024902746081352234\n",
            "Epoch: 12, iter 1300: loss = 0.2841823995113373\n",
            "Epoch: 12, iter 1400: loss = 0.31691786646842957\n",
            "Epoch: 12, iter 1500: loss = 0.01572541706264019\n",
            "Epoch: 12, iter 1600: loss = 0.4081847369670868\n",
            "Epoch: 12, iter 1700: loss = 0.0036349683068692684\n",
            "Epoch: 12, iter 1800: loss = 0.0026923452969640493\n",
            "Epoch: 12, iter 1900: loss = 0.001828820095397532\n",
            "Epoch: 12, iter 2000: loss = 0.5823081135749817\n",
            "Epoch: 12, iter 2100: loss = 0.26262083649635315\n",
            "Epoch: 12, iter 2200: loss = 0.07263336330652237\n",
            "Epoch: 12, iter 2300: loss = 0.2377443015575409\n",
            "Epoch: 12, iter 2400: loss = 0.04682140424847603\n",
            "Epoch: 12, iter 2500: loss = 0.0031506558880209923\n",
            "Epoch: 12, iter 2600: loss = 0.03741420805454254\n",
            "Epoch: 12, iter 2700: loss = 0.007993487641215324\n",
            "Epoch: 12, iter 2800: loss = 0.012088454328477383\n",
            "Epoch: 12, iter 2900: loss = 0.306803822517395\n",
            "Epoch: 12, iter 3000: loss = 0.09842107445001602\n",
            "Epoch: 12, iter 3100: loss = 0.04037908837199211\n",
            "Epoch: 12, iter 3200: loss = 0.7161822319030762\n",
            "Epoch: 12, iter 3300: loss = 0.004892516415566206\n",
            "Epoch: 12, iter 3400: loss = 0.2528470754623413\n",
            "Epoch: 12, iter 3500: loss = 0.11453882604837418\n",
            "Epoch: 12, iter 3600: loss = 0.0015472904779016972\n",
            "Epoch: 12, iter 3700: loss = 0.7918118238449097\n",
            "Epoch: 12, iter 3800: loss = 0.25084248185157776\n",
            "Epoch: 12, iter 3900: loss = 0.9371276497840881\n",
            "Epoch: 12, iter 4000: loss = 0.0003820797137450427\n",
            "Epoch: 12, iter 4100: loss = 0.5168877243995667\n",
            "Epoch: 12, iter 4200: loss = 0.9640254974365234\n",
            "Epoch: 12, iter 4300: loss = 0.06852024793624878\n",
            "Epoch: 12, iter 4400: loss = 0.20142988860607147\n",
            "Epoch: 12, iter 4500: loss = 0.2214042842388153\n",
            "Epoch: 12, iter 4600: loss = 0.05198518559336662\n",
            "Epoch: 12, iter 4700: loss = 0.3617943227291107\n",
            "Epoch: 12, iter 4800: loss = 0.3894914984703064\n",
            "Epoch: 12, iter 4900: loss = 0.3182239234447479\n",
            "Epoch: 12, iter 5000: loss = 0.08685425668954849\n",
            "Epoch: 12, iter 5100: loss = 0.037820037454366684\n",
            "Epoch: 12, iter 5200: loss = 0.26712697744369507\n",
            "Epoch: 12, iter 5300: loss = 0.7212699055671692\n",
            "Epoch: 12, iter 5400: loss = 0.0033436587546020746\n",
            "Epoch: 12, iter 5500: loss = 0.010192504152655602\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:33<00:00,  5.96it/s]\n",
            "Epoch 12 loss average: 0.211\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8491    0.7964    0.8219     11182\n",
            "           2     0.7380    0.5377    0.6221       969\n",
            "           3     0.7862    0.7125    0.7475      1600\n",
            "           4     0.7393    0.5828    0.6518       827\n",
            "           5     0.3750    0.0205    0.0390       146\n",
            "           6     0.5512    0.4620    0.5027       303\n",
            "\n",
            "   micro avg     0.8254    0.7447    0.7830     15027\n",
            "   macro avg     0.6731    0.5187    0.5642     15027\n",
            "weighted avg     0.8186    0.7447    0.7777     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5750    0.5152    0.5435      1019\n",
            "           2     0.2742    0.1667    0.2073       102\n",
            "           3     0.4286    0.3621    0.3925       116\n",
            "           4     0.4086    0.3220    0.3602       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.4286    0.2553    0.3200        47\n",
            "\n",
            "   micro avg     0.5297    0.4468    0.4847      1419\n",
            "   macro avg     0.3525    0.2702    0.3039      1419\n",
            "weighted avg     0.5159    0.4468    0.4778      1419\n",
            "\n",
            "0.21108409004244216 0.777673290933584 0.49451755745468706 0.4778194286385636\n",
            "Patience counter: 11\n",
            "Done! It took 1.2e+04 secs\n",
            "\n",
            "Current RUN: 1\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.47862179693579676\n",
            "Best test f1 weighted\n",
            "0.371353864532691\n",
            "Best epoch\n",
            "1\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/conversation_length.pkl'),\n",
            " 'data': 'dailydialog',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 7,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/datasets/dailydialog/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [7, 256]\n",
            "\tdecoder2output.linears.0.bias\t [7]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 7.9 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.886055588722229\n",
            "Epoch: 1, iter 100: loss = 0.3550548255443573\n",
            "Epoch: 1, iter 200: loss = 0.6268457770347595\n",
            "Epoch: 1, iter 300: loss = 0.4459048807621002\n",
            "Epoch: 1, iter 400: loss = 1.2467011213302612\n",
            "Epoch: 1, iter 500: loss = 0.5341191291809082\n",
            "Epoch: 1, iter 600: loss = 0.12105096131563187\n",
            "Epoch: 1, iter 700: loss = 1.0456385612487793\n",
            "Epoch: 1, iter 800: loss = 1.0738188028335571\n",
            "Epoch: 1, iter 900: loss = 0.170758455991745\n",
            "Epoch: 1, iter 1000: loss = 0.19385689496994019\n",
            "Epoch: 1, iter 1100: loss = 0.385629802942276\n",
            "Epoch: 1, iter 1200: loss = 0.7002232074737549\n",
            "Epoch: 1, iter 1300: loss = 0.3197902739048004\n",
            "Epoch: 1, iter 1400: loss = 0.33394721150398254\n",
            "Epoch: 1, iter 1500: loss = 1.1393775939941406\n",
            "Epoch: 1, iter 1600: loss = 0.10220453888177872\n",
            "Epoch: 1, iter 1700: loss = 0.15151017904281616\n",
            "Epoch: 1, iter 1800: loss = 2.02978777885437\n",
            "Epoch: 1, iter 1900: loss = 0.23623120784759521\n",
            "Epoch: 1, iter 2000: loss = 0.5825733542442322\n",
            "Epoch: 1, iter 2100: loss = 0.155410036444664\n",
            "Epoch: 1, iter 2200: loss = 1.451583981513977\n",
            "Epoch: 1, iter 2300: loss = 0.14294633269309998\n",
            "Epoch: 1, iter 2400: loss = 0.2495308220386505\n",
            "Epoch: 1, iter 2500: loss = 0.16225123405456543\n",
            "Epoch: 1, iter 2600: loss = 0.011014183051884174\n",
            "Epoch: 1, iter 2700: loss = 0.026892445981502533\n",
            "Epoch: 1, iter 2800: loss = 0.5715717673301697\n",
            "Epoch: 1, iter 2900: loss = 0.010856223292648792\n",
            "Epoch: 1, iter 3000: loss = 0.6994211673736572\n",
            "Epoch: 1, iter 3100: loss = 0.3681202232837677\n",
            "Epoch: 1, iter 3200: loss = 0.4204416871070862\n",
            "Epoch: 1, iter 3300: loss = 1.0609111785888672\n",
            "Epoch: 1, iter 3400: loss = 0.0862940102815628\n",
            "Epoch: 1, iter 3500: loss = 0.5839569568634033\n",
            "Epoch: 1, iter 3600: loss = 0.24845845997333527\n",
            "Epoch: 1, iter 3700: loss = 0.22200042009353638\n",
            "Epoch: 1, iter 3800: loss = 0.5968435406684875\n",
            "Epoch: 1, iter 3900: loss = 0.21643196046352386\n",
            "Epoch: 1, iter 4000: loss = 0.9654154181480408\n",
            "Epoch: 1, iter 4100: loss = 0.24153637886047363\n",
            "Epoch: 1, iter 4200: loss = 0.3458632528781891\n",
            "Epoch: 1, iter 4300: loss = 0.6577373147010803\n",
            "Epoch: 1, iter 4400: loss = 0.34547799825668335\n",
            "Epoch: 1, iter 4500: loss = 0.34191012382507324\n",
            "Epoch: 1, iter 4600: loss = 0.46546676754951477\n",
            "Epoch: 1, iter 4700: loss = 0.22646114230155945\n",
            "Epoch: 1, iter 4800: loss = 0.23526252806186676\n",
            "Epoch: 1, iter 4900: loss = 0.037607721984386444\n",
            "Epoch: 1, iter 5000: loss = 0.17093853652477264\n",
            "Epoch: 1, iter 5100: loss = 0.12180117517709732\n",
            "Epoch: 1, iter 5200: loss = 0.202470600605011\n",
            "Epoch: 1, iter 5300: loss = 0.5289584398269653\n",
            "Epoch: 1, iter 5400: loss = 0.013151934370398521\n",
            "Epoch: 1, iter 5500: loss = 0.8818340301513672\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 1 loss average: 0.478\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6302    0.3899    0.4818     11182\n",
            "           2     0.1000    0.0010    0.0020       969\n",
            "           3     0.5698    0.2756    0.3715      1600\n",
            "           4     0.0000    0.0000    0.0000       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.0000    0.0000    0.0000       303\n",
            "\n",
            "   micro avg     0.6234    0.3196    0.4225     15027\n",
            "   macro avg     0.2167    0.1111    0.1426     15027\n",
            "weighted avg     0.5361    0.3196    0.3982     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6323    0.5348    0.5795      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.4495    0.4224    0.4356       116\n",
            "           4     0.0000    0.0000    0.0000       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6105    0.4186    0.4967      1419\n",
            "   macro avg     0.1803    0.1595    0.1692      1419\n",
            "weighted avg     0.4908    0.4186    0.4517      1419\n",
            "\n",
            "0.4782494754886667 0.39818650541491757 0.48848709885374286 0.4517361141551638\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 0.7738623023033142\n",
            "Epoch: 2, iter 100: loss = 0.05398186296224594\n",
            "Epoch: 2, iter 200: loss = 1.1406575441360474\n",
            "Epoch: 2, iter 300: loss = 0.01193016953766346\n",
            "Epoch: 2, iter 400: loss = 0.018862565979361534\n",
            "Epoch: 2, iter 500: loss = 0.2199411392211914\n",
            "Epoch: 2, iter 600: loss = 0.4474027752876282\n",
            "Epoch: 2, iter 700: loss = 1.7575523853302002\n",
            "Epoch: 2, iter 800: loss = 0.14040496945381165\n",
            "Epoch: 2, iter 900: loss = 0.011169558390974998\n",
            "Epoch: 2, iter 1000: loss = 0.444288432598114\n",
            "Epoch: 2, iter 1100: loss = 0.20609058439731598\n",
            "Epoch: 2, iter 1200: loss = 0.06813589483499527\n",
            "Epoch: 2, iter 1300: loss = 0.12648141384124756\n",
            "Epoch: 2, iter 1400: loss = 0.05127652734518051\n",
            "Epoch: 2, iter 1500: loss = 0.19101358950138092\n",
            "Epoch: 2, iter 1600: loss = 0.22268573939800262\n",
            "Epoch: 2, iter 1700: loss = 0.009409703314304352\n",
            "Epoch: 2, iter 1800: loss = 0.009468162432312965\n",
            "Epoch: 2, iter 1900: loss = 0.29787349700927734\n",
            "Epoch: 2, iter 2000: loss = 0.038371093571186066\n",
            "Epoch: 2, iter 2100: loss = 0.46560028195381165\n",
            "Epoch: 2, iter 2200: loss = 0.1519177407026291\n",
            "Epoch: 2, iter 2300: loss = 0.4073949456214905\n",
            "Epoch: 2, iter 2400: loss = 0.01933693140745163\n",
            "Epoch: 2, iter 2500: loss = 0.1132766604423523\n",
            "Epoch: 2, iter 2600: loss = 0.7249197363853455\n",
            "Epoch: 2, iter 2700: loss = 1.3593201637268066\n",
            "Epoch: 2, iter 2800: loss = 0.13505978882312775\n",
            "Epoch: 2, iter 2900: loss = 0.08814872056245804\n",
            "Epoch: 2, iter 3000: loss = 0.10578780621290207\n",
            "Epoch: 2, iter 3100: loss = 0.6049971580505371\n",
            "Epoch: 2, iter 3200: loss = 1.3052470684051514\n",
            "Epoch: 2, iter 3300: loss = 0.2637997567653656\n",
            "Epoch: 2, iter 3400: loss = 0.09714780002832413\n",
            "Epoch: 2, iter 3500: loss = 1.6361337900161743\n",
            "Epoch: 2, iter 3600: loss = 1.5448282957077026\n",
            "Epoch: 2, iter 3700: loss = 0.4349096119403839\n",
            "Epoch: 2, iter 3800: loss = 0.03923371806740761\n",
            "Epoch: 2, iter 3900: loss = 0.16618886590003967\n",
            "Epoch: 2, iter 4000: loss = 0.20167893171310425\n",
            "Epoch: 2, iter 4100: loss = 0.07636845856904984\n",
            "Epoch: 2, iter 4200: loss = 0.20531556010246277\n",
            "Epoch: 2, iter 4300: loss = 0.1426742970943451\n",
            "Epoch: 2, iter 4400: loss = 0.01532917283475399\n",
            "Epoch: 2, iter 4500: loss = 0.06253655254840851\n",
            "Epoch: 2, iter 4600: loss = 0.3515108823776245\n",
            "Epoch: 2, iter 4700: loss = 0.18867655098438263\n",
            "Epoch: 2, iter 4800: loss = 0.02527565322816372\n",
            "Epoch: 2, iter 4900: loss = 0.274066299200058\n",
            "Epoch: 2, iter 5000: loss = 0.03838503360748291\n",
            "Epoch: 2, iter 5100: loss = 0.24135932326316833\n",
            "Epoch: 2, iter 5200: loss = 0.2574881911277771\n",
            "Epoch: 2, iter 5300: loss = 0.6283349990844727\n",
            "Epoch: 2, iter 5400: loss = 0.3087243139743805\n",
            "Epoch: 2, iter 5500: loss = 0.11825349181890488\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 2 loss average: 0.404\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6879    0.5052    0.5826     11182\n",
            "           2     0.3125    0.0052    0.0102       969\n",
            "           3     0.6407    0.4725    0.5439      1600\n",
            "           4     0.3365    0.0423    0.0752       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2500    0.0033    0.0065       303\n",
            "\n",
            "   micro avg     0.6774    0.4290    0.5253     15027\n",
            "   macro avg     0.3713    0.1714    0.2030     15027\n",
            "weighted avg     0.6238    0.4290    0.4963     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6964    0.3984    0.5069      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.5699    0.4569    0.5072       116\n",
            "           4     0.3043    0.0593    0.0993       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6667    0.3284    0.4400      1419\n",
            "   macro avg     0.2618    0.1524    0.1856      1419\n",
            "weighted avg     0.5720    0.3284    0.4137      1419\n",
            "\n",
            "0.40360455900489484 0.49632637722157824 0.4702897498475481 0.41370382441386766\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 0.2838420271873474\n",
            "Epoch: 3, iter 100: loss = 0.11158943176269531\n",
            "Epoch: 3, iter 200: loss = 0.23443935811519623\n",
            "Epoch: 3, iter 300: loss = 0.10067367553710938\n",
            "Epoch: 3, iter 400: loss = 0.009463371708989143\n",
            "Epoch: 3, iter 500: loss = 0.016927244141697884\n",
            "Epoch: 3, iter 600: loss = 0.23148001730442047\n",
            "Epoch: 3, iter 700: loss = 0.43561604619026184\n",
            "Epoch: 3, iter 800: loss = 0.01558014564216137\n",
            "Epoch: 3, iter 900: loss = 0.25302571058273315\n",
            "Epoch: 3, iter 1000: loss = 0.09785794466733932\n",
            "Epoch: 3, iter 1100: loss = 1.0762141942977905\n",
            "Epoch: 3, iter 1200: loss = 0.4956047236919403\n",
            "Epoch: 3, iter 1300: loss = 0.09804907441139221\n",
            "Epoch: 3, iter 1400: loss = 0.03070610947906971\n",
            "Epoch: 3, iter 1500: loss = 0.32373759150505066\n",
            "Epoch: 3, iter 1600: loss = 0.5019651651382446\n",
            "Epoch: 3, iter 1700: loss = 0.18061459064483643\n",
            "Epoch: 3, iter 1800: loss = 0.3688783645629883\n",
            "Epoch: 3, iter 1900: loss = 0.2488246113061905\n",
            "Epoch: 3, iter 2000: loss = 0.13027895987033844\n",
            "Epoch: 3, iter 2100: loss = 1.175466537475586\n",
            "Epoch: 3, iter 2200: loss = 0.16716492176055908\n",
            "Epoch: 3, iter 2300: loss = 0.11942295730113983\n",
            "Epoch: 3, iter 2400: loss = 0.10253694653511047\n",
            "Epoch: 3, iter 2500: loss = 0.7745205760002136\n",
            "Epoch: 3, iter 2600: loss = 0.24937589466571808\n",
            "Epoch: 3, iter 2700: loss = 0.5103499293327332\n",
            "Epoch: 3, iter 2800: loss = 0.26230454444885254\n",
            "Epoch: 3, iter 2900: loss = 0.39298316836357117\n",
            "Epoch: 3, iter 3000: loss = 0.5529860854148865\n",
            "Epoch: 3, iter 3100: loss = 0.43693819642066956\n",
            "Epoch: 3, iter 3200: loss = 0.7167455554008484\n",
            "Epoch: 3, iter 3300: loss = 0.5601818561553955\n",
            "Epoch: 3, iter 3400: loss = 0.35873398184776306\n",
            "Epoch: 3, iter 3500: loss = 0.06861097365617752\n",
            "Epoch: 3, iter 3600: loss = 0.1625806987285614\n",
            "Epoch: 3, iter 3700: loss = 0.31892988085746765\n",
            "Epoch: 3, iter 3800: loss = 0.6818450093269348\n",
            "Epoch: 3, iter 3900: loss = 0.32272469997406006\n",
            "Epoch: 3, iter 4000: loss = 0.9957682490348816\n",
            "Epoch: 3, iter 4100: loss = 1.9343174695968628\n",
            "Epoch: 3, iter 4200: loss = 0.9029355049133301\n",
            "Epoch: 3, iter 4300: loss = 0.07858429849147797\n",
            "Epoch: 3, iter 4400: loss = 0.411365807056427\n",
            "Epoch: 3, iter 4500: loss = 0.15842188894748688\n",
            "Epoch: 3, iter 4600: loss = 0.021841324865818024\n",
            "Epoch: 3, iter 4700: loss = 0.7390862703323364\n",
            "Epoch: 3, iter 4800: loss = 0.31241247057914734\n",
            "Epoch: 3, iter 4900: loss = 0.06390661001205444\n",
            "Epoch: 3, iter 5000: loss = 0.04738414287567139\n",
            "Epoch: 3, iter 5100: loss = 0.16883914172649384\n",
            "Epoch: 3, iter 5200: loss = 0.1655377298593521\n",
            "Epoch: 3, iter 5300: loss = 1.1496187448501587\n",
            "Epoch: 3, iter 5400: loss = 0.056241441518068314\n",
            "Epoch: 3, iter 5500: loss = 0.1476859748363495\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 3 loss average: 0.361\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7265    0.5766    0.6430     11182\n",
            "           2     0.4340    0.0712    0.1223       969\n",
            "           3     0.6618    0.5369    0.5928      1600\n",
            "           4     0.4333    0.1415    0.2133       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.1500    0.0099    0.0186       303\n",
            "\n",
            "   micro avg     0.7057    0.4988    0.5845     15027\n",
            "   macro avg     0.4009    0.2227    0.2650     15027\n",
            "weighted avg     0.6660    0.4988    0.5616     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6490    0.5172    0.5756      1019\n",
            "           2     0.4000    0.0196    0.0374       102\n",
            "           3     0.3889    0.4828    0.4308       116\n",
            "           4     0.2105    0.0339    0.0584       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6010    0.4151    0.4910      1419\n",
            "   macro avg     0.2747    0.1756    0.1837      1419\n",
            "weighted avg     0.5441    0.4151    0.4561      1419\n",
            "\n",
            "0.3609650490570875 0.5615733145023497 0.4993244079456778 0.4561323075785682\n",
            "Patience counter: 1\n",
            "Epoch: 4, iter 0: loss = 0.9156653881072998\n",
            "Epoch: 4, iter 100: loss = 0.6316320300102234\n",
            "Epoch: 4, iter 200: loss = 0.007150193210691214\n",
            "Epoch: 4, iter 300: loss = 0.5441808104515076\n",
            "Epoch: 4, iter 400: loss = 0.6388623118400574\n",
            "Epoch: 4, iter 500: loss = 0.3590739667415619\n",
            "Epoch: 4, iter 600: loss = 0.49145975708961487\n",
            "Epoch: 4, iter 700: loss = 0.18121232092380524\n",
            "Epoch: 4, iter 800: loss = 0.2758409082889557\n",
            "Epoch: 4, iter 900: loss = 0.18816208839416504\n",
            "Epoch: 4, iter 1000: loss = 0.0026660682633519173\n",
            "Epoch: 4, iter 1100: loss = 0.05652295798063278\n",
            "Epoch: 4, iter 1200: loss = 0.4081220328807831\n",
            "Epoch: 4, iter 1300: loss = 0.9233846664428711\n",
            "Epoch: 4, iter 1400: loss = 0.05573306232690811\n",
            "Epoch: 4, iter 1500: loss = 0.06418508291244507\n",
            "Epoch: 4, iter 1600: loss = 0.37352874875068665\n",
            "Epoch: 4, iter 1700: loss = 0.7376395463943481\n",
            "Epoch: 4, iter 1800: loss = 0.028064727783203125\n",
            "Epoch: 4, iter 1900: loss = 0.34138599038124084\n",
            "Epoch: 4, iter 2000: loss = 0.06822045147418976\n",
            "Epoch: 4, iter 2100: loss = 0.08171603828668594\n",
            "Epoch: 4, iter 2200: loss = 0.13719716668128967\n",
            "Epoch: 4, iter 2300: loss = 0.20331469178199768\n",
            "Epoch: 4, iter 2400: loss = 0.31809601187705994\n",
            "Epoch: 4, iter 2500: loss = 0.3774317800998688\n",
            "Epoch: 4, iter 2600: loss = 0.09083066880702972\n",
            "Epoch: 4, iter 2700: loss = 0.6942085027694702\n",
            "Epoch: 4, iter 2800: loss = 0.0232310239225626\n",
            "Epoch: 4, iter 2900: loss = 0.36318662762641907\n",
            "Epoch: 4, iter 3000: loss = 0.3326360583305359\n",
            "Epoch: 4, iter 3100: loss = 0.0672239288687706\n",
            "Epoch: 4, iter 3200: loss = 0.9522193670272827\n",
            "Epoch: 4, iter 3300: loss = 0.736923336982727\n",
            "Epoch: 4, iter 3400: loss = 0.14634957909584045\n",
            "Epoch: 4, iter 3500: loss = 0.10043448954820633\n",
            "Epoch: 4, iter 3600: loss = 0.6817516684532166\n",
            "Epoch: 4, iter 3700: loss = 0.11876009404659271\n",
            "Epoch: 4, iter 3800: loss = 0.30170175433158875\n",
            "Epoch: 4, iter 3900: loss = 0.7478554844856262\n",
            "Epoch: 4, iter 4000: loss = 0.19851963222026825\n",
            "Epoch: 4, iter 4100: loss = 0.07120498269796371\n",
            "Epoch: 4, iter 4200: loss = 0.011876372620463371\n",
            "Epoch: 4, iter 4300: loss = 1.2173529863357544\n",
            "Epoch: 4, iter 4400: loss = 0.21521253883838654\n",
            "Epoch: 4, iter 4500: loss = 0.2822563052177429\n",
            "Epoch: 4, iter 4600: loss = 0.17558592557907104\n",
            "Epoch: 4, iter 4700: loss = 0.10053940117359161\n",
            "Epoch: 4, iter 4800: loss = 0.8185053467750549\n",
            "Epoch: 4, iter 4900: loss = 1.086161494255066\n",
            "Epoch: 4, iter 5000: loss = 0.39117345213890076\n",
            "Epoch: 4, iter 5100: loss = 0.8466957807540894\n",
            "Epoch: 4, iter 5200: loss = 0.012592747807502747\n",
            "Epoch: 4, iter 5300: loss = 0.4934915602207184\n",
            "Epoch: 4, iter 5400: loss = 0.25260046124458313\n",
            "Epoch: 4, iter 5500: loss = 0.3416886031627655\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:37<00:00,  5.93it/s]\n",
            "Epoch 4 loss average: 0.321\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7574    0.6342    0.6904     11182\n",
            "           2     0.4852    0.1527    0.2323       969\n",
            "           3     0.6882    0.5919    0.6364      1600\n",
            "           4     0.5392    0.3241    0.4048       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3171    0.0429    0.0756       303\n",
            "\n",
            "   micro avg     0.7311    0.5635    0.6365     15027\n",
            "   macro avg     0.4645    0.2910    0.3399     15027\n",
            "weighted avg     0.7042    0.5635    0.6203     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6333    0.5407    0.5834      1019\n",
            "           2     0.6667    0.0196    0.0381       102\n",
            "           3     0.4779    0.4655    0.4716       116\n",
            "           4     0.4340    0.1949    0.2690       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3333    0.0638    0.1071        47\n",
            "\n",
            "   micro avg     0.6040    0.4461    0.5132      1419\n",
            "   macro avg     0.4242    0.2141    0.2449      1419\n",
            "weighted avg     0.5889    0.4461    0.4861      1419\n",
            "\n",
            "0.32138908129480886 0.62025993822849 0.5441398241141397 0.48614035385158955\n",
            "Patience counter: 2\n",
            "Epoch: 5, iter 0: loss = 0.9551772475242615\n",
            "Epoch: 5, iter 100: loss = 0.018421020358800888\n",
            "Epoch: 5, iter 200: loss = 0.22570426762104034\n",
            "Epoch: 5, iter 300: loss = 0.022343244403600693\n",
            "Epoch: 5, iter 400: loss = 0.013316908851265907\n",
            "Epoch: 5, iter 500: loss = 0.006501279305666685\n",
            "Epoch: 5, iter 600: loss = 0.20233650505542755\n",
            "Epoch: 5, iter 700: loss = 0.01728958450257778\n",
            "Epoch: 5, iter 800: loss = 0.17866629362106323\n",
            "Epoch: 5, iter 900: loss = 0.22656786441802979\n",
            "Epoch: 5, iter 1000: loss = 0.017111774533987045\n",
            "Epoch: 5, iter 1100: loss = 0.03374633193016052\n",
            "Epoch: 5, iter 1200: loss = 0.05268804356455803\n",
            "Epoch: 5, iter 1300: loss = 1.3131707906723022\n",
            "Epoch: 5, iter 1400: loss = 0.1287771314382553\n",
            "Epoch: 5, iter 1500: loss = 0.3447248935699463\n",
            "Epoch: 5, iter 1600: loss = 0.14777985215187073\n",
            "Epoch: 5, iter 1700: loss = 0.052617188543081284\n",
            "Epoch: 5, iter 1800: loss = 0.4354439377784729\n",
            "Epoch: 5, iter 1900: loss = 0.3687814176082611\n",
            "Epoch: 5, iter 2000: loss = 0.01696300506591797\n",
            "Epoch: 5, iter 2100: loss = 0.0018652534345164895\n",
            "Epoch: 5, iter 2200: loss = 0.113540880382061\n",
            "Epoch: 5, iter 2300: loss = 0.051182638853788376\n",
            "Epoch: 5, iter 2400: loss = 0.05229801684617996\n",
            "Epoch: 5, iter 2500: loss = 0.10804491490125656\n",
            "Epoch: 5, iter 2600: loss = 0.02490927092730999\n",
            "Epoch: 5, iter 2700: loss = 0.2198205441236496\n",
            "Epoch: 5, iter 2800: loss = 0.2309267222881317\n",
            "Epoch: 5, iter 2900: loss = 0.11601829528808594\n",
            "Epoch: 5, iter 3000: loss = 0.025441039353609085\n",
            "Epoch: 5, iter 3100: loss = 1.198126196861267\n",
            "Epoch: 5, iter 3200: loss = 0.22270557284355164\n",
            "Epoch: 5, iter 3300: loss = 0.0168365016579628\n",
            "Epoch: 5, iter 3400: loss = 0.3408941626548767\n",
            "Epoch: 5, iter 3500: loss = 0.059072572737932205\n",
            "Epoch: 5, iter 3600: loss = 0.5412590503692627\n",
            "Epoch: 5, iter 3700: loss = 0.01385965570807457\n",
            "Epoch: 5, iter 3800: loss = 0.07524453848600388\n",
            "Epoch: 5, iter 3900: loss = 1.098152756690979\n",
            "Epoch: 5, iter 4000: loss = 0.5067229866981506\n",
            "Epoch: 5, iter 4100: loss = 0.08119355887174606\n",
            "Epoch: 5, iter 4200: loss = 0.06344924122095108\n",
            "Epoch: 5, iter 4300: loss = 0.012254028581082821\n",
            "Epoch: 5, iter 4400: loss = 0.34880977869033813\n",
            "Epoch: 5, iter 4500: loss = 0.7355660200119019\n",
            "Epoch: 5, iter 4600: loss = 0.1336900144815445\n",
            "Epoch: 5, iter 4700: loss = 0.08434436470270157\n",
            "Epoch: 5, iter 4800: loss = 0.09974545240402222\n",
            "Epoch: 5, iter 4900: loss = 0.0024137189611792564\n",
            "Epoch: 5, iter 5000: loss = 0.5890182256698608\n",
            "Epoch: 5, iter 5100: loss = 0.2118324339389801\n",
            "Epoch: 5, iter 5200: loss = 0.09749669581651688\n",
            "Epoch: 5, iter 5300: loss = 0.09457294642925262\n",
            "Epoch: 5, iter 5400: loss = 0.18802858889102936\n",
            "Epoch: 5, iter 5500: loss = 0.3247967064380646\n",
            "100%|███████████████████████████████████████| 5559/5559 [15:38<00:00,  5.92it/s]\n",
            "Epoch 5 loss average: 0.293\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7893    0.6759    0.7282     11182\n",
            "           2     0.5864    0.3117    0.4070       969\n",
            "           3     0.6988    0.6250    0.6598      1600\n",
            "           4     0.5413    0.3881    0.4521       827\n",
            "           5     1.0000    0.0068    0.0136       146\n",
            "           6     0.3298    0.1023    0.1562       303\n",
            "\n",
            "   micro avg     0.7546    0.6131    0.6765     15027\n",
            "   macro avg     0.6576    0.3516    0.4028     15027\n",
            "weighted avg     0.7458    0.6131    0.6666     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5930    0.5191    0.5536      1019\n",
            "           2     0.5000    0.0686    0.1207       102\n",
            "           3     0.5595    0.4052    0.4700       116\n",
            "           4     0.3882    0.2797    0.3251       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.5556    0.1064    0.1786        47\n",
            "\n",
            "   micro avg     0.5729    0.4376    0.4962      1419\n",
            "   macro avg     0.4327    0.2298    0.2747      1419\n",
            "weighted avg     0.5582    0.4376    0.4776      1419\n",
            "\n",
            "0.29267963129635405 0.6665661357123845 0.5074949340080588 0.4776206295200452\n",
            "Patience counter: 3\n",
            "Epoch: 6, iter 0: loss = 0.16355133056640625\n",
            "Epoch: 6, iter 100: loss = 0.6153500080108643\n",
            "  3%|█▏                                      | 169/5559 [00:28<15:35,  5.76it/s]Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLdEYFpSSIrG",
        "colab_type": "text"
      },
      "source": [
        "## Train my TL-ERC Model with Cornell weights (Source) and Iemocap(target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vakaAq7SVXN",
        "colab_type": "text"
      },
      "source": [
        "## Installing essentials "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG2cOT8pD3wV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "6000f691-877f-4262-f8d1-bd1a93e719e8"
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\r\u001b[K     |█                               | 10kB 18.2MB/s eta 0:00:01\r\u001b[K     |██▏                             | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |███▏                            | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 143kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 153kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 163kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 174kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 184kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 194kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 204kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 215kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 225kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 235kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 245kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 256kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 266kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 276kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 286kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 296kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 307kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 317kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMd3eExyD7Io",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "7d59a8c2-5788-4b2e-a6a8-ed76925b969d"
      },
      "source": [
        "pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 17.2MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 1.9MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.37)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.37)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqW83pmd1loP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b590a615-adc3-468a-c311-5051c80a8135"
      },
      "source": [
        "%cd TL-ERC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'TL-ERC/'\n",
            "/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC/bert_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5C0oMkPShx_",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing my Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fyDNM4Ce1w65",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "859aa078-259a-47ac-dea4-eac92aec1232"
      },
      "source": [
        "!python iemocap_preprocess.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'iemocap_preprocess.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL8_pyoe4smB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "187fdb78-01d4-4d37-d581-587e8353e931"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bert_model\t\t   environment.yml\tiemocap_preprocess.py\n",
            "dailydialog_preprocess.py  generative_weights\tsetup.py\n",
            "datasets\t\t   glove.840B.300d.txt\tutils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-DeCyS3cCkRN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8682023a-34ab-486d-c92f-15224a0c76e9"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/conv-emotion-master/TL-ERC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp04cNbMIHV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from models import *"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SakinaModel.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakinavohracs/EmotionDetection/blob/master/EmotionDetection-DD-Ubuntu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAhzlpZSUe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b90a5c49-9cae-498e-f99d-8cd280817f70"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/Sakina Model/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf1dFoIeVoJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0fbac4e1-4075-435c-e785-256680c41787"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/Sakina\\ Model/TL-ERC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NINDhn63WDwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATASET_DIRECTORY_PATH = \"./datasets/\"\n",
        "GENERATIVE_WEIGHTS_DIRECTORY_PATH = \"./generative_weights/\"\n",
        "\n",
        "if not os.path.exists(DATASET_DIRECTORY_PATH):\n",
        "    os.makedirs(DATASET_DIRECTORY_PATH)\n",
        "\n",
        "\n",
        "if not os.path.exists(GENERATIVE_WEIGHTS_DIRECTORY_PATH):\n",
        "    os.makedirs(GENERATIVE_WEIGHTS_DIRECTORY_PATH)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_ZDpZEX4nC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c400f483-823d-493c-c39a-45f49ddf9296"
      },
      "source": [
        "%cd bert_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/bert_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2R2ZXYTYAPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        },
        "outputId": "c4ad7133-14cd-48e3-9d09-a59b0f8ac77c"
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg9iv36QYG66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "outputId": "829d9d04-b475-4add-dce7-7c7b05160a9d"
      },
      "source": [
        "pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_pretrained_bert in /usr/local/lib/python3.6/dist-packages (0.6.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.37)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.37)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9ZTFX6LYWyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2ad1bc44-4470-4439-f4cd-e1be127b0304"
      },
      "source": [
        "!python train.py --load_checkpoint=../generative_weights/ubuntu_weights.pkl --data=dailydialog"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8600    0.8152    0.8370     11182\n",
            "           2     0.7380    0.5728    0.6450       969\n",
            "           3     0.7895    0.7362    0.7620      1600\n",
            "           4     0.7452    0.6082    0.6698       827\n",
            "           5     0.5357    0.2055    0.2970       146\n",
            "           6     0.6712    0.4851    0.5632       303\n",
            "\n",
            "   micro avg     0.8358    0.7672    0.8000     15027\n",
            "   macro avg     0.7233    0.5705    0.6290     15027\n",
            "weighted avg     0.8314    0.7672    0.7967     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6096    0.5378    0.5714      1019\n",
            "           2     0.4286    0.2059    0.2781       102\n",
            "           3     0.4286    0.4138    0.4211       116\n",
            "           4     0.4138    0.3051    0.3512       118\n",
            "           5     0.4000    0.1176    0.1818        17\n",
            "           6     0.4500    0.1915    0.2687        47\n",
            "\n",
            "   micro avg     0.5666    0.4679    0.5125      1419\n",
            "   macro avg     0.4551    0.2953    0.3454      1419\n",
            "weighted avg     0.5577    0.4679    0.5050      1419\n",
            "\n",
            "0.1945006907529168 0.796674211592781 0.5347641632400565 0.505046058969415\n",
            "Patience counter: 8\n",
            "Epoch: 12, iter 0: loss = 0.0661792904138565\n",
            "Epoch: 12, iter 100: loss = 0.019103210419416428\n",
            "Epoch: 12, iter 200: loss = 0.04931648448109627\n",
            "Epoch: 12, iter 300: loss = 0.03338494524359703\n",
            "Epoch: 12, iter 400: loss = 0.008400495164096355\n",
            "Epoch: 12, iter 500: loss = 0.00390022830106318\n",
            "Epoch: 12, iter 600: loss = 0.013517866842448711\n",
            "Epoch: 12, iter 700: loss = 0.1688862442970276\n",
            "Epoch: 12, iter 800: loss = 0.0009321246761828661\n",
            "Epoch: 12, iter 900: loss = 0.8584010004997253\n",
            "Epoch: 12, iter 1000: loss = 0.49369457364082336\n",
            "Epoch: 12, iter 1100: loss = 0.09544696658849716\n",
            "Epoch: 12, iter 1200: loss = 0.0008045094436965883\n",
            "Epoch: 12, iter 1300: loss = 0.4704616367816925\n",
            "Epoch: 12, iter 1400: loss = 0.14874368906021118\n",
            "Epoch: 12, iter 1500: loss = 0.08230407536029816\n",
            "Epoch: 12, iter 1600: loss = 0.00016863943892531097\n",
            "Epoch: 12, iter 1700: loss = 0.004909032490104437\n",
            "Epoch: 12, iter 1800: loss = 0.01342822052538395\n",
            "Epoch: 12, iter 1900: loss = 0.07823620736598969\n",
            "Epoch: 12, iter 2000: loss = 0.308113157749176\n",
            "Epoch: 12, iter 2100: loss = 0.3527126610279083\n",
            "Epoch: 12, iter 2200: loss = 0.04088333994150162\n",
            "Epoch: 12, iter 2300: loss = 0.0024614844005554914\n",
            "Epoch: 12, iter 2400: loss = 0.7232541441917419\n",
            "Epoch: 12, iter 2500: loss = 0.0033153167460113764\n",
            "Epoch: 12, iter 2600: loss = 0.23938244581222534\n",
            "Epoch: 12, iter 2700: loss = 0.028620051220059395\n",
            "Epoch: 12, iter 2800: loss = 2.411252021789551\n",
            "Epoch: 12, iter 2900: loss = 0.004539454355835915\n",
            "Epoch: 12, iter 3000: loss = 2.3915975093841553\n",
            "Epoch: 12, iter 3100: loss = 0.0009631294524297118\n",
            "Epoch: 12, iter 3200: loss = 0.008166232146322727\n",
            "Epoch: 12, iter 3300: loss = 0.3293154537677765\n",
            "Epoch: 12, iter 3400: loss = 0.4465341866016388\n",
            "Epoch: 12, iter 3500: loss = 0.0031892608385533094\n",
            "Epoch: 12, iter 3600: loss = 0.27635833621025085\n",
            "Epoch: 12, iter 3700: loss = 0.039813071489334106\n",
            "Epoch: 12, iter 3800: loss = 0.0038740031886845827\n",
            "Epoch: 12, iter 3900: loss = 0.007138593588024378\n",
            "Epoch: 12, iter 4000: loss = 0.0330926738679409\n",
            "Epoch: 12, iter 4100: loss = 0.009423065930604935\n",
            "Epoch: 12, iter 4200: loss = 0.006033122073858976\n",
            "Epoch: 12, iter 4300: loss = 0.004348292015492916\n",
            "Epoch: 12, iter 4400: loss = 0.2660644054412842\n",
            "Epoch: 12, iter 4500: loss = 0.004495696630328894\n",
            "Epoch: 12, iter 4600: loss = 0.7622885704040527\n",
            "Epoch: 12, iter 4700: loss = 0.023796912282705307\n",
            "Epoch: 12, iter 4800: loss = 0.027067963033914566\n",
            "Epoch: 12, iter 4900: loss = 0.004964711610227823\n",
            "Epoch: 12, iter 5000: loss = 0.10734913498163223\n",
            "Epoch: 12, iter 5100: loss = 0.06736165285110474\n",
            "Epoch: 12, iter 5200: loss = 0.22025102376937866\n",
            "Epoch: 12, iter 5300: loss = 0.008853668347001076\n",
            "Epoch: 12, iter 5400: loss = 0.0028051435947418213\n",
            "Epoch: 12, iter 5500: loss = 0.7069428563117981\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:11<00:00, 12.90it/s]\n",
            "Epoch 12 loss average: 0.182\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8670    0.8305    0.8484     11182\n",
            "           2     0.7675    0.6233    0.6879       969\n",
            "           3     0.8040    0.7488    0.7754      1600\n",
            "           4     0.7589    0.6167    0.6805       827\n",
            "           5     0.5846    0.2603    0.3602       146\n",
            "           6     0.6723    0.5281    0.5915       303\n",
            "\n",
            "   micro avg     0.8448    0.7851    0.8138     15027\n",
            "   macro avg     0.7424    0.6013    0.6573     15027\n",
            "weighted avg     0.8412    0.7851    0.8111     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5862    0.5270    0.5550      1019\n",
            "           2     0.4138    0.2353    0.3000       102\n",
            "           3     0.4513    0.4397    0.4454       116\n",
            "           4     0.4301    0.3390    0.3791       118\n",
            "           5     0.4286    0.1765    0.2500        17\n",
            "           6     0.3514    0.2766    0.3095        47\n",
            "\n",
            "   micro avg     0.5458    0.4708    0.5055      1419\n",
            "   macro avg     0.4436    0.3323    0.3732      1419\n",
            "weighted avg     0.5402    0.4708    0.5013      1419\n",
            "\n",
            "0.18207704384182335 0.8110840338349306 0.5295180865892074 0.5013316236330398\n",
            "Patience counter: 9\n",
            "Epoch: 13, iter 0: loss = 0.3301745355129242\n",
            "Epoch: 13, iter 100: loss = 0.004709393717348576\n",
            "Epoch: 13, iter 200: loss = 0.006725039333105087\n",
            "Epoch: 13, iter 300: loss = 0.0016876205336302519\n",
            "Epoch: 13, iter 400: loss = 0.18225142359733582\n",
            "Epoch: 13, iter 500: loss = 0.003599798306822777\n",
            "Epoch: 13, iter 600: loss = 0.00010733027738751844\n",
            "Epoch: 13, iter 700: loss = 0.00047040797653608024\n",
            "Epoch: 13, iter 800: loss = 0.07575858384370804\n",
            "Epoch: 13, iter 900: loss = 0.00010087526607094333\n",
            "Epoch: 13, iter 1000: loss = 0.0025656158104538918\n",
            "Epoch: 13, iter 1100: loss = 0.1885223090648651\n",
            "Epoch: 13, iter 1200: loss = 0.15887980163097382\n",
            "Epoch: 13, iter 1300: loss = 2.3719100952148438\n",
            "Epoch: 13, iter 1400: loss = 0.018207112327218056\n",
            "Epoch: 13, iter 1500: loss = 1.7912884950637817\n",
            "Epoch: 13, iter 1600: loss = 0.0032349801622331142\n",
            "Epoch: 13, iter 1700: loss = 0.001953832572326064\n",
            "Epoch: 13, iter 1800: loss = 0.4361356794834137\n",
            "Epoch: 13, iter 1900: loss = 0.00024376739747822285\n",
            "Epoch: 13, iter 2000: loss = 0.294405996799469\n",
            "Epoch: 13, iter 2100: loss = 0.045110683888196945\n",
            "Epoch: 13, iter 2200: loss = 0.0006867412594147027\n",
            "Epoch: 13, iter 2300: loss = 0.6467946171760559\n",
            "Epoch: 13, iter 2400: loss = 0.07276489585638046\n",
            "Epoch: 13, iter 2500: loss = 0.5282760858535767\n",
            "Epoch: 13, iter 2600: loss = 0.25372985005378723\n",
            "Epoch: 13, iter 2700: loss = 0.156696155667305\n",
            "Epoch: 13, iter 2800: loss = 0.13025783002376556\n",
            "Epoch: 13, iter 2900: loss = 0.0007127149729058146\n",
            "Epoch: 13, iter 3000: loss = 0.024749813601374626\n",
            "Epoch: 13, iter 3100: loss = 0.3115467429161072\n",
            "Epoch: 13, iter 3200: loss = 0.03816632553935051\n",
            "Epoch: 13, iter 3300: loss = 0.00015825666196178645\n",
            "Epoch: 13, iter 3400: loss = 0.0449872687458992\n",
            "Epoch: 13, iter 3500: loss = 0.07993342727422714\n",
            "Epoch: 13, iter 3600: loss = 0.0014983715955168009\n",
            "Epoch: 13, iter 3700: loss = 0.11517477035522461\n",
            "Epoch: 13, iter 3800: loss = 0.007951969280838966\n",
            "Epoch: 13, iter 3900: loss = 0.002449921565130353\n",
            "Epoch: 13, iter 4000: loss = 0.01291585247963667\n",
            "Epoch: 13, iter 4100: loss = 0.005743876099586487\n",
            "Epoch: 13, iter 4200: loss = 0.03894468769431114\n",
            "Epoch: 13, iter 4300: loss = 0.0030027111060917377\n",
            "Epoch: 13, iter 4400: loss = 0.2754838764667511\n",
            "Epoch: 13, iter 4500: loss = 0.021328723058104515\n",
            "Epoch: 13, iter 4600: loss = 0.013026826083660126\n",
            "Epoch: 13, iter 4700: loss = 0.4265381097793579\n",
            "Epoch: 13, iter 4800: loss = 0.010854518972337246\n",
            "Epoch: 13, iter 4900: loss = 0.005553444381803274\n",
            "Epoch: 13, iter 5000: loss = 0.3994532823562622\n",
            "Epoch: 13, iter 5100: loss = 0.035104308277368546\n",
            "Epoch: 13, iter 5200: loss = 0.012160439044237137\n",
            "Epoch: 13, iter 5300: loss = 0.21369698643684387\n",
            "Epoch: 13, iter 5400: loss = 0.21471886336803436\n",
            "Epoch: 13, iter 5500: loss = 0.15616042912006378\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:11<00:00, 12.89it/s]\n",
            "Epoch 13 loss average: 0.178\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8740    0.8316    0.8523     11182\n",
            "           2     0.7669    0.6450    0.7007       969\n",
            "           3     0.8131    0.7669    0.7893      1600\n",
            "           4     0.7783    0.6409    0.7029       827\n",
            "           5     0.5789    0.3014    0.3964       146\n",
            "           6     0.6872    0.5149    0.5887       303\n",
            "\n",
            "   micro avg     0.8519    0.7906    0.8201     15027\n",
            "   macro avg     0.7497    0.6168    0.6717     15027\n",
            "weighted avg     0.8487    0.7906    0.8178     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5586    0.5613    0.5600      1019\n",
            "           2     0.3793    0.2157    0.2750       102\n",
            "           3     0.4737    0.3879    0.4265       116\n",
            "           4     0.4595    0.2881    0.3542       118\n",
            "           5     0.5000    0.0588    0.1053        17\n",
            "           6     0.3636    0.1702    0.2319        47\n",
            "\n",
            "   micro avg     0.5349    0.4806    0.5063      1419\n",
            "   macro avg     0.4558    0.2804    0.3255      1419\n",
            "weighted avg     0.5234    0.4806    0.4951      1419\n",
            "\n",
            "0.1780032602702431 0.8178493092450873 0.5204515013708281 0.49514338404267005\n",
            "Patience counter: 10\n",
            "Epoch: 14, iter 0: loss = 0.13779239356517792\n",
            "Epoch: 14, iter 100: loss = 0.10441090166568756\n",
            "Epoch: 14, iter 200: loss = 0.011242054402828217\n",
            "Epoch: 14, iter 300: loss = 0.21705739200115204\n",
            "Epoch: 14, iter 400: loss = 0.0013151320163160563\n",
            "Epoch: 14, iter 500: loss = 0.0007315965485759079\n",
            "Epoch: 14, iter 600: loss = 0.003373516723513603\n",
            "Epoch: 14, iter 700: loss = 0.08537667244672775\n",
            "Epoch: 14, iter 800: loss = 0.0003635851899161935\n",
            "Epoch: 14, iter 900: loss = 0.2567089796066284\n",
            "Epoch: 14, iter 1000: loss = 0.000520339934155345\n",
            "Epoch: 14, iter 1100: loss = 0.00048183617764152586\n",
            "Epoch: 14, iter 1200: loss = 0.022440655156970024\n",
            "Epoch: 14, iter 1300: loss = 0.015175627544522285\n",
            "Epoch: 14, iter 1400: loss = 0.0007825099164620042\n",
            "Epoch: 14, iter 1500: loss = 0.041122689843177795\n",
            "Epoch: 14, iter 1600: loss = 0.015035432763397694\n",
            "Epoch: 14, iter 1700: loss = 0.004198678769171238\n",
            "Epoch: 14, iter 1800: loss = 0.2698505222797394\n",
            "Epoch: 14, iter 1900: loss = 0.2238486260175705\n",
            "Epoch: 14, iter 2000: loss = 0.11379910260438919\n",
            "Epoch: 14, iter 2100: loss = 0.19169683754444122\n",
            "Epoch: 14, iter 2200: loss = 2.495521068572998\n",
            "Epoch: 14, iter 2300: loss = 0.4345051050186157\n",
            "Epoch: 14, iter 2400: loss = 0.1649274379014969\n",
            "Epoch: 14, iter 2500: loss = 0.7781713008880615\n",
            "Epoch: 14, iter 2600: loss = 0.18430234491825104\n",
            "Epoch: 14, iter 2700: loss = 0.026370663195848465\n",
            "Epoch: 14, iter 2800: loss = 0.42201104760169983\n",
            "Epoch: 14, iter 2900: loss = 0.022011272609233856\n",
            "Epoch: 14, iter 3000: loss = 0.14786671102046967\n",
            "Epoch: 14, iter 3100: loss = 0.0006008463096804917\n",
            "Epoch: 14, iter 3200: loss = 0.0002491627528797835\n",
            "Epoch: 14, iter 3300: loss = 0.18518386781215668\n",
            "Epoch: 14, iter 3400: loss = 0.00037772610085085034\n",
            "Epoch: 14, iter 3500: loss = 0.0034800502471625805\n",
            "Epoch: 14, iter 3600: loss = 0.5034730434417725\n",
            "Epoch: 14, iter 3700: loss = 0.0006437475676648319\n",
            "Epoch: 14, iter 3800: loss = 0.0016853439155966043\n",
            "Epoch: 14, iter 3900: loss = 0.00017219282744918019\n",
            "Epoch: 14, iter 4000: loss = 0.001841471646912396\n",
            "Epoch: 14, iter 4100: loss = 0.7793668508529663\n",
            "Epoch: 14, iter 4200: loss = 0.1409098356962204\n",
            "Epoch: 14, iter 4300: loss = 0.07846315950155258\n",
            "Epoch: 14, iter 4400: loss = 0.21087396144866943\n",
            "Epoch: 14, iter 4500: loss = 0.9203948378562927\n",
            "Epoch: 14, iter 4600: loss = 0.008408496156334877\n",
            "Epoch: 14, iter 4700: loss = 0.003262402256950736\n",
            "Epoch: 14, iter 4800: loss = 0.004416788462549448\n",
            "Epoch: 14, iter 4900: loss = 0.000771852268371731\n",
            "Epoch: 14, iter 5000: loss = 0.3584018647670746\n",
            "Epoch: 14, iter 5100: loss = 0.04832212254405022\n",
            "Epoch: 14, iter 5200: loss = 0.0012770784087479115\n",
            "Epoch: 14, iter 5300: loss = 0.0008028431329876184\n",
            "Epoch: 14, iter 5400: loss = 0.05755992233753204\n",
            "Epoch: 14, iter 5500: loss = 0.001569064217619598\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:10<00:00, 12.90it/s]\n",
            "Epoch 14 loss average: 0.175\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8715    0.8368    0.8538     11182\n",
            "           2     0.7593    0.6316    0.6896       969\n",
            "           3     0.8163    0.7806    0.7981      1600\n",
            "           4     0.7781    0.6530    0.7101       827\n",
            "           5     0.5915    0.2877    0.3871       146\n",
            "           6     0.6680    0.5380    0.5960       303\n",
            "\n",
            "   micro avg     0.8495    0.7961    0.8219     15027\n",
            "   macro avg     0.7475    0.6213    0.6724     15027\n",
            "weighted avg     0.8464    0.7961    0.8196     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5915    0.5329    0.5607      1019\n",
            "           2     0.3676    0.2451    0.2941       102\n",
            "           3     0.5287    0.3966    0.4532       116\n",
            "           4     0.4795    0.2966    0.3665       118\n",
            "           5     0.4444    0.2353    0.3077        17\n",
            "           6     0.4000    0.2128    0.2778        47\n",
            "\n",
            "   micro avg     0.5619    0.4672    0.5102      1419\n",
            "   macro avg     0.4686    0.3199    0.3767      1419\n",
            "weighted avg     0.5529    0.4672    0.5042      1419\n",
            "\n",
            "0.1750511698345901 0.819619078906555 0.4934897312368454 0.5041699773423176\n",
            "Patience counter: 11\n",
            "Done! It took 6.5e+03 secs\n",
            "\n",
            "Current RUN: 2\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.49049721588939427\n",
            "Best test f1 weighted\n",
            "0.4719282201423072\n",
            "Best epoch\n",
            "3\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/conversation_length.pkl'),\n",
            " 'data': 'dailydialog',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/ubuntu_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 7,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [7, 256]\n",
            "\tdecoder2output.linears.0.bias\t [7]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.0 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 2.101332187652588\n",
            "Epoch: 1, iter 100: loss = 0.5297907590866089\n",
            "Epoch: 1, iter 200: loss = 0.9455361366271973\n",
            "Epoch: 1, iter 300: loss = 1.8163577318191528\n",
            "Epoch: 1, iter 400: loss = 0.8330456614494324\n",
            "Epoch: 1, iter 500: loss = 0.31081053614616394\n",
            "Epoch: 1, iter 600: loss = 0.10790803283452988\n",
            "Epoch: 1, iter 700: loss = 0.3232388198375702\n",
            "Epoch: 1, iter 800: loss = 0.26408350467681885\n",
            "Epoch: 1, iter 900: loss = 0.26316043734550476\n",
            "Epoch: 1, iter 1000: loss = 0.3767978847026825\n",
            "Epoch: 1, iter 1100: loss = 1.2729192972183228\n",
            "Epoch: 1, iter 1200: loss = 1.4392980337142944\n",
            "Epoch: 1, iter 1300: loss = 0.11813073605298996\n",
            "Epoch: 1, iter 1400: loss = 0.013083048164844513\n",
            "Epoch: 1, iter 1500: loss = 0.17526330053806305\n",
            "Epoch: 1, iter 1600: loss = 0.1418565958738327\n",
            "Epoch: 1, iter 1700: loss = 0.052878543734550476\n",
            "Epoch: 1, iter 1800: loss = 0.21427695453166962\n",
            "Epoch: 1, iter 1900: loss = 0.7995423674583435\n",
            "Epoch: 1, iter 2000: loss = 0.25349634885787964\n",
            "Epoch: 1, iter 2100: loss = 0.6271076202392578\n",
            "Epoch: 1, iter 2200: loss = 0.029121380299329758\n",
            "Epoch: 1, iter 2300: loss = 2.317089557647705\n",
            "Epoch: 1, iter 2400: loss = 0.3582473695278168\n",
            "Epoch: 1, iter 2500: loss = 0.19324719905853271\n",
            "Epoch: 1, iter 2600: loss = 1.2302966117858887\n",
            "Epoch: 1, iter 2700: loss = 0.7845165729522705\n",
            "Epoch: 1, iter 2800: loss = 0.45116373896598816\n",
            "Epoch: 1, iter 2900: loss = 0.04736122488975525\n",
            "Epoch: 1, iter 3000: loss = 0.8286809921264648\n",
            "Epoch: 1, iter 3100: loss = 0.08456217497587204\n",
            "Epoch: 1, iter 3200: loss = 0.058509789407253265\n",
            "Epoch: 1, iter 3300: loss = 1.1857260465621948\n",
            "Epoch: 1, iter 3400: loss = 0.43219485878944397\n",
            "Epoch: 1, iter 3500: loss = 0.30376142263412476\n",
            "Epoch: 1, iter 3600: loss = 0.315592497587204\n",
            "Epoch: 1, iter 3700: loss = 0.18617171049118042\n",
            "Epoch: 1, iter 3800: loss = 0.2340526133775711\n",
            "Epoch: 1, iter 3900: loss = 0.023222558200359344\n",
            "Epoch: 1, iter 4000: loss = 0.963747501373291\n",
            "Epoch: 1, iter 4100: loss = 0.25216564536094666\n",
            "Epoch: 1, iter 4200: loss = 0.0820220410823822\n",
            "Epoch: 1, iter 4300: loss = 0.8776760697364807\n",
            "Epoch: 1, iter 4400: loss = 0.07520537078380585\n",
            "Epoch: 1, iter 4500: loss = 0.5615660548210144\n",
            "Epoch: 1, iter 4600: loss = 0.07802092283964157\n",
            "Epoch: 1, iter 4700: loss = 0.28903037309646606\n",
            "Epoch: 1, iter 4800: loss = 0.16224221885204315\n",
            "Epoch: 1, iter 4900: loss = 1.5318069458007812\n",
            "Epoch: 1, iter 5000: loss = 0.23085330426692963\n",
            "Epoch: 1, iter 5100: loss = 0.07466573268175125\n",
            "Epoch: 1, iter 5200: loss = 0.08452267199754715\n",
            "Epoch: 1, iter 5300: loss = 0.0811992809176445\n",
            "Epoch: 1, iter 5400: loss = 0.16176018118858337\n",
            "Epoch: 1, iter 5500: loss = 0.2705286145210266\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:15<00:00, 12.77it/s]\n",
            "Epoch 1 loss average: 0.479\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6338    0.3845    0.4787     11182\n",
            "           2     0.4545    0.0103    0.0202       969\n",
            "           3     0.5895    0.2800    0.3797      1600\n",
            "           4     0.2000    0.0012    0.0024       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.0000    0.0000    0.0000       303\n",
            "\n",
            "   micro avg     0.6281    0.3167    0.4211     15027\n",
            "   macro avg     0.3130    0.1127    0.1468     15027\n",
            "weighted avg     0.5747    0.3167    0.3980     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7210    0.3778    0.4958      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.5000    0.4569    0.4775       116\n",
            "           4     0.0000    0.0000    0.0000       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6822    0.3087    0.4250      1419\n",
            "   macro avg     0.2035    0.1391    0.1622      1419\n",
            "weighted avg     0.5586    0.3087    0.3951      1419\n",
            "\n",
            "0.47931542805171357 0.39803855170650765 0.4582582963366595 0.3950827458517002\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 0.17410947382450104\n",
            "Epoch: 2, iter 100: loss = 0.006025952752679586\n",
            "Epoch: 2, iter 200: loss = 0.009273435920476913\n",
            "Epoch: 2, iter 300: loss = 0.08573119342327118\n",
            "Epoch: 2, iter 400: loss = 0.311970591545105\n",
            "Epoch: 2, iter 500: loss = 0.09765566885471344\n",
            "Epoch: 2, iter 600: loss = 0.14036081731319427\n",
            "Epoch: 2, iter 700: loss = 0.13936930894851685\n",
            "Epoch: 2, iter 800: loss = 0.45398005843162537\n",
            "Epoch: 2, iter 900: loss = 0.36494478583335876\n",
            "Epoch: 2, iter 1000: loss = 0.7444166541099548\n",
            "Epoch: 2, iter 1100: loss = 1.2627625465393066\n",
            "Epoch: 2, iter 1200: loss = 0.9518007040023804\n",
            "Epoch: 2, iter 1300: loss = 0.18210282921791077\n",
            "Epoch: 2, iter 1400: loss = 1.3789955377578735\n",
            "Epoch: 2, iter 1500: loss = 0.7989509105682373\n",
            "Epoch: 2, iter 1600: loss = 0.09715301543474197\n",
            "Epoch: 2, iter 1700: loss = 0.18790657818317413\n",
            "Epoch: 2, iter 1800: loss = 0.7389590740203857\n",
            "Epoch: 2, iter 1900: loss = 0.5073025226593018\n",
            "Epoch: 2, iter 2000: loss = 0.2315291315317154\n",
            "Epoch: 2, iter 2100: loss = 0.20001646876335144\n",
            "Epoch: 2, iter 2200: loss = 0.12032365798950195\n",
            "Epoch: 2, iter 2300: loss = 1.0081043243408203\n",
            "Epoch: 2, iter 2400: loss = 0.4274641275405884\n",
            "Epoch: 2, iter 2500: loss = 0.1732751578092575\n",
            "Epoch: 2, iter 2600: loss = 0.2922847270965576\n",
            "Epoch: 2, iter 2700: loss = 0.6496535539627075\n",
            "Epoch: 2, iter 2800: loss = 0.025856176391243935\n",
            "Epoch: 2, iter 2900: loss = 0.4692355692386627\n",
            "Epoch: 2, iter 3000: loss = 0.0676397904753685\n",
            "Epoch: 2, iter 3100: loss = 0.007828641682863235\n",
            "Epoch: 2, iter 3200: loss = 0.05274400860071182\n",
            "Epoch: 2, iter 3300: loss = 0.11960230022668839\n",
            "Epoch: 2, iter 3400: loss = 0.43223243951797485\n",
            "Epoch: 2, iter 3500: loss = 1.4966838359832764\n",
            "Epoch: 2, iter 3600: loss = 0.8210777640342712\n",
            "Epoch: 2, iter 3700: loss = 0.3417937755584717\n",
            "Epoch: 2, iter 3800: loss = 1.3955175876617432\n",
            "Epoch: 2, iter 3900: loss = 0.015935678035020828\n",
            "Epoch: 2, iter 4000: loss = 0.2518346309661865\n",
            "Epoch: 2, iter 4100: loss = 0.06930891424417496\n",
            "Epoch: 2, iter 4200: loss = 0.11744485795497894\n",
            "Epoch: 2, iter 4300: loss = 0.2551524043083191\n",
            "Epoch: 2, iter 4400: loss = 0.39359134435653687\n",
            "Epoch: 2, iter 4500: loss = 0.017042839899659157\n",
            "Epoch: 2, iter 4600: loss = 0.5580875277519226\n",
            "Epoch: 2, iter 4700: loss = 0.13523416221141815\n",
            "Epoch: 2, iter 4800: loss = 0.03772839903831482\n",
            "Epoch: 2, iter 4900: loss = 0.08993253856897354\n",
            "Epoch: 2, iter 5000: loss = 0.12229813635349274\n",
            "Epoch: 2, iter 5100: loss = 0.17825458943843842\n",
            "Epoch: 2, iter 5200: loss = 0.8922313451766968\n",
            "Epoch: 2, iter 5300: loss = 0.7260823845863342\n",
            "Epoch: 2, iter 5400: loss = 0.35272425413131714\n",
            "Epoch: 2, iter 5500: loss = 0.07808158546686172\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:13<00:00, 12.82it/s]\n",
            "Epoch 2 loss average: 0.402\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6883    0.5047    0.5824     11182\n",
            "           2     0.3226    0.0103    0.0200       969\n",
            "           3     0.6255    0.4562    0.5276      1600\n",
            "           4     0.3871    0.0435    0.0783       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.0000    0.0000    0.0000       303\n",
            "\n",
            "   micro avg     0.6765    0.4272    0.5237     15027\n",
            "   macro avg     0.3373    0.1691    0.2014     15027\n",
            "weighted avg     0.6209    0.4272    0.4951     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6504    0.5113    0.5725      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.6491    0.3190    0.4277       116\n",
            "           4     0.3750    0.0254    0.0476       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6478    0.3953    0.4910      1419\n",
            "   macro avg     0.2791    0.1426    0.1746      1419\n",
            "weighted avg     0.5513    0.3953    0.4501      1419\n",
            "\n",
            "0.40196487292575156 0.4951226433504134 0.48952252661918944 0.4500655667613592\n",
            "Patience counter: 1\n",
            "Epoch: 3, iter 0: loss = 0.024306442588567734\n",
            "Epoch: 3, iter 100: loss = 0.006031781435012817\n",
            "Epoch: 3, iter 200: loss = 0.03265868499875069\n",
            "Epoch: 3, iter 300: loss = 0.07878968864679337\n",
            "Epoch: 3, iter 400: loss = 0.16800753772258759\n",
            "Epoch: 3, iter 500: loss = 0.5223172307014465\n",
            "Epoch: 3, iter 600: loss = 0.06320149451494217\n",
            "Epoch: 3, iter 700: loss = 0.43720003962516785\n",
            "Epoch: 3, iter 800: loss = 0.7600131034851074\n",
            "Epoch: 3, iter 900: loss = 0.7663387060165405\n",
            "Epoch: 3, iter 1000: loss = 0.44424116611480713\n",
            "Epoch: 3, iter 1100: loss = 0.5343918204307556\n",
            "Epoch: 3, iter 1200: loss = 0.21990741789340973\n",
            "Epoch: 3, iter 1300: loss = 0.44454705715179443\n",
            "Epoch: 3, iter 1400: loss = 0.04389575496315956\n",
            "Epoch: 3, iter 1500: loss = 0.18045872449874878\n",
            "Epoch: 3, iter 1600: loss = 1.3216376304626465\n",
            "Epoch: 3, iter 1700: loss = 0.5099361538887024\n",
            "Epoch: 3, iter 1800: loss = 0.020125744864344597\n",
            "Epoch: 3, iter 1900: loss = 0.014371087774634361\n",
            "Epoch: 3, iter 2000: loss = 0.046427786350250244\n",
            "Epoch: 3, iter 2100: loss = 0.34784331917762756\n",
            "Epoch: 3, iter 2200: loss = 0.3101402819156647\n",
            "Epoch: 3, iter 2300: loss = 0.35010430216789246\n",
            "Epoch: 3, iter 2400: loss = 0.03685840964317322\n",
            "Epoch: 3, iter 2500: loss = 0.6826149225234985\n",
            "Epoch: 3, iter 2600: loss = 0.3146666884422302\n",
            "Epoch: 3, iter 2700: loss = 0.10672871768474579\n",
            "Epoch: 3, iter 2800: loss = 0.0064300186932086945\n",
            "Epoch: 3, iter 2900: loss = 0.1352793276309967\n",
            "Epoch: 3, iter 3000: loss = 0.4608648419380188\n",
            "Epoch: 3, iter 3100: loss = 0.08550117909908295\n",
            "Epoch: 3, iter 3200: loss = 0.0899796262383461\n",
            "Epoch: 3, iter 3300: loss = 0.08695019781589508\n",
            "Epoch: 3, iter 3400: loss = 0.2995920479297638\n",
            "Epoch: 3, iter 3500: loss = 0.11771335452795029\n",
            "Epoch: 3, iter 3600: loss = 0.12613332271575928\n",
            "Epoch: 3, iter 3700: loss = 0.394071102142334\n",
            "Epoch: 3, iter 3800: loss = 0.11823223531246185\n",
            "Epoch: 3, iter 3900: loss = 0.5835806727409363\n",
            "Epoch: 3, iter 4000: loss = 0.7122095823287964\n",
            "Epoch: 3, iter 4100: loss = 0.1246953010559082\n",
            "Epoch: 3, iter 4200: loss = 0.479225754737854\n",
            "Epoch: 3, iter 4300: loss = 0.2780126929283142\n",
            "Epoch: 3, iter 4400: loss = 0.3896174132823944\n",
            "Epoch: 3, iter 4500: loss = 0.07918500155210495\n",
            "Epoch: 3, iter 4600: loss = 0.5128094553947449\n",
            "Epoch: 3, iter 4700: loss = 0.02402624487876892\n",
            "Epoch: 3, iter 4800: loss = 0.02024078369140625\n",
            "Epoch: 3, iter 4900: loss = 0.2197202891111374\n",
            "Epoch: 3, iter 5000: loss = 0.1606593132019043\n",
            "Epoch: 3, iter 5100: loss = 0.1745435893535614\n",
            "Epoch: 3, iter 5200: loss = 0.21659035980701447\n",
            "Epoch: 3, iter 5300: loss = 0.020907757803797722\n",
            "Epoch: 3, iter 5400: loss = 0.28514328598976135\n",
            "Epoch: 3, iter 5500: loss = 0.4624415636062622\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:13<00:00, 12.81it/s]\n",
            "Epoch 3 loss average: 0.365\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7209    0.5560    0.6278     11182\n",
            "           2     0.5115    0.0918    0.1557       969\n",
            "           3     0.6567    0.5356    0.5900      1600\n",
            "           4     0.4691    0.1838    0.2641       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2778    0.0165    0.0312       303\n",
            "\n",
            "   micro avg     0.7008    0.4871    0.5747     15027\n",
            "   macro avg     0.4393    0.2306    0.2781     15027\n",
            "weighted avg     0.6708    0.4871    0.5552     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6016    0.5780    0.5896      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.3974    0.5172    0.4494       116\n",
            "           4     0.5000    0.0508    0.0923       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.5736    0.4616    0.5115      1419\n",
            "   macro avg     0.2498    0.1910    0.1886      1419\n",
            "weighted avg     0.5061    0.4616    0.4678      1419\n",
            "\n",
            "0.3646266900818555 0.5551832195762468 0.5076916650943664 0.46780756232894544\n",
            "Patience counter: 2\n",
            "Epoch: 4, iter 0: loss = 0.05563103407621384\n",
            "Epoch: 4, iter 100: loss = 0.5500320196151733\n",
            "Epoch: 4, iter 200: loss = 0.14263510704040527\n",
            "Epoch: 4, iter 300: loss = 0.480371356010437\n",
            "Epoch: 4, iter 400: loss = 0.06351729482412338\n",
            "Epoch: 4, iter 500: loss = 0.7955915927886963\n",
            "Epoch: 4, iter 600: loss = 0.008910132572054863\n",
            "Epoch: 4, iter 700: loss = 0.44092896580696106\n",
            "Epoch: 4, iter 800: loss = 0.430420458316803\n",
            "Epoch: 4, iter 900: loss = 0.599623441696167\n",
            "Epoch: 4, iter 1000: loss = 0.09092242270708084\n",
            "Epoch: 4, iter 1100: loss = 0.166334867477417\n",
            "Epoch: 4, iter 1200: loss = 0.4305877089500427\n",
            "Epoch: 4, iter 1300: loss = 0.003609744133427739\n",
            "Epoch: 4, iter 1400: loss = 0.21747469902038574\n",
            "Epoch: 4, iter 1500: loss = 0.24935351312160492\n",
            "Epoch: 4, iter 1600: loss = 1.1239489316940308\n",
            "Epoch: 4, iter 1700: loss = 0.1568426638841629\n",
            "Epoch: 4, iter 1800: loss = 0.002947967266663909\n",
            "Epoch: 4, iter 1900: loss = 0.2600473165512085\n",
            "Epoch: 4, iter 2000: loss = 0.07733453810214996\n",
            "Epoch: 4, iter 2100: loss = 0.2921099364757538\n",
            "Epoch: 4, iter 2200: loss = 0.364493191242218\n",
            "Epoch: 4, iter 2300: loss = 0.8171712160110474\n",
            "Epoch: 4, iter 2400: loss = 0.05983079597353935\n",
            "Epoch: 4, iter 2500: loss = 1.909593939781189\n",
            "Epoch: 4, iter 2600: loss = 0.06458727270364761\n",
            "Epoch: 4, iter 2700: loss = 0.5719934105873108\n",
            "Epoch: 4, iter 2800: loss = 0.090024434030056\n",
            "Epoch: 4, iter 2900: loss = 0.812097430229187\n",
            "Epoch: 4, iter 3000: loss = 0.136191725730896\n",
            "Epoch: 4, iter 3100: loss = 0.20579613745212555\n",
            "Epoch: 4, iter 3200: loss = 0.06778240948915482\n",
            "Epoch: 4, iter 3300: loss = 0.29060497879981995\n",
            "Epoch: 4, iter 3400: loss = 0.1390485167503357\n",
            "Epoch: 4, iter 3500: loss = 0.2280942052602768\n",
            "Epoch: 4, iter 3600: loss = 0.06037040054798126\n",
            "Epoch: 4, iter 3700: loss = 0.06316279619932175\n",
            "Epoch: 4, iter 3800: loss = 0.8891441822052002\n",
            "Epoch: 4, iter 3900: loss = 0.011060312390327454\n",
            "Epoch: 4, iter 4000: loss = 0.6665213108062744\n",
            "Epoch: 4, iter 4100: loss = 0.2660798728466034\n",
            "Epoch: 4, iter 4200: loss = 0.26701635122299194\n",
            "Epoch: 4, iter 4300: loss = 0.7478453516960144\n",
            "Epoch: 4, iter 4400: loss = 0.45143213868141174\n",
            "Epoch: 4, iter 4500: loss = 0.30495816469192505\n",
            "Epoch: 4, iter 4600: loss = 0.06957769393920898\n",
            "Epoch: 4, iter 4700: loss = 0.04715067520737648\n",
            "Epoch: 4, iter 4800: loss = 0.024480553343892097\n",
            "Epoch: 4, iter 4900: loss = 0.28440219163894653\n",
            "Epoch: 4, iter 5000: loss = 0.320819228887558\n",
            "Epoch: 4, iter 5100: loss = 0.12812066078186035\n",
            "Epoch: 4, iter 5200: loss = 0.022007418796420097\n",
            "Epoch: 4, iter 5300: loss = 0.34525951743125916\n",
            "Epoch: 4, iter 5400: loss = 0.027334466576576233\n",
            "Epoch: 4, iter 5500: loss = 1.983237624168396\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.81it/s]\n",
            "Epoch 4 loss average: 0.334\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7510    0.6064    0.6710     11182\n",
            "           2     0.4818    0.1362    0.2124       969\n",
            "           3     0.6852    0.5850    0.6312      1600\n",
            "           4     0.4857    0.2672    0.3448       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2143    0.0198    0.0363       303\n",
            "\n",
            "   micro avg     0.7242    0.5374    0.6170     15027\n",
            "   macro avg     0.4363    0.2691    0.3159     15027\n",
            "weighted avg     0.6939    0.5374    0.5999     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6809    0.4544    0.5450      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.4831    0.4914    0.4872       116\n",
            "           4     0.3478    0.1356    0.1951       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6343    0.3777    0.4735      1419\n",
            "   macro avg     0.2520    0.1802    0.2046      1419\n",
            "weighted avg     0.5574    0.3777    0.4474      1419\n",
            "\n",
            "0.33351599410476834 0.5999279200976834 0.5124544226494269 0.44744129680215344\n",
            "Patience counter: 3\n",
            "Epoch: 5, iter 0: loss = 0.15420660376548767\n",
            "Epoch: 5, iter 100: loss = 0.19968478381633759\n",
            "Epoch: 5, iter 200: loss = 0.20909932255744934\n",
            "Epoch: 5, iter 300: loss = 0.4409899413585663\n",
            "Epoch: 5, iter 400: loss = 0.2918791174888611\n",
            "Epoch: 5, iter 500: loss = 0.05509353429079056\n",
            "Epoch: 5, iter 600: loss = 0.6141037940979004\n",
            "Epoch: 5, iter 700: loss = 0.5025044083595276\n",
            "Epoch: 5, iter 800: loss = 0.013526517897844315\n",
            "Epoch: 5, iter 900: loss = 0.04284314066171646\n",
            "Epoch: 5, iter 1000: loss = 0.028632301837205887\n",
            "Epoch: 5, iter 1100: loss = 0.5372219681739807\n",
            "Epoch: 5, iter 1200: loss = 0.2180355042219162\n",
            "Epoch: 5, iter 1300: loss = 0.07925727218389511\n",
            "Epoch: 5, iter 1400: loss = 1.313437819480896\n",
            "Epoch: 5, iter 1500: loss = 0.8688900470733643\n",
            "Epoch: 5, iter 1600: loss = 0.016679493710398674\n",
            "Epoch: 5, iter 1700: loss = 0.5715559720993042\n",
            "Epoch: 5, iter 1800: loss = 0.22897517681121826\n",
            "Epoch: 5, iter 1900: loss = 0.0023005083203315735\n",
            "Epoch: 5, iter 2000: loss = 0.015355929732322693\n",
            "Epoch: 5, iter 2100: loss = 1.1798042058944702\n",
            "Epoch: 5, iter 2200: loss = 0.2298719733953476\n",
            "Epoch: 5, iter 2300: loss = 0.6756673455238342\n",
            "Epoch: 5, iter 2400: loss = 0.3532434105873108\n",
            "Epoch: 5, iter 2500: loss = 0.35643064975738525\n",
            "Epoch: 5, iter 2600: loss = 0.32894572615623474\n",
            "Epoch: 5, iter 2700: loss = 0.5701043605804443\n",
            "Epoch: 5, iter 2800: loss = 0.20694464445114136\n",
            "Epoch: 5, iter 2900: loss = 0.13929423689842224\n",
            "Epoch: 5, iter 3000: loss = 0.7705321907997131\n",
            "Epoch: 5, iter 3100: loss = 0.11900436133146286\n",
            "Epoch: 5, iter 3200: loss = 0.3793567717075348\n",
            "Epoch: 5, iter 3300: loss = 0.007652300875633955\n",
            "Epoch: 5, iter 3400: loss = 0.01188637688755989\n",
            "Epoch: 5, iter 3500: loss = 0.9798296093940735\n",
            "Epoch: 5, iter 3600: loss = 0.004158885683864355\n",
            "Epoch: 5, iter 3700: loss = 0.14723193645477295\n",
            "Epoch: 5, iter 3800: loss = 0.22915561497211456\n",
            "Epoch: 5, iter 3900: loss = 0.0644967257976532\n",
            "Epoch: 5, iter 4000: loss = 0.5905956625938416\n",
            "Epoch: 5, iter 4100: loss = 0.05285932496190071\n",
            "Epoch: 5, iter 4200: loss = 0.15035659074783325\n",
            "Epoch: 5, iter 4300: loss = 0.004272844176739454\n",
            "Epoch: 5, iter 4400: loss = 0.13069318234920502\n",
            "Epoch: 5, iter 4500: loss = 0.011370335705578327\n",
            "Epoch: 5, iter 4600: loss = 0.01492011733353138\n",
            "Epoch: 5, iter 4700: loss = 0.14791801571846008\n",
            "Epoch: 5, iter 4800: loss = 0.5311570167541504\n",
            "Epoch: 5, iter 4900: loss = 0.029521748423576355\n",
            "Epoch: 5, iter 5000: loss = 0.3539160490036011\n",
            "Epoch: 5, iter 5100: loss = 0.01902604289352894\n",
            "Epoch: 5, iter 5200: loss = 0.15696346759796143\n",
            "Epoch: 5, iter 5300: loss = 0.5570484399795532\n",
            "Epoch: 5, iter 5400: loss = 0.3781029284000397\n",
            "Epoch: 5, iter 5500: loss = 0.636568009853363\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:13<00:00, 12.81it/s]\n",
            "Epoch 5 loss average: 0.307\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7723    0.6441    0.7024     11182\n",
            "           2     0.5419    0.2270    0.3200       969\n",
            "           3     0.6922    0.5875    0.6356      1600\n",
            "           4     0.5155    0.3011    0.3802       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3200    0.0792    0.1270       303\n",
            "\n",
            "   micro avg     0.7413    0.5746    0.6474     15027\n",
            "   macro avg     0.4737    0.3065    0.3608     15027\n",
            "weighted avg     0.7182    0.5746    0.6345     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6797    0.4769    0.5606      1019\n",
            "           2     0.3649    0.2647    0.3068       102\n",
            "           3     0.5357    0.3879    0.4500       116\n",
            "           4     0.3636    0.1356    0.1975       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.4000    0.0426    0.0769        47\n",
            "\n",
            "   micro avg     0.6247    0.4059    0.4921      1419\n",
            "   macro avg     0.3907    0.2180    0.2653      1419\n",
            "weighted avg     0.6016    0.4059    0.4804      1419\n",
            "\n",
            "0.30675987693591344 0.6344592316954786 0.5292767329444337 0.4803549213426939\n",
            "Patience counter: 4\n",
            "Epoch: 6, iter 0: loss = 0.6302440166473389\n",
            "Epoch: 6, iter 100: loss = 0.008897866122424603\n",
            "Epoch: 6, iter 200: loss = 0.7275100350379944\n",
            "Epoch: 6, iter 300: loss = 0.2609294652938843\n",
            "Epoch: 6, iter 400: loss = 0.15792356431484222\n",
            "Epoch: 6, iter 500: loss = 0.1559993177652359\n",
            "Epoch: 6, iter 600: loss = 0.35780277848243713\n",
            "Epoch: 6, iter 700: loss = 0.6142519116401672\n",
            "Epoch: 6, iter 800: loss = 0.005132464226335287\n",
            "Epoch: 6, iter 900: loss = 0.2249547839164734\n",
            "Epoch: 6, iter 1000: loss = 0.10585634410381317\n",
            "Epoch: 6, iter 1100: loss = 0.00795508362352848\n",
            "Epoch: 6, iter 1200: loss = 0.024267025291919708\n",
            "Epoch: 6, iter 1300: loss = 0.046138107776641846\n",
            "Epoch: 6, iter 1400: loss = 0.006425709929317236\n",
            "Epoch: 6, iter 1500: loss = 1.0140618085861206\n",
            "Epoch: 6, iter 1600: loss = 0.15449801087379456\n",
            "Epoch: 6, iter 1700: loss = 0.3688892424106598\n",
            "Epoch: 6, iter 1800: loss = 0.028460798785090446\n",
            "Epoch: 6, iter 1900: loss = 0.17820918560028076\n",
            "Epoch: 6, iter 2000: loss = 1.2173783779144287\n",
            "Epoch: 6, iter 2100: loss = 0.0829925388097763\n",
            "Epoch: 6, iter 2200: loss = 0.4433773159980774\n",
            "Epoch: 6, iter 2300: loss = 1.0915501117706299\n",
            "Epoch: 6, iter 2400: loss = 0.535530149936676\n",
            "Epoch: 6, iter 2500: loss = 0.07887132465839386\n",
            "Epoch: 6, iter 2600: loss = 0.584457278251648\n",
            "Epoch: 6, iter 2700: loss = 0.21752306818962097\n",
            "Epoch: 6, iter 2800: loss = 0.04309628903865814\n",
            "Epoch: 6, iter 2900: loss = 0.2788783311843872\n",
            "Epoch: 6, iter 3000: loss = 0.04703061655163765\n",
            "Epoch: 6, iter 3100: loss = 0.030378036201000214\n",
            "Epoch: 6, iter 3200: loss = 0.04528990760445595\n",
            "Epoch: 6, iter 3300: loss = 0.44946083426475525\n",
            "Epoch: 6, iter 3400: loss = 0.38634049892425537\n",
            "Epoch: 6, iter 3500: loss = 0.24218223989009857\n",
            "Epoch: 6, iter 3600: loss = 0.16878536343574524\n",
            "Epoch: 6, iter 3700: loss = 0.23681879043579102\n",
            "Epoch: 6, iter 3800: loss = 0.1788734346628189\n",
            "Epoch: 6, iter 3900: loss = 0.2743668258190155\n",
            "Epoch: 6, iter 4000: loss = 0.2441321611404419\n",
            "Epoch: 6, iter 4100: loss = 0.1752685308456421\n",
            "Epoch: 6, iter 4200: loss = 0.5217499732971191\n",
            "Epoch: 6, iter 4300: loss = 0.016214199364185333\n",
            "Epoch: 6, iter 4400: loss = 0.5255782604217529\n",
            "Epoch: 6, iter 4500: loss = 0.37197378277778625\n",
            "Epoch: 6, iter 4600: loss = 0.06073427200317383\n",
            "Epoch: 6, iter 4700: loss = 0.03306331858038902\n",
            "Epoch: 6, iter 4800: loss = 1.9009746313095093\n",
            "Epoch: 6, iter 4900: loss = 0.005337563809007406\n",
            "Epoch: 6, iter 5000: loss = 0.3590940535068512\n",
            "Epoch: 6, iter 5100: loss = 0.017823707312345505\n",
            "Epoch: 6, iter 5200: loss = 0.33822816610336304\n",
            "Epoch: 6, iter 5300: loss = 1.5487040281295776\n",
            "Epoch: 6, iter 5400: loss = 0.024733444675803185\n",
            "Epoch: 6, iter 5500: loss = 0.005783254746347666\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:13<00:00, 12.82it/s]\n",
            "Epoch 6 loss average: 0.293\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7775    0.6795    0.7252     11182\n",
            "           2     0.5774    0.2693    0.3673       969\n",
            "           3     0.7116    0.6169    0.6609      1600\n",
            "           4     0.5816    0.3748    0.4559       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3604    0.1320    0.1932       303\n",
            "\n",
            "   micro avg     0.7501    0.6120    0.6740     15027\n",
            "   macro avg     0.5014    0.3454    0.4004     15027\n",
            "weighted avg     0.7309    0.6120    0.6627     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6481    0.5025    0.5661      1019\n",
            "           2     0.4348    0.1961    0.2703       102\n",
            "           3     0.4405    0.3190    0.3700       116\n",
            "           4     0.3143    0.2797    0.2960       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3333    0.0213    0.0400        47\n",
            "\n",
            "   micro avg     0.5866    0.4249    0.4928      1419\n",
            "   macro avg     0.3618    0.2197    0.2570      1419\n",
            "weighted avg     0.5698    0.4249    0.4821      1419\n",
            "\n",
            "0.29321473946325854 0.6626856963542379 0.5065288028006647 0.4821036248170054\n",
            "Patience counter: 5\n",
            "Epoch: 7, iter 0: loss = 0.16929106414318085\n",
            "Epoch: 7, iter 100: loss = 0.0029832154978066683\n",
            "Epoch: 7, iter 200: loss = 0.4827723801136017\n",
            "Epoch: 7, iter 300: loss = 0.9587228298187256\n",
            "Epoch: 7, iter 400: loss = 0.005572889000177383\n",
            "Epoch: 7, iter 500: loss = 0.010443860664963722\n",
            "Epoch: 7, iter 600: loss = 0.052882321178913116\n",
            "Epoch: 7, iter 700: loss = 0.3408659100532532\n",
            "Epoch: 7, iter 800: loss = 0.1026841402053833\n",
            "Epoch: 7, iter 900: loss = 0.24835895001888275\n",
            "Epoch: 7, iter 1000: loss = 0.03885399550199509\n",
            "Epoch: 7, iter 1100: loss = 0.32355257868766785\n",
            "Epoch: 7, iter 1200: loss = 0.008515412919223309\n",
            "Epoch: 7, iter 1300: loss = 0.06323880702257156\n",
            "Epoch: 7, iter 1400: loss = 0.06848926842212677\n",
            "Epoch: 7, iter 1500: loss = 0.07050106674432755\n",
            "Epoch: 7, iter 1600: loss = 0.08217255026102066\n",
            "Epoch: 7, iter 1700: loss = 0.006316805258393288\n",
            "Epoch: 7, iter 1800: loss = 0.2445758581161499\n",
            "Epoch: 7, iter 1900: loss = 0.1330224871635437\n",
            "Epoch: 7, iter 2000: loss = 0.02335108444094658\n",
            "Epoch: 7, iter 2100: loss = 0.07962574064731598\n",
            "Epoch: 7, iter 2200: loss = 0.44582122564315796\n",
            "Epoch: 7, iter 2300: loss = 0.09165114909410477\n",
            "Epoch: 7, iter 2400: loss = 0.015612591058015823\n",
            "Epoch: 7, iter 2500: loss = 0.010692217387259007\n",
            "Epoch: 7, iter 2600: loss = 0.28656429052352905\n",
            "Epoch: 7, iter 2700: loss = 0.006893203593790531\n",
            "Epoch: 7, iter 2800: loss = 0.9030489325523376\n",
            "Epoch: 7, iter 2900: loss = 0.6792701482772827\n",
            "Epoch: 7, iter 3000: loss = 0.4173584282398224\n",
            "Epoch: 7, iter 3100: loss = 0.4142690896987915\n",
            "Epoch: 7, iter 3200: loss = 0.06901925802230835\n",
            "Epoch: 7, iter 3300: loss = 0.31373196840286255\n",
            "Epoch: 7, iter 3400: loss = 0.07501067966222763\n",
            "Epoch: 7, iter 3500: loss = 1.6907230615615845\n",
            "Epoch: 7, iter 3600: loss = 0.36454710364341736\n",
            "Epoch: 7, iter 3700: loss = 0.3607938289642334\n",
            "Epoch: 7, iter 3800: loss = 0.0010540608782321215\n",
            "Epoch: 7, iter 3900: loss = 0.768937349319458\n",
            "Epoch: 7, iter 4000: loss = 0.0020546342711895704\n",
            "Epoch: 7, iter 4100: loss = 0.2008986473083496\n",
            "Epoch: 7, iter 4200: loss = 0.2610129415988922\n",
            "Epoch: 7, iter 4300: loss = 0.4222061336040497\n",
            "Epoch: 7, iter 4400: loss = 0.7017379999160767\n",
            "Epoch: 7, iter 4500: loss = 0.0072261737659573555\n",
            "Epoch: 7, iter 4600: loss = 0.31772249937057495\n",
            "Epoch: 7, iter 4700: loss = 0.05280934274196625\n",
            "Epoch: 7, iter 4800: loss = 0.7354646325111389\n",
            "Epoch: 7, iter 4900: loss = 0.2125832885503769\n",
            "Epoch: 7, iter 5000: loss = 0.38400450348854065\n",
            "Epoch: 7, iter 5100: loss = 0.01954479329288006\n",
            "Epoch: 7, iter 5200: loss = 0.015741614624857903\n",
            "Epoch: 7, iter 5300: loss = 0.05187833681702614\n",
            "Epoch: 7, iter 5400: loss = 0.29323554039001465\n",
            "Epoch: 7, iter 5500: loss = 0.3136105239391327\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.85it/s]\n",
            "Epoch 7 loss average: 0.269\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8052    0.7142    0.7570     11182\n",
            "           2     0.6194    0.3695    0.4628       969\n",
            "           3     0.7252    0.6350    0.6771      1600\n",
            "           4     0.5868    0.4293    0.4958       827\n",
            "           5     0.4000    0.0137    0.0265       146\n",
            "           6     0.3769    0.1617    0.2263       303\n",
            "\n",
            "   micro avg     0.7728    0.6499    0.7060     15027\n",
            "   macro avg     0.5856    0.3872    0.4409     15027\n",
            "weighted avg     0.7601    0.6499    0.6973     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6391    0.5388    0.5847      1019\n",
            "           2     0.3457    0.2745    0.3060       102\n",
            "           3     0.3500    0.5431    0.4257       116\n",
            "           4     0.4219    0.2288    0.2967       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.4545    0.1064    0.1724        47\n",
            "\n",
            "   micro avg     0.5619    0.4736    0.5140      1419\n",
            "   macro avg     0.3685    0.2819    0.2976      1419\n",
            "weighted avg     0.5626    0.4736    0.5070      1419\n",
            "\n",
            "0.26940031469523923 0.6973273835095486 0.5470905498966125 0.5070324829870507\n",
            "Patience counter: 6\n",
            "Epoch: 8, iter 0: loss = 0.14532026648521423\n",
            "Epoch: 8, iter 100: loss = 0.3404214382171631\n",
            "Epoch: 8, iter 200: loss = 0.22002986073493958\n",
            "Epoch: 8, iter 300: loss = 0.1403004229068756\n",
            "Epoch: 8, iter 400: loss = 0.00047305470798164606\n",
            "Epoch: 8, iter 500: loss = 0.19842444360256195\n",
            "Epoch: 8, iter 600: loss = 0.7983769774436951\n",
            "Epoch: 8, iter 700: loss = 0.100155308842659\n",
            "Epoch: 8, iter 800: loss = 0.29428529739379883\n",
            "Epoch: 8, iter 900: loss = 0.691423237323761\n",
            "Epoch: 8, iter 1000: loss = 0.0010979280341416597\n",
            "Epoch: 8, iter 1100: loss = 0.05987638235092163\n",
            "Epoch: 8, iter 1200: loss = 0.0034420364536345005\n",
            "Epoch: 8, iter 1300: loss = 1.0102397203445435\n",
            "Epoch: 8, iter 1400: loss = 0.5625890493392944\n",
            "Epoch: 8, iter 1500: loss = 0.0020983032882213593\n",
            "Epoch: 8, iter 1600: loss = 0.02104336768388748\n",
            "Epoch: 8, iter 1700: loss = 0.5075452327728271\n",
            "Epoch: 8, iter 1800: loss = 0.3838948607444763\n",
            "Epoch: 8, iter 1900: loss = 0.8453862071037292\n",
            "Epoch: 8, iter 2000: loss = 0.007897697389125824\n",
            "Epoch: 8, iter 2100: loss = 0.0024538803845643997\n",
            "Epoch: 8, iter 2200: loss = 0.07624626904726028\n",
            "Epoch: 8, iter 2300: loss = 0.0005871637258678675\n",
            "Epoch: 8, iter 2400: loss = 0.16601331532001495\n",
            "Epoch: 8, iter 2500: loss = 0.42604249715805054\n",
            "Epoch: 8, iter 2600: loss = 0.03409068286418915\n",
            "Epoch: 8, iter 2700: loss = 0.13080182671546936\n",
            "Epoch: 8, iter 2800: loss = 0.019220491871237755\n",
            "Epoch: 8, iter 2900: loss = 0.3072507977485657\n",
            "Epoch: 8, iter 3000: loss = 0.11384733021259308\n",
            "Epoch: 8, iter 3100: loss = 0.30144110321998596\n",
            "Epoch: 8, iter 3200: loss = 0.040661584585905075\n",
            "Epoch: 8, iter 3300: loss = 0.005788564682006836\n",
            "Epoch: 8, iter 3400: loss = 0.06644611805677414\n",
            "Epoch: 8, iter 3500: loss = 0.6871063709259033\n",
            "Epoch: 8, iter 3600: loss = 0.0005768486298620701\n",
            "Epoch: 8, iter 3700: loss = 0.3110146224498749\n",
            "Epoch: 8, iter 3800: loss = 0.1022915169596672\n",
            "Epoch: 8, iter 3900: loss = 0.6263940334320068\n",
            "Epoch: 8, iter 4000: loss = 0.0838967114686966\n",
            "Epoch: 8, iter 4100: loss = 0.379264235496521\n",
            "Epoch: 8, iter 4200: loss = 0.036658380180597305\n",
            "Epoch: 8, iter 4300: loss = 0.4689742922782898\n",
            "Epoch: 8, iter 4400: loss = 0.003194723278284073\n",
            "Epoch: 8, iter 4500: loss = 0.88490891456604\n",
            "Epoch: 8, iter 4600: loss = 0.04641158506274223\n",
            "Epoch: 8, iter 4700: loss = 0.007526409812271595\n",
            "Epoch: 8, iter 4800: loss = 0.19071075320243835\n",
            "Epoch: 8, iter 4900: loss = 0.04113497957587242\n",
            "Epoch: 8, iter 5000: loss = 0.16815105080604553\n",
            "Epoch: 8, iter 5100: loss = 0.10839233547449112\n",
            "Epoch: 8, iter 5200: loss = 0.06557479500770569\n",
            "Epoch: 8, iter 5300: loss = 0.22595688700675964\n",
            "Epoch: 8, iter 5400: loss = 0.30762428045272827\n",
            "Epoch: 8, iter 5500: loss = 0.481993168592453\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.85it/s]\n",
            "Epoch 8 loss average: 0.256\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8152    0.7260    0.7680     11182\n",
            "           2     0.6192    0.3860    0.4755       969\n",
            "           3     0.7444    0.6625    0.7011      1600\n",
            "           4     0.6468    0.4341    0.5195       827\n",
            "           5     0.4000    0.0137    0.0265       146\n",
            "           6     0.4226    0.2343    0.3015       303\n",
            "\n",
            "   micro avg     0.7853    0.6644    0.7198     15027\n",
            "   macro avg     0.6080    0.4094    0.4654     15027\n",
            "weighted avg     0.7738    0.6644    0.7117     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6352    0.5025    0.5611      1019\n",
            "           2     0.4545    0.0980    0.1613       102\n",
            "           3     0.4545    0.4310    0.4425       116\n",
            "           4     0.4737    0.2288    0.3086       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.2778    0.1064    0.1538        47\n",
            "\n",
            "   micro avg     0.5951    0.4257    0.4963      1419\n",
            "   macro avg     0.3826    0.2278    0.2712      1419\n",
            "weighted avg     0.5746    0.4257    0.4815      1419\n",
            "\n",
            "0.2564959909892165 0.7117442684076494 0.5057372305564005 0.4814502865838684\n",
            "Patience counter: 7\n",
            "Epoch: 9, iter 0: loss = 0.36931583285331726\n",
            "Epoch: 9, iter 100: loss = 0.0017440075753256679\n",
            "Epoch: 9, iter 200: loss = 0.0051505728624761105\n",
            "Epoch: 9, iter 300: loss = 0.11693819612264633\n",
            "Epoch: 9, iter 400: loss = 0.5052677392959595\n",
            "Epoch: 9, iter 500: loss = 0.11911505460739136\n",
            "Epoch: 9, iter 600: loss = 0.0255657359957695\n",
            "Epoch: 9, iter 700: loss = 0.11983801424503326\n",
            "Epoch: 9, iter 800: loss = 0.8852786421775818\n",
            "Epoch: 9, iter 900: loss = 0.13989707827568054\n",
            "Epoch: 9, iter 1000: loss = 0.4810371398925781\n",
            "Epoch: 9, iter 1100: loss = 0.5428798794746399\n",
            "Epoch: 9, iter 1200: loss = 0.001500534126535058\n",
            "Epoch: 9, iter 1300: loss = 0.12619087100028992\n",
            "Epoch: 9, iter 1400: loss = 0.018848806619644165\n",
            "Epoch: 9, iter 1500: loss = 0.30108442902565\n",
            "Epoch: 9, iter 1600: loss = 0.5003436803817749\n",
            "Epoch: 9, iter 1700: loss = 0.023345546796917915\n",
            "Epoch: 9, iter 1800: loss = 0.001039609545841813\n",
            "Epoch: 9, iter 1900: loss = 0.06012560427188873\n",
            "Epoch: 9, iter 2000: loss = 0.023931965231895447\n",
            "Epoch: 9, iter 2100: loss = 0.27767378091812134\n",
            "Epoch: 9, iter 2200: loss = 0.13744404911994934\n",
            "Epoch: 9, iter 2300: loss = 0.053966403007507324\n",
            "Epoch: 9, iter 2400: loss = 0.0030495785176753998\n",
            "Epoch: 9, iter 2500: loss = 0.3411765694618225\n",
            "Epoch: 9, iter 2600: loss = 0.406297892332077\n",
            "Epoch: 9, iter 2700: loss = 0.4683515727519989\n",
            "Epoch: 9, iter 2800: loss = 0.013886814005672932\n",
            "Epoch: 9, iter 2900: loss = 0.1955825388431549\n",
            "Epoch: 9, iter 3000: loss = 0.15527623891830444\n",
            "Epoch: 9, iter 3100: loss = 0.001978288870304823\n",
            "Epoch: 9, iter 3200: loss = 0.3032291829586029\n",
            "Epoch: 9, iter 3300: loss = 0.3554743528366089\n",
            "Epoch: 9, iter 3400: loss = 0.09230645000934601\n",
            "Epoch: 9, iter 3500: loss = 0.11574669927358627\n",
            "Epoch: 9, iter 3600: loss = 0.06148120388388634\n",
            "Epoch: 9, iter 3700: loss = 0.15688231587409973\n",
            "Epoch: 9, iter 3800: loss = 0.0016908012330532074\n",
            "Epoch: 9, iter 3900: loss = 0.0013200219254940748\n",
            "Epoch: 9, iter 4000: loss = 0.0003380477137397975\n",
            "Epoch: 9, iter 4100: loss = 0.26789504289627075\n",
            "Epoch: 9, iter 4200: loss = 0.0056000035256147385\n",
            "Epoch: 9, iter 4300: loss = 0.07154591381549835\n",
            "Epoch: 9, iter 4400: loss = 0.0003467962087597698\n",
            "Epoch: 9, iter 4500: loss = 0.4236915111541748\n",
            "Epoch: 9, iter 4600: loss = 0.5651856660842896\n",
            "Epoch: 9, iter 4700: loss = 0.16679005324840546\n",
            "Epoch: 9, iter 4800: loss = 0.13419197499752045\n",
            "Epoch: 9, iter 4900: loss = 0.11589808017015457\n",
            "Epoch: 9, iter 5000: loss = 0.01655917428433895\n",
            "Epoch: 9, iter 5100: loss = 0.008504425175487995\n",
            "Epoch: 9, iter 5200: loss = 0.20050571858882904\n",
            "Epoch: 9, iter 5300: loss = 0.0019738953560590744\n",
            "Epoch: 9, iter 5400: loss = 0.060146573930978775\n",
            "Epoch: 9, iter 5500: loss = 0.21603290736675262\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.85it/s]\n",
            "Epoch 9 loss average: 0.244\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8268    0.7525    0.7879     11182\n",
            "           2     0.6511    0.4448    0.5285       969\n",
            "           3     0.7558    0.6906    0.7218      1600\n",
            "           4     0.6455    0.4667    0.5418       827\n",
            "           5     0.1250    0.0068    0.0130       146\n",
            "           6     0.4450    0.3201    0.3724       303\n",
            "\n",
            "   micro avg     0.7950    0.6944    0.7413     15027\n",
            "   macro avg     0.5748    0.4469    0.4942     15027\n",
            "weighted avg     0.7834    0.6944    0.7347     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6595    0.5093    0.5748      1019\n",
            "           2     0.2451    0.2451    0.2451       102\n",
            "           3     0.4118    0.4224    0.4170       116\n",
            "           4     0.3750    0.1271    0.1899       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3750    0.0638    0.1091        47\n",
            "\n",
            "   micro avg     0.5786    0.4306    0.4937      1419\n",
            "   macro avg     0.3444    0.2280    0.2560      1419\n",
            "weighted avg     0.5685    0.4306    0.4838      1419\n",
            "\n",
            "0.24355680461598975 0.7346919491926956 0.5242184864678622 0.48384630052785993\n",
            "Patience counter: 8\n",
            "Epoch: 10, iter 0: loss = 0.19468308985233307\n",
            "Epoch: 10, iter 100: loss = 0.005262297112494707\n",
            "Epoch: 10, iter 200: loss = 0.009024301543831825\n",
            "Epoch: 10, iter 300: loss = 0.009910108521580696\n",
            "Epoch: 10, iter 400: loss = 0.007334063295274973\n",
            "Epoch: 10, iter 500: loss = 0.024881182238459587\n",
            "Epoch: 10, iter 600: loss = 0.11312038451433182\n",
            "Epoch: 10, iter 700: loss = 0.20818643271923065\n",
            "Epoch: 10, iter 800: loss = 0.1634279191493988\n",
            "Epoch: 10, iter 900: loss = 0.011271222494542599\n",
            "Epoch: 10, iter 1000: loss = 0.0010071074357256293\n",
            "Epoch: 10, iter 1100: loss = 0.8144277930259705\n",
            "Epoch: 10, iter 1200: loss = 0.12084411084651947\n",
            "Epoch: 10, iter 1300: loss = 0.009327859617769718\n",
            "Epoch: 10, iter 1400: loss = 0.4327142536640167\n",
            "Epoch: 10, iter 1500: loss = 0.0705891028046608\n",
            "Epoch: 10, iter 1600: loss = 0.06409355998039246\n",
            "Epoch: 10, iter 1700: loss = 0.0013808250660076737\n",
            "Epoch: 10, iter 1800: loss = 0.5673871040344238\n",
            "Epoch: 10, iter 1900: loss = 0.6240591406822205\n",
            "Epoch: 10, iter 2000: loss = 0.0012994029093533754\n",
            "Epoch: 10, iter 2100: loss = 0.05611409991979599\n",
            "Epoch: 10, iter 2200: loss = 0.3164595365524292\n",
            "Epoch: 10, iter 2300: loss = 0.0021057932171970606\n",
            "Epoch: 10, iter 2400: loss = 0.015574161894619465\n",
            "Epoch: 10, iter 2500: loss = 0.08292733877897263\n",
            "Epoch: 10, iter 2600: loss = 0.21082091331481934\n",
            "Epoch: 10, iter 2700: loss = 0.02377467416226864\n",
            "Epoch: 10, iter 2800: loss = 0.2968630790710449\n",
            "Epoch: 10, iter 2900: loss = 0.0056282165460288525\n",
            "Epoch: 10, iter 3000: loss = 0.12990380823612213\n",
            "Epoch: 10, iter 3100: loss = 0.007799468468874693\n",
            "Epoch: 10, iter 3200: loss = 0.26948311924934387\n",
            "Epoch: 10, iter 3300: loss = 1.186339259147644\n",
            "Epoch: 10, iter 3400: loss = 0.9259322881698608\n",
            "Epoch: 10, iter 3500: loss = 0.37697017192840576\n",
            "Epoch: 10, iter 3600: loss = 0.0033960100263357162\n",
            "Epoch: 10, iter 3700: loss = 0.4553644359111786\n",
            "Epoch: 10, iter 3800: loss = 0.0005920567782595754\n",
            "Epoch: 10, iter 3900: loss = 0.5200440883636475\n",
            "Epoch: 10, iter 4000: loss = 0.4632568955421448\n",
            "Epoch: 10, iter 4100: loss = 0.41800227761268616\n",
            "Epoch: 10, iter 4200: loss = 0.11060589551925659\n",
            "Epoch: 10, iter 4300: loss = 0.809452474117279\n",
            "Epoch: 10, iter 4400: loss = 0.0017195146065205336\n",
            "Epoch: 10, iter 4500: loss = 0.004744121804833412\n",
            "Epoch: 10, iter 4600: loss = 0.24744798243045807\n",
            "Epoch: 10, iter 4700: loss = 0.7745682597160339\n",
            "Epoch: 10, iter 4800: loss = 0.1024758368730545\n",
            "Epoch: 10, iter 4900: loss = 0.04327787831425667\n",
            "Epoch: 10, iter 5000: loss = 0.08741354197263718\n",
            "Epoch: 10, iter 5100: loss = 0.597855269908905\n",
            "Epoch: 10, iter 5200: loss = 0.45243990421295166\n",
            "Epoch: 10, iter 5300: loss = 0.018220283091068268\n",
            "Epoch: 10, iter 5400: loss = 0.0022458299063146114\n",
            "Epoch: 10, iter 5500: loss = 0.24738071858882904\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.87it/s]\n",
            "Epoch 10 loss average: 0.230\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8410    0.7665    0.8020     11182\n",
            "           2     0.6582    0.4572    0.5396       969\n",
            "           3     0.7773    0.6937    0.7332      1600\n",
            "           4     0.7048    0.5284    0.6040       827\n",
            "           5     0.6667    0.0959    0.1677       146\n",
            "           6     0.5440    0.3465    0.4234       303\n",
            "\n",
            "   micro avg     0.8137    0.7107    0.7587     15027\n",
            "   macro avg     0.6987    0.4814    0.5450     15027\n",
            "weighted avg     0.8073    0.7107    0.7531     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6355    0.4671    0.5385      1019\n",
            "           2     0.4884    0.2059    0.2897       102\n",
            "           3     0.4080    0.4397    0.4232       116\n",
            "           4     0.3871    0.3051    0.3412       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.4091    0.1915    0.2609        47\n",
            "\n",
            "   micro avg     0.5746    0.4179    0.4839      1419\n",
            "   macro avg     0.3880    0.2682    0.3089      1419\n",
            "weighted avg     0.5706    0.4179    0.4791      1419\n",
            "\n",
            "0.23023207073103755 0.7530845290000464 0.5111235657981542 0.47911123563916924\n",
            "Patience counter: 9\n",
            "Epoch: 11, iter 0: loss = 0.01212836429476738\n",
            "Epoch: 11, iter 100: loss = 0.00339115085080266\n",
            "Epoch: 11, iter 200: loss = 0.2452113777399063\n",
            "Epoch: 11, iter 300: loss = 0.0563361719250679\n",
            "Epoch: 11, iter 400: loss = 0.09510001540184021\n",
            "Epoch: 11, iter 500: loss = 0.050641704350709915\n",
            "Epoch: 11, iter 600: loss = 0.08688810467720032\n",
            "Epoch: 11, iter 700: loss = 0.0011918236268684268\n",
            "Epoch: 11, iter 800: loss = 0.1842186599969864\n",
            "Epoch: 11, iter 900: loss = 0.01704900525510311\n",
            "Epoch: 11, iter 1000: loss = 0.00205142330378294\n",
            "Epoch: 11, iter 1100: loss = 0.17725160717964172\n",
            "Epoch: 11, iter 1200: loss = 0.5953284502029419\n",
            "Epoch: 11, iter 1300: loss = 0.652443528175354\n",
            "Epoch: 11, iter 1400: loss = 0.27598586678504944\n",
            "Epoch: 11, iter 1500: loss = 0.006651368923485279\n",
            "Epoch: 11, iter 1600: loss = 0.02780510112643242\n",
            "Epoch: 11, iter 1700: loss = 0.08330757915973663\n",
            "Epoch: 11, iter 1800: loss = 0.08490834385156631\n",
            "Epoch: 11, iter 1900: loss = 0.2411595582962036\n",
            "Epoch: 11, iter 2000: loss = 0.0037015187554061413\n",
            "Epoch: 11, iter 2100: loss = 0.2241898775100708\n",
            "Epoch: 11, iter 2200: loss = 0.026477424427866936\n",
            "Epoch: 11, iter 2300: loss = 0.29767242074012756\n",
            "Epoch: 11, iter 2400: loss = 0.10105855762958527\n",
            "Epoch: 11, iter 2500: loss = 0.7861726880073547\n",
            "Epoch: 11, iter 2600: loss = 0.9487400054931641\n",
            "Epoch: 11, iter 2700: loss = 0.3657359480857849\n",
            "Epoch: 11, iter 2800: loss = 0.5290142893791199\n",
            "Epoch: 11, iter 2900: loss = 0.5245790481567383\n",
            "Epoch: 11, iter 3000: loss = 0.28748366236686707\n",
            "Epoch: 11, iter 3100: loss = 0.24688492715358734\n",
            "Epoch: 11, iter 3200: loss = 0.5686280727386475\n",
            "Epoch: 11, iter 3300: loss = 0.15252704918384552\n",
            "Epoch: 11, iter 3400: loss = 0.0007908488041721284\n",
            "Epoch: 11, iter 3500: loss = 0.2073100060224533\n",
            "Epoch: 11, iter 3600: loss = 0.6975473165512085\n",
            "Epoch: 11, iter 3700: loss = 0.243541419506073\n",
            "Epoch: 11, iter 3800: loss = 0.00413645850494504\n",
            "Epoch: 11, iter 3900: loss = 0.7068020105361938\n",
            "Epoch: 11, iter 4000: loss = 0.002189047634601593\n",
            "Epoch: 11, iter 4100: loss = 0.41928839683532715\n",
            "Epoch: 11, iter 4200: loss = 0.3129674792289734\n",
            "Epoch: 11, iter 4300: loss = 0.0026329944375902414\n",
            "Epoch: 11, iter 4400: loss = 1.1533571481704712\n",
            "Epoch: 11, iter 4500: loss = 0.035474326461553574\n",
            "Epoch: 11, iter 4600: loss = 0.3816658854484558\n",
            "Epoch: 11, iter 4700: loss = 0.17865733802318573\n",
            "Epoch: 11, iter 4800: loss = 0.3720836639404297\n",
            "Epoch: 11, iter 4900: loss = 0.42886993288993835\n",
            "Epoch: 11, iter 5000: loss = 0.03475877270102501\n",
            "Epoch: 11, iter 5100: loss = 0.25048038363456726\n",
            "Epoch: 11, iter 5200: loss = 0.0257710050791502\n",
            "Epoch: 11, iter 5300: loss = 0.024903828278183937\n",
            "Epoch: 11, iter 5400: loss = 0.12632562220096588\n",
            "Epoch: 11, iter 5500: loss = 0.529410183429718\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:11<00:00, 12.87it/s]\n",
            "Epoch 11 loss average: 0.222\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8405    0.7786    0.8084     11182\n",
            "           2     0.6746    0.4964    0.5719       969\n",
            "           3     0.7659    0.6994    0.7311      1600\n",
            "           4     0.7100    0.5417    0.6145       827\n",
            "           5     0.4138    0.0822    0.1371       146\n",
            "           6     0.5779    0.3795    0.4582       303\n",
            "\n",
            "   micro avg     0.8126    0.7241    0.7658     15027\n",
            "   macro avg     0.6638    0.4963    0.5535     15027\n",
            "weighted avg     0.8052    0.7241    0.7606     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5969    0.5378    0.5658      1019\n",
            "           2     0.2791    0.1176    0.1655       102\n",
            "           3     0.4615    0.4138    0.4364       116\n",
            "           4     0.4127    0.2203    0.2873       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3478    0.1702    0.2286        47\n",
            "\n",
            "   micro avg     0.5578    0.4524    0.4996      1419\n",
            "   macro avg     0.3497    0.2433    0.2806      1419\n",
            "weighted avg     0.5323    0.4524    0.4854      1419\n",
            "\n",
            "0.2224792817471877 0.7606400008609273 0.5196593950223971 0.4853547806218482\n",
            "Patience counter: 10\n",
            "Epoch: 12, iter 0: loss = 0.010976285673677921\n",
            "Epoch: 12, iter 100: loss = 0.0011117703979834914\n",
            "Epoch: 12, iter 200: loss = 0.004545827396214008\n",
            "Epoch: 12, iter 300: loss = 0.27395641803741455\n",
            "Epoch: 12, iter 400: loss = 0.1309928596019745\n",
            "Epoch: 12, iter 500: loss = 0.004290292505174875\n",
            "Epoch: 12, iter 600: loss = 0.2815321683883667\n",
            "Epoch: 12, iter 700: loss = 0.11566966772079468\n",
            "Epoch: 12, iter 800: loss = 0.12496312707662582\n",
            "Epoch: 12, iter 900: loss = 0.0932607352733612\n",
            "Epoch: 12, iter 1000: loss = 0.050369057804346085\n",
            "Epoch: 12, iter 1100: loss = 0.46567097306251526\n",
            "Epoch: 12, iter 1200: loss = 0.038190051913261414\n",
            "Epoch: 12, iter 1300: loss = 0.02055835723876953\n",
            "Epoch: 12, iter 1400: loss = 0.0009644236997701228\n",
            "Epoch: 12, iter 1500: loss = 0.24813807010650635\n",
            "Epoch: 12, iter 1600: loss = 0.18976648151874542\n",
            "Epoch: 12, iter 1700: loss = 0.06410056352615356\n",
            "Epoch: 12, iter 1800: loss = 0.0002563921734690666\n",
            "Epoch: 12, iter 1900: loss = 0.14204037189483643\n",
            "Epoch: 12, iter 2000: loss = 0.16039255261421204\n",
            "Epoch: 12, iter 2100: loss = 0.20244666934013367\n",
            "Epoch: 12, iter 2200: loss = 0.01209886372089386\n",
            "Epoch: 12, iter 2300: loss = 0.18710966408252716\n",
            "Epoch: 12, iter 2400: loss = 0.0428534671664238\n",
            "Epoch: 12, iter 2500: loss = 0.6072655320167542\n",
            "Epoch: 12, iter 2600: loss = 0.3266879916191101\n",
            "Epoch: 12, iter 2700: loss = 0.2586357593536377\n",
            "Epoch: 12, iter 2800: loss = 0.36437997221946716\n",
            "Epoch: 12, iter 2900: loss = 1.97767174243927\n",
            "Epoch: 12, iter 3000: loss = 0.12016583979129791\n",
            "Epoch: 12, iter 3100: loss = 0.19221650063991547\n",
            "Epoch: 12, iter 3200: loss = 0.36370229721069336\n",
            "Epoch: 12, iter 3300: loss = 0.3394327759742737\n",
            "Epoch: 12, iter 3400: loss = 0.002402274403721094\n",
            "Epoch: 12, iter 3500: loss = 0.26780518889427185\n",
            "Epoch: 12, iter 3600: loss = 0.015290956012904644\n",
            "Epoch: 12, iter 3700: loss = 0.07736002653837204\n",
            "Epoch: 12, iter 3800: loss = 0.24640782177448273\n",
            "Epoch: 12, iter 3900: loss = 0.06367259472608566\n",
            "Epoch: 12, iter 4000: loss = 0.023751942440867424\n",
            "Epoch: 12, iter 4100: loss = 0.09623397886753082\n",
            "Epoch: 12, iter 4200: loss = 0.000802978640422225\n",
            "Epoch: 12, iter 4300: loss = 0.03213801980018616\n",
            "Epoch: 12, iter 4400: loss = 0.15525905787944794\n",
            "Epoch: 12, iter 4500: loss = 0.0011815030593425035\n",
            "Epoch: 12, iter 4600: loss = 0.02382894605398178\n",
            "Epoch: 12, iter 4700: loss = 0.18577376008033752\n",
            "Epoch: 12, iter 4800: loss = 0.018741285428404808\n",
            "Epoch: 12, iter 4900: loss = 0.5251637697219849\n",
            "Epoch: 12, iter 5000: loss = 0.0006147675449028611\n",
            "Epoch: 12, iter 5100: loss = 0.3983639180660248\n",
            "Epoch: 12, iter 5200: loss = 0.010003866627812386\n",
            "Epoch: 12, iter 5300: loss = 0.6821532845497131\n",
            "Epoch: 12, iter 5400: loss = 0.010996263474225998\n",
            "Epoch: 12, iter 5500: loss = 0.010364257730543613\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.86it/s]\n",
            "Epoch 12 loss average: 0.214\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8461    0.7899    0.8170     11182\n",
            "           2     0.7013    0.5015    0.5848       969\n",
            "           3     0.7693    0.6981    0.7320      1600\n",
            "           4     0.7215    0.5671    0.6351       827\n",
            "           5     0.5000    0.1027    0.1705       146\n",
            "           6     0.5797    0.3960    0.4706       303\n",
            "\n",
            "   micro avg     0.8195    0.7347    0.7748     15027\n",
            "   macro avg     0.6863    0.5092    0.5683     15027\n",
            "weighted avg     0.8130    0.7347    0.7697     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5895    0.5397    0.5635      1019\n",
            "           2     0.3077    0.1961    0.2395       102\n",
            "           3     0.4737    0.3879    0.4265       116\n",
            "           4     0.4416    0.2881    0.3487       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.6364    0.1489    0.2414        47\n",
            "\n",
            "   micro avg     0.5555    0.4623    0.5046      1419\n",
            "   macro avg     0.4081    0.2601    0.3033      1419\n",
            "weighted avg     0.5420    0.4623    0.4938      1419\n",
            "\n",
            "0.2141277007390317 0.7697260659304753 0.5191516746801796 0.4937525818339019\n",
            "Patience counter: 11\n",
            "Done! It took 5.6e+03 secs\n",
            "\n",
            "Current RUN: 3\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.5015028231376782\n",
            "Best test f1 weighted\n",
            "0.3950827458517002\n",
            "Best epoch\n",
            "1\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/conversation_length.pkl'),\n",
            " 'data': 'dailydialog',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/ubuntu_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 7,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [7, 256]\n",
            "\tdecoder2output.linears.0.bias\t [7]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.1 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.9777705669403076\n",
            "Epoch: 1, iter 100: loss = 1.179330587387085\n",
            "Epoch: 1, iter 200: loss = 0.32439297437667847\n",
            "Epoch: 1, iter 300: loss = 1.5182273387908936\n",
            "Epoch: 1, iter 400: loss = 0.06140017509460449\n",
            "Epoch: 1, iter 500: loss = 0.6027023792266846\n",
            "Epoch: 1, iter 600: loss = 0.4184040427207947\n",
            "Epoch: 1, iter 700: loss = 0.6204453706741333\n",
            "Epoch: 1, iter 800: loss = 0.9608938097953796\n",
            "Epoch: 1, iter 900: loss = 0.15937262773513794\n",
            "Epoch: 1, iter 1000: loss = 0.5067147016525269\n",
            "Epoch: 1, iter 1100: loss = 0.12232085317373276\n",
            "Epoch: 1, iter 1200: loss = 1.6459332704544067\n",
            "Epoch: 1, iter 1300: loss = 1.5285686254501343\n",
            "Epoch: 1, iter 1400: loss = 0.053027935326099396\n",
            "Epoch: 1, iter 1500: loss = 1.0796334743499756\n",
            "Epoch: 1, iter 1600: loss = 1.070274829864502\n",
            "Epoch: 1, iter 1700: loss = 1.6181026697158813\n",
            "Epoch: 1, iter 1800: loss = 0.2277536690235138\n",
            "Epoch: 1, iter 1900: loss = 0.43290451169013977\n",
            "Epoch: 1, iter 2000: loss = 0.10598263889551163\n",
            "Epoch: 1, iter 2100: loss = 0.1634494662284851\n",
            "Epoch: 1, iter 2200: loss = 0.2309153825044632\n",
            "Epoch: 1, iter 2300: loss = 0.11490211635828018\n",
            "Epoch: 1, iter 2400: loss = 0.10133428126573563\n",
            "Epoch: 1, iter 2500: loss = 0.04832923039793968\n",
            "Epoch: 1, iter 2600: loss = 0.193348690867424\n",
            "Epoch: 1, iter 2700: loss = 0.3411780893802643\n",
            "Epoch: 1, iter 2800: loss = 0.4999123513698578\n",
            "Epoch: 1, iter 2900: loss = 1.1772156953811646\n",
            "Epoch: 1, iter 3000: loss = 0.19761687517166138\n",
            "Epoch: 1, iter 3100: loss = 0.5102130174636841\n",
            "Epoch: 1, iter 3200: loss = 0.5251129865646362\n",
            "Epoch: 1, iter 3300: loss = 0.15887537598609924\n",
            "Epoch: 1, iter 3400: loss = 0.04051893576979637\n",
            "Epoch: 1, iter 3500: loss = 1.3529412746429443\n",
            "Epoch: 1, iter 3600: loss = 0.684109628200531\n",
            "Epoch: 1, iter 3700: loss = 0.05833646282553673\n",
            "Epoch: 1, iter 3800: loss = 0.0704919621348381\n",
            "Epoch: 1, iter 3900: loss = 0.06495670974254608\n",
            "Epoch: 1, iter 4000: loss = 1.7501541376113892\n",
            "Epoch: 1, iter 4100: loss = 0.33358633518218994\n",
            "Epoch: 1, iter 4200: loss = 0.8069770336151123\n",
            "Epoch: 1, iter 4300: loss = 0.040974196046590805\n",
            "Epoch: 1, iter 4400: loss = 0.15349863469600677\n",
            "Epoch: 1, iter 4500: loss = 1.0096697807312012\n",
            "Epoch: 1, iter 4600: loss = 0.32602059841156006\n",
            "Epoch: 1, iter 4700: loss = 0.4256368577480316\n",
            "Epoch: 1, iter 4800: loss = 0.040585145354270935\n",
            "Epoch: 1, iter 4900: loss = 0.4511931538581848\n",
            "Epoch: 1, iter 5000: loss = 0.9805992841720581\n",
            "Epoch: 1, iter 5100: loss = 0.18270660936832428\n",
            "Epoch: 1, iter 5200: loss = 0.07002916187047958\n",
            "Epoch: 1, iter 5300: loss = 1.1888842582702637\n",
            "Epoch: 1, iter 5400: loss = 0.5629007816314697\n",
            "Epoch: 1, iter 5500: loss = 0.45565125346183777\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.79it/s]\n",
            "Epoch 1 loss average: 0.502\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6229    0.3734    0.4669     11182\n",
            "           2     0.0870    0.0021    0.0040       969\n",
            "           3     0.5939    0.1956    0.2943      1600\n",
            "           4     0.2857    0.0024    0.0048       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.0000    0.0000    0.0000       303\n",
            "\n",
            "   micro avg     0.6187    0.2989    0.4031     15027\n",
            "   macro avg     0.2649    0.0956    0.1283     15027\n",
            "weighted avg     0.5481    0.2989    0.3793     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6686    0.4612    0.5459      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.5538    0.3103    0.3978       116\n",
            "           4     0.0000    0.0000    0.0000       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6589    0.3566    0.4627      1419\n",
            "   macro avg     0.2037    0.1286    0.1573      1419\n",
            "weighted avg     0.5254    0.3566    0.4245      1419\n",
            "\n",
            "0.5015167324763729 0.3792726952190735 0.47456515226692003 0.4245188122682224\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 0.05035766214132309\n",
            "Epoch: 2, iter 100: loss = 1.6018712520599365\n",
            "Epoch: 2, iter 200: loss = 0.01366830337792635\n",
            "Epoch: 2, iter 300: loss = 0.11847640573978424\n",
            "Epoch: 2, iter 400: loss = 0.024899495765566826\n",
            "Epoch: 2, iter 500: loss = 1.31704843044281\n",
            "Epoch: 2, iter 600: loss = 0.015585382468998432\n",
            "Epoch: 2, iter 700: loss = 0.08461017161607742\n",
            "Epoch: 2, iter 800: loss = 1.6528491973876953\n",
            "Epoch: 2, iter 900: loss = 0.48304736614227295\n",
            "Epoch: 2, iter 1000: loss = 0.7203328609466553\n",
            "Epoch: 2, iter 1100: loss = 0.3400443196296692\n",
            "Epoch: 2, iter 1200: loss = 0.701928436756134\n",
            "Epoch: 2, iter 1300: loss = 0.4160308539867401\n",
            "Epoch: 2, iter 1400: loss = 0.030007051303982735\n",
            "Epoch: 2, iter 1500: loss = 2.423377513885498\n",
            "Epoch: 2, iter 1600: loss = 0.011346633546054363\n",
            "Epoch: 2, iter 1700: loss = 1.4211701154708862\n",
            "Epoch: 2, iter 1800: loss = 0.10015488415956497\n",
            "Epoch: 2, iter 1900: loss = 1.383590579032898\n",
            "Epoch: 2, iter 2000: loss = 0.2413206249475479\n",
            "Epoch: 2, iter 2100: loss = 1.0222522020339966\n",
            "Epoch: 2, iter 2200: loss = 2.050607204437256\n",
            "Epoch: 2, iter 2300: loss = 1.4987870454788208\n",
            "Epoch: 2, iter 2400: loss = 0.2855337858200073\n",
            "Epoch: 2, iter 2500: loss = 0.06196825951337814\n",
            "Epoch: 2, iter 2600: loss = 0.10881445556879044\n",
            "Epoch: 2, iter 2700: loss = 0.02889205887913704\n",
            "Epoch: 2, iter 2800: loss = 0.5534752607345581\n",
            "Epoch: 2, iter 2900: loss = 0.3121083974838257\n",
            "Epoch: 2, iter 3000: loss = 0.9528983235359192\n",
            "Epoch: 2, iter 3100: loss = 0.1326771229505539\n",
            "Epoch: 2, iter 3200: loss = 0.1411461979150772\n",
            "Epoch: 2, iter 3300: loss = 0.2851502001285553\n",
            "Epoch: 2, iter 3400: loss = 1.4008408784866333\n",
            "Epoch: 2, iter 3500: loss = 0.7452371120452881\n",
            "Epoch: 2, iter 3600: loss = 0.3946911096572876\n",
            "Epoch: 2, iter 3700: loss = 0.16452668607234955\n",
            "Epoch: 2, iter 3800: loss = 0.22877265512943268\n",
            "Epoch: 2, iter 3900: loss = 0.1289462447166443\n",
            "Epoch: 2, iter 4000: loss = 0.48878028988838196\n",
            "Epoch: 2, iter 4100: loss = 0.08285532146692276\n",
            "Epoch: 2, iter 4200: loss = 0.10365940630435944\n",
            "Epoch: 2, iter 4300: loss = 0.02548784576356411\n",
            "Epoch: 2, iter 4400: loss = 0.7805829644203186\n",
            "Epoch: 2, iter 4500: loss = 1.865891456604004\n",
            "Epoch: 2, iter 4600: loss = 0.5698396563529968\n",
            "Epoch: 2, iter 4700: loss = 0.6089650392532349\n",
            "Epoch: 2, iter 4800: loss = 0.07418161630630493\n",
            "Epoch: 2, iter 4900: loss = 0.053916361182928085\n",
            "Epoch: 2, iter 5000: loss = 0.4811629354953766\n",
            "Epoch: 2, iter 5100: loss = 0.2989756166934967\n",
            "Epoch: 2, iter 5200: loss = 0.3297213315963745\n",
            "Epoch: 2, iter 5300: loss = 1.984223484992981\n",
            "Epoch: 2, iter 5400: loss = 0.15162895619869232\n",
            "Epoch: 2, iter 5500: loss = 0.6409825086593628\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.80it/s]\n",
            "Epoch 2 loss average: 0.417\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6833    0.4937    0.5732     11182\n",
            "           2     0.2903    0.0093    0.0180       969\n",
            "           3     0.6087    0.4131    0.4922      1600\n",
            "           4     0.3333    0.0351    0.0635       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2000    0.0033    0.0065       303\n",
            "\n",
            "   micro avg     0.6697    0.4139    0.5116     15027\n",
            "   macro avg     0.3526    0.1591    0.1922     15027\n",
            "weighted avg     0.6143    0.4139    0.4837     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6520    0.4504    0.5328      1019\n",
            "           2     0.6667    0.0392    0.0741       102\n",
            "           3     0.3986    0.4741    0.4331       116\n",
            "           4     0.1429    0.0169    0.0303       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6032    0.3665    0.4559      1419\n",
            "   macro avg     0.3100    0.1635    0.1784      1419\n",
            "weighted avg     0.5606    0.3665    0.4259      1419\n",
            "\n",
            "0.4173662296831735 0.483707100404901 0.4908285068021752 0.4258507521458706\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 0.14585380256175995\n",
            "Epoch: 3, iter 100: loss = 0.2487032115459442\n",
            "Epoch: 3, iter 200: loss = 0.0056657856330275536\n",
            "Epoch: 3, iter 300: loss = 0.03127538040280342\n",
            "Epoch: 3, iter 400: loss = 0.19367307424545288\n",
            "Epoch: 3, iter 500: loss = 0.06260747462511063\n",
            "Epoch: 3, iter 600: loss = 0.202865868806839\n",
            "Epoch: 3, iter 700: loss = 1.2912393808364868\n",
            "Epoch: 3, iter 800: loss = 0.5823092460632324\n",
            "Epoch: 3, iter 900: loss = 0.23837408423423767\n",
            "Epoch: 3, iter 1000: loss = 0.28242048621177673\n",
            "Epoch: 3, iter 1100: loss = 0.07243780791759491\n",
            "Epoch: 3, iter 1200: loss = 0.48303747177124023\n",
            "Epoch: 3, iter 1300: loss = 0.9587525129318237\n",
            "Epoch: 3, iter 1400: loss = 0.9265120625495911\n",
            "Epoch: 3, iter 1500: loss = 1.1546175479888916\n",
            "Epoch: 3, iter 1600: loss = 0.08505740016698837\n",
            "Epoch: 3, iter 1700: loss = 0.2277650386095047\n",
            "Epoch: 3, iter 1800: loss = 0.2092527598142624\n",
            "Epoch: 3, iter 1900: loss = 0.07310091704130173\n",
            "Epoch: 3, iter 2000: loss = 0.4613763391971588\n",
            "Epoch: 3, iter 2100: loss = 0.28701743483543396\n",
            "Epoch: 3, iter 2200: loss = 0.9296867251396179\n",
            "Epoch: 3, iter 2300: loss = 0.011245639063417912\n",
            "Epoch: 3, iter 2400: loss = 0.3203352093696594\n",
            "Epoch: 3, iter 2500: loss = 0.18142396211624146\n",
            "Epoch: 3, iter 2600: loss = 0.08891867846250534\n",
            "Epoch: 3, iter 2700: loss = 0.09987908601760864\n",
            "Epoch: 3, iter 2800: loss = 0.05543305352330208\n",
            "Epoch: 3, iter 2900: loss = 0.02329126186668873\n",
            "Epoch: 3, iter 3000: loss = 0.0861736387014389\n",
            "Epoch: 3, iter 3100: loss = 0.09073133021593094\n",
            "Epoch: 3, iter 3200: loss = 0.6809419989585876\n",
            "Epoch: 3, iter 3300: loss = 0.032166291028261185\n",
            "Epoch: 3, iter 3400: loss = 1.9919195175170898\n",
            "Epoch: 3, iter 3500: loss = 0.09679926186800003\n",
            "Epoch: 3, iter 3600: loss = 0.2846130430698395\n",
            "Epoch: 3, iter 3700: loss = 0.981457531452179\n",
            "Epoch: 3, iter 3800: loss = 0.2780580222606659\n",
            "Epoch: 3, iter 3900: loss = 0.008439029566943645\n",
            "Epoch: 3, iter 4000: loss = 0.49996060132980347\n",
            "Epoch: 3, iter 4100: loss = 0.8149993419647217\n",
            "Epoch: 3, iter 4200: loss = 0.13614368438720703\n",
            "Epoch: 3, iter 4300: loss = 0.18735776841640472\n",
            "Epoch: 3, iter 4400: loss = 0.05561669170856476\n",
            "Epoch: 3, iter 4500: loss = 0.048300787806510925\n",
            "Epoch: 3, iter 4600: loss = 0.4062294661998749\n",
            "Epoch: 3, iter 4700: loss = 0.1940241903066635\n",
            "Epoch: 3, iter 4800: loss = 0.2216474562883377\n",
            "Epoch: 3, iter 4900: loss = 0.3852687180042267\n",
            "Epoch: 3, iter 5000: loss = 0.2876792252063751\n",
            "Epoch: 3, iter 5100: loss = 0.910500168800354\n",
            "Epoch: 3, iter 5200: loss = 0.010245897807180882\n",
            "Epoch: 3, iter 5300: loss = 0.05947021394968033\n",
            "Epoch: 3, iter 5400: loss = 0.2651359736919403\n",
            "Epoch: 3, iter 5500: loss = 0.29976320266723633\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.80it/s]\n",
            "Epoch 3 loss average: 0.378\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7083    0.5466    0.6170     11182\n",
            "           2     0.3711    0.0372    0.0675       969\n",
            "           3     0.6377    0.5038    0.5628      1600\n",
            "           4     0.3600    0.0871    0.1402       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2143    0.0099    0.0189       303\n",
            "\n",
            "   micro avg     0.6888    0.4678    0.5572     15027\n",
            "   macro avg     0.3819    0.1974    0.2344     15027\n",
            "weighted avg     0.6430    0.4678    0.5315     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6650    0.3857    0.4882      1019\n",
            "           2     0.7500    0.0588    0.1091       102\n",
            "           3     0.4435    0.4397    0.4416       116\n",
            "           4     0.2581    0.0678    0.1074       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6148    0.3228    0.4233      1419\n",
            "   macro avg     0.3528    0.1587    0.1910      1419\n",
            "weighted avg     0.5892    0.3228    0.4034      1419\n",
            "\n",
            "0.3779013244263301 0.5315325237899936 0.456504677521674 0.40344871532708954\n",
            "Patience counter: 1\n",
            "Epoch: 4, iter 0: loss = 0.19475965201854706\n",
            "Epoch: 4, iter 100: loss = 0.743211030960083\n",
            "Epoch: 4, iter 200: loss = 0.5462955236434937\n",
            "Epoch: 4, iter 300: loss = 1.64946711063385\n",
            "Epoch: 4, iter 400: loss = 0.5093600749969482\n",
            "Epoch: 4, iter 500: loss = 0.005924629047513008\n",
            "Epoch: 4, iter 600: loss = 0.05779865011572838\n",
            "Epoch: 4, iter 700: loss = 0.7226181030273438\n",
            "Epoch: 4, iter 800: loss = 0.08524131774902344\n",
            "Epoch: 4, iter 900: loss = 0.35748913884162903\n",
            "Epoch: 4, iter 1000: loss = 0.7832881808280945\n",
            "Epoch: 4, iter 1100: loss = 0.42665696144104004\n",
            "Epoch: 4, iter 1200: loss = 0.3318164646625519\n",
            "Epoch: 4, iter 1300: loss = 0.42614758014678955\n",
            "Epoch: 4, iter 1400: loss = 0.0806414857506752\n",
            "Epoch: 4, iter 1500: loss = 0.28238198161125183\n",
            "Epoch: 4, iter 1600: loss = 0.16986696422100067\n",
            "Epoch: 4, iter 1700: loss = 0.007418322376906872\n",
            "Epoch: 4, iter 1800: loss = 0.7025843858718872\n",
            "Epoch: 4, iter 1900: loss = 0.021290332078933716\n",
            "Epoch: 4, iter 2000: loss = 0.00804521981626749\n",
            "Epoch: 4, iter 2100: loss = 0.06541658192873001\n",
            "Epoch: 4, iter 2200: loss = 0.8291513323783875\n",
            "Epoch: 4, iter 2300: loss = 0.4265764653682709\n",
            "Epoch: 4, iter 2400: loss = 0.22512920200824738\n",
            "Epoch: 4, iter 2500: loss = 1.104882001876831\n",
            "Epoch: 4, iter 2600: loss = 0.3348212242126465\n",
            "Epoch: 4, iter 2700: loss = 0.17133449018001556\n",
            "Epoch: 4, iter 2800: loss = 0.6891660690307617\n",
            "Epoch: 4, iter 2900: loss = 0.08264406770467758\n",
            "Epoch: 4, iter 3000: loss = 0.08767390251159668\n",
            "Epoch: 4, iter 3100: loss = 0.29910123348236084\n",
            "Epoch: 4, iter 3200: loss = 0.10621911287307739\n",
            "Epoch: 4, iter 3300: loss = 0.30548882484436035\n",
            "Epoch: 4, iter 3400: loss = 0.2079239785671234\n",
            "Epoch: 4, iter 3500: loss = 2.660551071166992\n",
            "Epoch: 4, iter 3600: loss = 1.3172892332077026\n",
            "Epoch: 4, iter 3700: loss = 0.07206255197525024\n",
            "Epoch: 4, iter 3800: loss = 1.4495067596435547\n",
            "Epoch: 4, iter 3900: loss = 0.07170845568180084\n",
            "Epoch: 4, iter 4000: loss = 0.2773372232913971\n",
            "Epoch: 4, iter 4100: loss = 0.07711898535490036\n",
            "Epoch: 4, iter 4200: loss = 0.6836667060852051\n",
            "Epoch: 4, iter 4300: loss = 0.8006865382194519\n",
            "Epoch: 4, iter 4400: loss = 0.05816822126507759\n",
            "Epoch: 4, iter 4500: loss = 0.008851617574691772\n",
            "Epoch: 4, iter 4600: loss = 0.2573949098587036\n",
            "Epoch: 4, iter 4700: loss = 0.24258257448673248\n",
            "Epoch: 4, iter 4800: loss = 0.04742535576224327\n",
            "Epoch: 4, iter 4900: loss = 1.7137476205825806\n",
            "Epoch: 4, iter 5000: loss = 0.13725785911083221\n",
            "Epoch: 4, iter 5100: loss = 0.02948475442826748\n",
            "Epoch: 4, iter 5200: loss = 0.02952032908797264\n",
            "Epoch: 4, iter 5300: loss = 1.0452526807785034\n",
            "Epoch: 4, iter 5400: loss = 0.12564672529697418\n",
            "Epoch: 4, iter 5500: loss = 0.11646629124879837\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.79it/s]\n",
            "Epoch 4 loss average: 0.344\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7346    0.5915    0.6553     11182\n",
            "           2     0.4700    0.1053    0.1720       969\n",
            "           3     0.6723    0.5681    0.6159      1600\n",
            "           4     0.4543    0.2225    0.2987       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2963    0.0264    0.0485       303\n",
            "\n",
            "   micro avg     0.7104    0.5202    0.6006     15027\n",
            "   macro avg     0.4379    0.2523    0.2984     15027\n",
            "weighted avg     0.6795    0.5202    0.5817     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7173    0.3288    0.4509      1019\n",
            "           2     0.4054    0.1471    0.2158       102\n",
            "           3     0.5217    0.4138    0.4615       116\n",
            "           4     0.4898    0.2034    0.2874       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.2000    0.0638    0.0968        47\n",
            "\n",
            "   micro avg     0.6439    0.2995    0.4089      1419\n",
            "   macro avg     0.3890    0.1928    0.2521      1419\n",
            "weighted avg     0.6343    0.2995    0.4041      1419\n",
            "\n",
            "0.3441228445776701 0.5817360855361537 0.45117182970767755 0.40412886423941985\n",
            "Patience counter: 2\n",
            "Epoch: 5, iter 0: loss = 0.14222948253154755\n",
            "Epoch: 5, iter 100: loss = 0.46143996715545654\n",
            "Epoch: 5, iter 200: loss = 0.39648669958114624\n",
            "Epoch: 5, iter 300: loss = 0.02469276823103428\n",
            "Epoch: 5, iter 400: loss = 0.2118758112192154\n",
            "Epoch: 5, iter 500: loss = 0.3978767693042755\n",
            "Epoch: 5, iter 600: loss = 1.0859720706939697\n",
            "Epoch: 5, iter 700: loss = 0.022951534017920494\n",
            "Epoch: 5, iter 800: loss = 0.02844710648059845\n",
            "Epoch: 5, iter 900: loss = 0.17103782296180725\n",
            "Epoch: 5, iter 1000: loss = 0.26346176862716675\n",
            "Epoch: 5, iter 1100: loss = 0.4770872890949249\n",
            "Epoch: 5, iter 1200: loss = 0.0563659742474556\n",
            "Epoch: 5, iter 1300: loss = 0.12567301094532013\n",
            "Epoch: 5, iter 1400: loss = 0.27618688344955444\n",
            "Epoch: 5, iter 1500: loss = 0.42661604285240173\n",
            "Epoch: 5, iter 1600: loss = 0.37640631198883057\n",
            "Epoch: 5, iter 1700: loss = 0.7922557592391968\n",
            "Epoch: 5, iter 1800: loss = 0.04859905689954758\n",
            "Epoch: 5, iter 1900: loss = 1.4840307235717773\n",
            "Epoch: 5, iter 2000: loss = 0.5730834603309631\n",
            "Epoch: 5, iter 2100: loss = 0.5480965971946716\n",
            "Epoch: 5, iter 2200: loss = 0.020542128011584282\n",
            "Epoch: 5, iter 2300: loss = 0.0036375008057802916\n",
            "Epoch: 5, iter 2400: loss = 1.1286983489990234\n",
            "Epoch: 5, iter 2500: loss = 0.32424092292785645\n",
            "Epoch: 5, iter 2600: loss = 0.13977128267288208\n",
            "Epoch: 5, iter 2700: loss = 0.037508197128772736\n",
            "Epoch: 5, iter 2800: loss = 0.007548442110419273\n",
            "Epoch: 5, iter 2900: loss = 0.6117690205574036\n",
            "Epoch: 5, iter 3000: loss = 0.08009492605924606\n",
            "Epoch: 5, iter 3100: loss = 0.4280301630496979\n",
            "Epoch: 5, iter 3200: loss = 0.1668827086687088\n",
            "Epoch: 5, iter 3300: loss = 0.5572425127029419\n",
            "Epoch: 5, iter 3400: loss = 0.0037929662503302097\n",
            "Epoch: 5, iter 3500: loss = 0.16802513599395752\n",
            "Epoch: 5, iter 3600: loss = 0.24381586909294128\n",
            "Epoch: 5, iter 3700: loss = 0.5091952681541443\n",
            "Epoch: 5, iter 3800: loss = 0.22712166607379913\n",
            "Epoch: 5, iter 3900: loss = 0.4426223635673523\n",
            "Epoch: 5, iter 4000: loss = 0.8277193903923035\n",
            "Epoch: 5, iter 4100: loss = 1.2814828157424927\n",
            "Epoch: 5, iter 4200: loss = 0.17588478326797485\n",
            "Epoch: 5, iter 4300: loss = 0.24669429659843445\n",
            "Epoch: 5, iter 4400: loss = 0.008188503794372082\n",
            "Epoch: 5, iter 4500: loss = 0.3465036153793335\n",
            "Epoch: 5, iter 4600: loss = 0.052843205630779266\n",
            "Epoch: 5, iter 4700: loss = 0.3858947455883026\n",
            "Epoch: 5, iter 4800: loss = 0.07026372104883194\n",
            "Epoch: 5, iter 4900: loss = 0.12710900604724884\n",
            "Epoch: 5, iter 5000: loss = 0.004989951848983765\n",
            "Epoch: 5, iter 5100: loss = 0.3016030490398407\n",
            "Epoch: 5, iter 5200: loss = 0.1728382110595703\n",
            "Epoch: 5, iter 5300: loss = 0.2686658799648285\n",
            "Epoch: 5, iter 5400: loss = 0.2510198652744293\n",
            "Epoch: 5, iter 5500: loss = 0.6155855059623718\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.79it/s]\n",
            "Epoch 5 loss average: 0.316\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7632    0.6384    0.6953     11182\n",
            "           2     0.5510    0.2064    0.3003       969\n",
            "           3     0.7042    0.5981    0.6468      1600\n",
            "           4     0.4948    0.2866    0.3629       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2985    0.0660    0.1081       303\n",
            "\n",
            "   micro avg     0.7359    0.5692    0.6419     15027\n",
            "   macro avg     0.4686    0.2993    0.3522     15027\n",
            "weighted avg     0.7117    0.5692    0.6278     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6402    0.4975    0.5599      1019\n",
            "           2     0.3559    0.2059    0.2609       102\n",
            "           3     0.5500    0.3793    0.4490       116\n",
            "           4     0.3571    0.2119    0.2660       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.5000    0.0426    0.0784        47\n",
            "\n",
            "   micro avg     0.5960    0.4221    0.4942      1419\n",
            "   macro avg     0.4005    0.2229    0.2690      1419\n",
            "weighted avg     0.5765    0.4221    0.4822      1419\n",
            "\n",
            "0.3155895065896683 0.6277578687111721 0.5392277247285286 0.4822477476598925\n",
            "Patience counter: 3\n",
            "Epoch: 6, iter 0: loss = 0.5632227063179016\n",
            "Epoch: 6, iter 100: loss = 0.5553941130638123\n",
            "Epoch: 6, iter 200: loss = 1.0593881607055664\n",
            "Epoch: 6, iter 300: loss = 0.03918846324086189\n",
            "Epoch: 6, iter 400: loss = 0.1722254902124405\n",
            "Epoch: 6, iter 500: loss = 0.3567785322666168\n",
            "Epoch: 6, iter 600: loss = 0.01870538666844368\n",
            "Epoch: 6, iter 700: loss = 0.44162431359291077\n",
            "Epoch: 6, iter 800: loss = 0.09861347824335098\n",
            "Epoch: 6, iter 900: loss = 0.6189543008804321\n",
            "Epoch: 6, iter 1000: loss = 0.24770545959472656\n",
            "Epoch: 6, iter 1100: loss = 0.0016507310792803764\n",
            "Epoch: 6, iter 1200: loss = 0.0016191586619243026\n",
            "Epoch: 6, iter 1300: loss = 0.9979867339134216\n",
            "Epoch: 6, iter 1400: loss = 0.272748738527298\n",
            "Epoch: 6, iter 1500: loss = 0.005742209497839212\n",
            "Epoch: 6, iter 1600: loss = 0.001451547839678824\n",
            "Epoch: 6, iter 1700: loss = 0.007692339364439249\n",
            "Epoch: 6, iter 1800: loss = 0.3027888536453247\n",
            "Epoch: 6, iter 1900: loss = 0.11201437562704086\n",
            "Epoch: 6, iter 2000: loss = 0.3837983310222626\n",
            "Epoch: 6, iter 2100: loss = 0.063083216547966\n",
            "Epoch: 6, iter 2200: loss = 0.5990797281265259\n",
            "Epoch: 6, iter 2300: loss = 0.0005835476331412792\n",
            "Epoch: 6, iter 2400: loss = 0.06391433626413345\n",
            "Epoch: 6, iter 2500: loss = 0.43737995624542236\n",
            "Epoch: 6, iter 2600: loss = 1.4000910520553589\n",
            "Epoch: 6, iter 2700: loss = 0.2899110019207001\n",
            "Epoch: 6, iter 2800: loss = 0.12826785445213318\n",
            "Epoch: 6, iter 2900: loss = 0.01542411558330059\n",
            "Epoch: 6, iter 3000: loss = 0.3804067373275757\n",
            "Epoch: 6, iter 3100: loss = 0.16510382294654846\n",
            "Epoch: 6, iter 3200: loss = 0.6467625498771667\n",
            "Epoch: 6, iter 3300: loss = 0.12005060166120529\n",
            "Epoch: 6, iter 3400: loss = 0.2061646431684494\n",
            "Epoch: 6, iter 3500: loss = 0.10144861787557602\n",
            "Epoch: 6, iter 3600: loss = 0.3700858950614929\n",
            "Epoch: 6, iter 3700: loss = 0.004512277897447348\n",
            "Epoch: 6, iter 3800: loss = 0.04931236803531647\n",
            "Epoch: 6, iter 3900: loss = 0.19225741922855377\n",
            "Epoch: 6, iter 4000: loss = 0.15128418803215027\n",
            "Epoch: 6, iter 4100: loss = 0.03359203413128853\n",
            "Epoch: 6, iter 4200: loss = 0.15320749580860138\n",
            "Epoch: 6, iter 4300: loss = 0.47609183192253113\n",
            "Epoch: 6, iter 4400: loss = 0.04087208956480026\n",
            "Epoch: 6, iter 4500: loss = 0.2043723464012146\n",
            "Epoch: 6, iter 4600: loss = 0.003807825269177556\n",
            "Epoch: 6, iter 4700: loss = 0.731030285358429\n",
            "Epoch: 6, iter 4800: loss = 0.00672813830897212\n",
            "Epoch: 6, iter 4900: loss = 0.857999861240387\n",
            "Epoch: 6, iter 5000: loss = 0.3674834072589874\n",
            "Epoch: 6, iter 5100: loss = 0.6582560539245605\n",
            "Epoch: 6, iter 5200: loss = 1.316598653793335\n",
            "Epoch: 6, iter 5300: loss = 0.2686314582824707\n",
            "Epoch: 6, iter 5400: loss = 0.06401427835226059\n",
            "Epoch: 6, iter 5500: loss = 0.0947517678141594\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.80it/s]\n",
            "Epoch 6 loss average: 0.293\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7896    0.6831    0.7325     11182\n",
            "           2     0.5592    0.2632    0.3579       969\n",
            "           3     0.7078    0.6206    0.6613      1600\n",
            "           4     0.5356    0.3640    0.4334       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.4537    0.1617    0.2384       303\n",
            "\n",
            "   micro avg     0.7569    0.6146    0.6784     15027\n",
            "   macro avg     0.5076    0.3488    0.4039     15027\n",
            "weighted avg     0.7376    0.6146    0.6672     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6639    0.4769    0.5551      1019\n",
            "           2     0.4828    0.1373    0.2137       102\n",
            "           3     0.4094    0.4483    0.4280       116\n",
            "           4     0.3962    0.1780    0.2456       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3333    0.1064    0.1613        47\n",
            "\n",
            "   micro avg     0.6046    0.4073    0.4867      1419\n",
            "   macro avg     0.3810    0.2245    0.2673      1419\n",
            "weighted avg     0.5889    0.4073    0.4747      1419\n",
            "\n",
            "0.2931430851130362 0.6672174810637653 0.5139476981201273 0.4747492596351327\n",
            "Patience counter: 4\n",
            "Epoch: 7, iter 0: loss = 0.11047561466693878\n",
            "Epoch: 7, iter 100: loss = 0.2467881590127945\n",
            "Epoch: 7, iter 200: loss = 0.018573056906461716\n",
            "Epoch: 7, iter 300: loss = 3.468950033187866\n",
            "Epoch: 7, iter 400: loss = 0.16928954422473907\n",
            "Epoch: 7, iter 500: loss = 0.44007983803749084\n",
            "Epoch: 7, iter 600: loss = 0.006349970120936632\n",
            "Epoch: 7, iter 700: loss = 0.8702661395072937\n",
            "Epoch: 7, iter 800: loss = 0.0867551937699318\n",
            "Epoch: 7, iter 900: loss = 0.1021481528878212\n",
            "Epoch: 7, iter 1000: loss = 0.004800861701369286\n",
            "Epoch: 7, iter 1100: loss = 0.05857696011662483\n",
            "Epoch: 7, iter 1200: loss = 0.47167664766311646\n",
            "Epoch: 7, iter 1300: loss = 0.6806075572967529\n",
            "Epoch: 7, iter 1400: loss = 0.2262265682220459\n",
            "Epoch: 7, iter 1500: loss = 0.0892704427242279\n",
            "Epoch: 7, iter 1600: loss = 0.17795854806900024\n",
            "Epoch: 7, iter 1700: loss = 0.3270092010498047\n",
            "Epoch: 7, iter 1800: loss = 0.0403648242354393\n",
            "Epoch: 7, iter 1900: loss = 0.1618649810552597\n",
            "Epoch: 7, iter 2000: loss = 0.3990023136138916\n",
            "Epoch: 7, iter 2100: loss = 0.2541961669921875\n",
            "Epoch: 7, iter 2200: loss = 0.909166157245636\n",
            "Epoch: 7, iter 2300: loss = 0.03104863129556179\n",
            "Epoch: 7, iter 2400: loss = 0.008239666931331158\n",
            "Epoch: 7, iter 2500: loss = 0.4732789695262909\n",
            "Epoch: 7, iter 2600: loss = 0.07984062284231186\n",
            "Epoch: 7, iter 2700: loss = 0.0036695548333227634\n",
            "Epoch: 7, iter 2800: loss = 0.0058525968343019485\n",
            "Epoch: 7, iter 2900: loss = 0.3491804599761963\n",
            "Epoch: 7, iter 3000: loss = 0.19627168774604797\n",
            "Epoch: 7, iter 3100: loss = 0.08652399480342865\n",
            "Epoch: 7, iter 3200: loss = 0.28942033648490906\n",
            "Epoch: 7, iter 3300: loss = 0.4739629626274109\n",
            "Epoch: 7, iter 3400: loss = 2.4350619316101074\n",
            "Epoch: 7, iter 3500: loss = 0.38630756735801697\n",
            "Epoch: 7, iter 3600: loss = 0.24882832169532776\n",
            "Epoch: 7, iter 3700: loss = 0.29330193996429443\n",
            "Epoch: 7, iter 3800: loss = 0.005469022784382105\n",
            "Epoch: 7, iter 3900: loss = 0.25242453813552856\n",
            "Epoch: 7, iter 4000: loss = 0.13695310056209564\n",
            "Epoch: 7, iter 4100: loss = 0.4909435212612152\n",
            "Epoch: 7, iter 4200: loss = 0.042073093354701996\n",
            "Epoch: 7, iter 4300: loss = 0.8598603010177612\n",
            "Epoch: 7, iter 4400: loss = 0.1597575843334198\n",
            "Epoch: 7, iter 4500: loss = 0.25466111302375793\n",
            "Epoch: 7, iter 4600: loss = 0.002233476610854268\n",
            "Epoch: 7, iter 4700: loss = 0.005271392874419689\n",
            "Epoch: 7, iter 4800: loss = 0.009863845072686672\n",
            "Epoch: 7, iter 4900: loss = 0.011579731479287148\n",
            "Epoch: 7, iter 5000: loss = 0.613972544670105\n",
            "Epoch: 7, iter 5100: loss = 0.2050829976797104\n",
            "Epoch: 7, iter 5200: loss = 0.5135308504104614\n",
            "Epoch: 7, iter 5300: loss = 0.013112509623169899\n",
            "Epoch: 7, iter 5400: loss = 0.17200537025928497\n",
            "Epoch: 7, iter 5500: loss = 0.8268263339996338\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:13<00:00, 12.82it/s]\n",
            "Epoch 7 loss average: 0.274\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8017    0.7115    0.7539     11182\n",
            "           2     0.6140    0.3529    0.4482       969\n",
            "           3     0.7422    0.6569    0.6969      1600\n",
            "           4     0.5624    0.4196    0.4806       827\n",
            "           5     0.2000    0.0068    0.0132       146\n",
            "           6     0.3710    0.1518    0.2155       303\n",
            "\n",
            "   micro avg     0.7706    0.6484    0.7042     15027\n",
            "   macro avg     0.5485    0.3833    0.4347     15027\n",
            "weighted avg     0.7556    0.6484    0.6950     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6180    0.5270    0.5689      1019\n",
            "           2     0.4242    0.1373    0.2074       102\n",
            "           3     0.4783    0.4741    0.4762       116\n",
            "           4     0.4091    0.2288    0.2935       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.6000    0.1915    0.2903        47\n",
            "\n",
            "   micro avg     0.5847    0.4524    0.5101      1419\n",
            "   macro avg     0.4216    0.2598    0.3060      1419\n",
            "weighted avg     0.5672    0.4524    0.4964      1419\n",
            "\n",
            "0.27400097870724877 0.6950385757804113 0.5176648370474705 0.49635901464540977\n",
            "Patience counter: 5\n",
            "Epoch: 8, iter 0: loss = 0.07278631627559662\n",
            "Epoch: 8, iter 100: loss = 0.44627484679222107\n",
            "Epoch: 8, iter 200: loss = 0.6242475509643555\n",
            "Epoch: 8, iter 300: loss = 0.020144762471318245\n",
            "Epoch: 8, iter 400: loss = 0.2017180472612381\n",
            "Epoch: 8, iter 500: loss = 0.15565797686576843\n",
            "Epoch: 8, iter 600: loss = 0.24444985389709473\n",
            "Epoch: 8, iter 700: loss = 0.03817586228251457\n",
            "Epoch: 8, iter 800: loss = 0.03512367978692055\n",
            "Epoch: 8, iter 900: loss = 0.5586054921150208\n",
            "Epoch: 8, iter 1000: loss = 0.2823527753353119\n",
            "Epoch: 8, iter 1100: loss = 0.22633394598960876\n",
            "Epoch: 8, iter 1200: loss = 0.013619378209114075\n",
            "Epoch: 8, iter 1300: loss = 0.01547620352357626\n",
            "Epoch: 8, iter 1400: loss = 0.14972658455371857\n",
            "Epoch: 8, iter 1500: loss = 0.325440376996994\n",
            "Epoch: 8, iter 1600: loss = 0.03811107948422432\n",
            "Epoch: 8, iter 1700: loss = 1.7212779521942139\n",
            "Epoch: 8, iter 1800: loss = 1.096853494644165\n",
            "Epoch: 8, iter 1900: loss = 0.04354573413729668\n",
            "Epoch: 8, iter 2000: loss = 0.3337256610393524\n",
            "Epoch: 8, iter 2100: loss = 0.009556785225868225\n",
            "Epoch: 8, iter 2200: loss = 0.0025328374467790127\n",
            "Epoch: 8, iter 2300: loss = 0.4375664293766022\n",
            "Epoch: 8, iter 2400: loss = 0.3397182822227478\n",
            "Epoch: 8, iter 2500: loss = 1.2497481107711792\n",
            "Epoch: 8, iter 2600: loss = 0.22036992013454437\n",
            "Epoch: 8, iter 2700: loss = 0.0019879580941051245\n",
            "Epoch: 8, iter 2800: loss = 0.053022321313619614\n",
            "Epoch: 8, iter 2900: loss = 0.03365607559680939\n",
            "Epoch: 8, iter 3000: loss = 0.030953850597143173\n",
            "Epoch: 8, iter 3100: loss = 0.01937873661518097\n",
            "Epoch: 8, iter 3200: loss = 1.171017050743103\n",
            "Epoch: 8, iter 3300: loss = 0.17247547209262848\n",
            "Epoch: 8, iter 3400: loss = 0.045412346720695496\n",
            "Epoch: 8, iter 3500: loss = 1.3107514381408691\n",
            "Epoch: 8, iter 3600: loss = 0.004195974674075842\n",
            "Epoch: 8, iter 3700: loss = 0.17886506021022797\n",
            "Epoch: 8, iter 3800: loss = 0.07823288440704346\n",
            "Epoch: 8, iter 3900: loss = 0.7223713994026184\n",
            "Epoch: 8, iter 4000: loss = 0.17584814131259918\n",
            "Epoch: 8, iter 4100: loss = 0.1224813237786293\n",
            "Epoch: 8, iter 4200: loss = 0.0021634206641465425\n",
            "Epoch: 8, iter 4300: loss = 0.05036507546901703\n",
            "Epoch: 8, iter 4400: loss = 0.06266742944717407\n",
            "Epoch: 8, iter 4500: loss = 0.07510735839605331\n",
            "Epoch: 8, iter 4600: loss = 0.04468907043337822\n",
            "Epoch: 8, iter 4700: loss = 2.1024184226989746\n",
            "Epoch: 8, iter 4800: loss = 0.05071360990405083\n",
            "Epoch: 8, iter 4900: loss = 0.10602889209985733\n",
            "Epoch: 8, iter 5000: loss = 0.0027166944928467274\n",
            "Epoch: 8, iter 5100: loss = 0.04662316292524338\n",
            "Epoch: 8, iter 5200: loss = 0.7073178887367249\n",
            "Epoch: 8, iter 5300: loss = 0.0026632242370396852\n",
            "Epoch: 8, iter 5400: loss = 0.06545472890138626\n",
            "Epoch: 8, iter 5500: loss = 0.0471523143351078\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.84it/s]\n",
            "Epoch 8 loss average: 0.259\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8145    0.7297    0.7698     11182\n",
            "           2     0.6411    0.3963    0.4898       969\n",
            "           3     0.7306    0.6475    0.6865      1600\n",
            "           4     0.5756    0.4281    0.4910       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.4483    0.2574    0.3270       303\n",
            "\n",
            "   micro avg     0.7806    0.6662    0.7189     15027\n",
            "   macro avg     0.5350    0.4098    0.4607     15027\n",
            "weighted avg     0.7660    0.6662    0.7111     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5657    0.6045    0.5844      1019\n",
            "           2     0.3171    0.2549    0.2826       102\n",
            "           3     0.3520    0.5431    0.4271       116\n",
            "           4     0.4375    0.1780    0.2530       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3846    0.1064    0.1667        47\n",
            "\n",
            "   micro avg     0.5181    0.5152    0.5166      1419\n",
            "   macro avg     0.3428    0.2811    0.2856      1419\n",
            "weighted avg     0.5069    0.5152    0.5015      1419\n",
            "\n",
            "0.2592544925867194 0.7110939023549953 0.5243618526022923 0.501483577673592\n",
            "Patience counter: 6\n",
            "Epoch: 9, iter 0: loss = 0.2502109110355377\n",
            "Epoch: 9, iter 100: loss = 0.018546633422374725\n",
            "Epoch: 9, iter 200: loss = 0.030530819669365883\n",
            "Epoch: 9, iter 300: loss = 0.14349912106990814\n",
            "Epoch: 9, iter 400: loss = 0.12489281594753265\n",
            "Epoch: 9, iter 500: loss = 0.1495431512594223\n",
            "Epoch: 9, iter 600: loss = 0.0013002182822674513\n",
            "Epoch: 9, iter 700: loss = 0.0023737112060189247\n",
            "Epoch: 9, iter 800: loss = 0.34385132789611816\n",
            "Epoch: 9, iter 900: loss = 0.001455463352613151\n",
            "Epoch: 9, iter 1000: loss = 0.14949984848499298\n",
            "Epoch: 9, iter 1100: loss = 0.3852478563785553\n",
            "Epoch: 9, iter 1200: loss = 0.05804671719670296\n",
            "Epoch: 9, iter 1300: loss = 0.010321362875401974\n",
            "Epoch: 9, iter 1400: loss = 0.0017498604720458388\n",
            "Epoch: 9, iter 1500: loss = 0.019970891997218132\n",
            "Epoch: 9, iter 1600: loss = 0.5759853720664978\n",
            "Epoch: 9, iter 1700: loss = 0.10957686603069305\n",
            "Epoch: 9, iter 1800: loss = 0.0024138223379850388\n",
            "Epoch: 9, iter 1900: loss = 0.11333543807268143\n",
            "Epoch: 9, iter 2000: loss = 0.29620102047920227\n",
            "Epoch: 9, iter 2100: loss = 0.0012311609461903572\n",
            "Epoch: 9, iter 2200: loss = 0.08219805359840393\n",
            "Epoch: 9, iter 2300: loss = 0.22859470546245575\n",
            "Epoch: 9, iter 2400: loss = 0.00438610790297389\n",
            "Epoch: 9, iter 2500: loss = 0.03716527670621872\n",
            "Epoch: 9, iter 2600: loss = 0.4478849768638611\n",
            "Epoch: 9, iter 2700: loss = 1.011814832687378\n",
            "Epoch: 9, iter 2800: loss = 0.39180615544319153\n",
            "Epoch: 9, iter 2900: loss = 0.00913948379456997\n",
            "Epoch: 9, iter 3000: loss = 0.014202394522726536\n",
            "Epoch: 9, iter 3100: loss = 0.0026474669575691223\n",
            "Epoch: 9, iter 3200: loss = 0.1453799605369568\n",
            "Epoch: 9, iter 3300: loss = 0.01708129607141018\n",
            "Epoch: 9, iter 3400: loss = 0.48303699493408203\n",
            "Epoch: 9, iter 3500: loss = 0.02143203094601631\n",
            "Epoch: 9, iter 3600: loss = 0.17315466701984406\n",
            "Epoch: 9, iter 3700: loss = 1.1518912315368652\n",
            "Epoch: 9, iter 3800: loss = 0.41049841046333313\n",
            "Epoch: 9, iter 3900: loss = 1.0494927167892456\n",
            "Epoch: 9, iter 4000: loss = 0.37413695454597473\n",
            "Epoch: 9, iter 4100: loss = 0.29900017380714417\n",
            "Epoch: 9, iter 4200: loss = 0.13832896947860718\n",
            "Epoch: 9, iter 4300: loss = 0.3798467814922333\n",
            "Epoch: 9, iter 4400: loss = 0.07982227951288223\n",
            "Epoch: 9, iter 4500: loss = 0.16584506630897522\n",
            "Epoch: 9, iter 4600: loss = 0.5676286220550537\n",
            "Epoch: 9, iter 4700: loss = 0.038704000413417816\n",
            "Epoch: 9, iter 4800: loss = 0.014307592064142227\n",
            "Epoch: 9, iter 4900: loss = 0.022927511483430862\n",
            "Epoch: 9, iter 5000: loss = 0.5680468082427979\n",
            "Epoch: 9, iter 5100: loss = 0.11223243176937103\n",
            "Epoch: 9, iter 5200: loss = 0.004473730456084013\n",
            "Epoch: 9, iter 5300: loss = 0.12474852055311203\n",
            "Epoch: 9, iter 5400: loss = 0.005199400242418051\n",
            "Epoch: 9, iter 5500: loss = 0.005265184678137302\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.85it/s]\n",
            "Epoch 9 loss average: 0.242\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8291    0.7489    0.7870     11182\n",
            "           2     0.6626    0.4438    0.5315       969\n",
            "           3     0.7585    0.6831    0.7188      1600\n",
            "           4     0.6167    0.4728    0.5352       827\n",
            "           5     0.3333    0.0068    0.0134       146\n",
            "           6     0.4581    0.3069    0.3676       303\n",
            "\n",
            "   micro avg     0.7968    0.6909    0.7401     15027\n",
            "   macro avg     0.6097    0.4437    0.4923     15027\n",
            "weighted avg     0.7869    0.6909    0.7334     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6508    0.3896    0.4874      1019\n",
            "           2     0.3182    0.2745    0.2947       102\n",
            "           3     0.5556    0.3879    0.4569       116\n",
            "           4     0.4468    0.1780    0.2545       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.3784    0.2979    0.3333        47\n",
            "\n",
            "   micro avg     0.5852    0.3559    0.4426      1419\n",
            "   macro avg     0.3916    0.2546    0.3045      1419\n",
            "weighted avg     0.5853    0.3559    0.4408      1419\n",
            "\n",
            "0.24248381328836377 0.7334082175291735 0.48363545150506426 0.44075940852640916\n",
            "Patience counter: 7\n",
            "Epoch: 10, iter 0: loss = 0.05883157625794411\n",
            "Epoch: 10, iter 100: loss = 0.008671139366924763\n",
            "Epoch: 10, iter 200: loss = 0.01368323341012001\n",
            "Epoch: 10, iter 300: loss = 0.45507872104644775\n",
            "Epoch: 10, iter 400: loss = 0.5837171673774719\n",
            "Epoch: 10, iter 500: loss = 0.2604554295539856\n",
            "Epoch: 10, iter 600: loss = 0.003711903002113104\n",
            "Epoch: 10, iter 700: loss = 0.3478023409843445\n",
            "Epoch: 10, iter 800: loss = 0.0010195171926170588\n",
            "Epoch: 10, iter 900: loss = 0.03482607379555702\n",
            "Epoch: 10, iter 1000: loss = 1.8724360466003418\n",
            "Epoch: 10, iter 1100: loss = 0.47150012850761414\n",
            "Epoch: 10, iter 1200: loss = 0.3700639009475708\n",
            "Epoch: 10, iter 1300: loss = 0.524246096611023\n",
            "Epoch: 10, iter 1400: loss = 0.015429000370204449\n",
            "Epoch: 10, iter 1500: loss = 0.0032484617549926043\n",
            "Epoch: 10, iter 1600: loss = 0.007362870965152979\n",
            "Epoch: 10, iter 1700: loss = 0.010148590430617332\n",
            "Epoch: 10, iter 1800: loss = 0.015418015420436859\n",
            "Epoch: 10, iter 1900: loss = 0.376691997051239\n",
            "Epoch: 10, iter 2000: loss = 0.16706572473049164\n",
            "Epoch: 10, iter 2100: loss = 0.024860385805368423\n",
            "Epoch: 10, iter 2200: loss = 0.0262558963149786\n",
            "Epoch: 10, iter 2300: loss = 0.0048323324881494045\n",
            "Epoch: 10, iter 2400: loss = 0.6090299487113953\n",
            "Epoch: 10, iter 2500: loss = 0.043148890137672424\n",
            "Epoch: 10, iter 2600: loss = 0.006665125489234924\n",
            "Epoch: 10, iter 2700: loss = 0.15622341632843018\n",
            "Epoch: 10, iter 2800: loss = 0.4190685451030731\n",
            "Epoch: 10, iter 2900: loss = 0.22302649915218353\n",
            "Epoch: 10, iter 3000: loss = 0.3642045259475708\n",
            "Epoch: 10, iter 3100: loss = 0.03287618234753609\n",
            "Epoch: 10, iter 3200: loss = 0.7183168530464172\n",
            "Epoch: 10, iter 3300: loss = 0.3164317309856415\n",
            "Epoch: 10, iter 3400: loss = 0.3869312107563019\n",
            "Epoch: 10, iter 3500: loss = 0.05518200248479843\n",
            "Epoch: 10, iter 3600: loss = 0.5169587731361389\n",
            "Epoch: 10, iter 3700: loss = 0.08926162123680115\n",
            "Epoch: 10, iter 3800: loss = 0.001549375127069652\n",
            "Epoch: 10, iter 3900: loss = 0.09949941188097\n",
            "Epoch: 10, iter 4000: loss = 0.08375484496355057\n",
            "Epoch: 10, iter 4100: loss = 0.06189185380935669\n",
            "Epoch: 10, iter 4200: loss = 0.4655604660511017\n",
            "Epoch: 10, iter 4300: loss = 0.021099397912621498\n",
            "Epoch: 10, iter 4400: loss = 0.14035919308662415\n",
            "Epoch: 10, iter 4500: loss = 0.2755123972892761\n",
            "Epoch: 10, iter 4600: loss = 0.0230120662599802\n",
            "Epoch: 10, iter 4700: loss = 0.5464197993278503\n",
            "Epoch: 10, iter 4800: loss = 0.37458890676498413\n",
            "Epoch: 10, iter 4900: loss = 0.006129909306764603\n",
            "Epoch: 10, iter 5000: loss = 0.0829659178853035\n",
            "Epoch: 10, iter 5100: loss = 0.43602049350738525\n",
            "Epoch: 10, iter 5200: loss = 0.009176281280815601\n",
            "Epoch: 10, iter 5300: loss = 0.25313079357147217\n",
            "Epoch: 10, iter 5400: loss = 0.1958150714635849\n",
            "Epoch: 10, iter 5500: loss = 0.2915286123752594\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.85it/s]\n",
            "Epoch 10 loss average: 0.232\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8338    0.7678    0.7994     11182\n",
            "           2     0.7027    0.5026    0.5860       969\n",
            "           3     0.7594    0.6706    0.7122      1600\n",
            "           4     0.6741    0.5103    0.5809       827\n",
            "           5     0.6667    0.0411    0.0774       146\n",
            "           6     0.5577    0.3828    0.4540       303\n",
            "\n",
            "   micro avg     0.8070    0.7114    0.7562     15027\n",
            "   macro avg     0.6991    0.4792    0.5350     15027\n",
            "weighted avg     0.8014    0.7114    0.7504     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6834    0.4151    0.5165      1019\n",
            "           2     0.3750    0.2353    0.2892       102\n",
            "           3     0.4310    0.4310    0.4310       116\n",
            "           4     0.4167    0.2966    0.3465       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.4706    0.1702    0.2500        47\n",
            "\n",
            "   micro avg     0.5993    0.3805    0.4655      1419\n",
            "   macro avg     0.3961    0.2580    0.3055      1419\n",
            "weighted avg     0.6032    0.3805    0.4640      1419\n",
            "\n",
            "0.23202445262922505 0.7503871525285372 0.5039948194688619 0.46401111226866004\n",
            "Patience counter: 8\n",
            "Epoch: 11, iter 0: loss = 0.00614339392632246\n",
            "Epoch: 11, iter 100: loss = 0.23947061598300934\n",
            "Epoch: 11, iter 200: loss = 0.1272093951702118\n",
            "Epoch: 11, iter 300: loss = 0.003725877031683922\n",
            "Epoch: 11, iter 400: loss = 0.0005733826546929777\n",
            "Epoch: 11, iter 500: loss = 0.5788906812667847\n",
            "Epoch: 11, iter 600: loss = 0.028031976893544197\n",
            "Epoch: 11, iter 700: loss = 0.005396210588514805\n",
            "Epoch: 11, iter 800: loss = 0.008906375616788864\n",
            "Epoch: 11, iter 900: loss = 0.3113938570022583\n",
            "Epoch: 11, iter 1000: loss = 0.30031388998031616\n",
            "Epoch: 11, iter 1100: loss = 0.0005316258757375181\n",
            "Epoch: 11, iter 1200: loss = 0.39918413758277893\n",
            "Epoch: 11, iter 1300: loss = 0.028500624001026154\n",
            "Epoch: 11, iter 1400: loss = 0.007068560924381018\n",
            "Epoch: 11, iter 1500: loss = 0.05369569733738899\n",
            "Epoch: 11, iter 1600: loss = 0.3057865500450134\n",
            "Epoch: 11, iter 1700: loss = 1.0475908517837524\n",
            "Epoch: 11, iter 1800: loss = 0.20571722090244293\n",
            "Epoch: 11, iter 1900: loss = 0.0011396383633837104\n",
            "Epoch: 11, iter 2000: loss = 0.007342961151152849\n",
            "Epoch: 11, iter 2100: loss = 0.01515231467783451\n",
            "Epoch: 11, iter 2200: loss = 0.42904534935951233\n",
            "Epoch: 11, iter 2300: loss = 0.4766009747982025\n",
            "Epoch: 11, iter 2400: loss = 0.002024152548983693\n",
            "Epoch: 11, iter 2500: loss = 0.0005188738577999175\n",
            "Epoch: 11, iter 2600: loss = 0.3265951871871948\n",
            "Epoch: 11, iter 2700: loss = 0.0016001566546037793\n",
            "Epoch: 11, iter 2800: loss = 0.15844601392745972\n",
            "Epoch: 11, iter 2900: loss = 0.0006055182311683893\n",
            "Epoch: 11, iter 3000: loss = 0.6689479351043701\n",
            "Epoch: 11, iter 3100: loss = 0.06665679812431335\n",
            "Epoch: 11, iter 3200: loss = 0.02014562301337719\n",
            "Epoch: 11, iter 3300: loss = 0.003262423910200596\n",
            "Epoch: 11, iter 3400: loss = 0.0024182212073355913\n",
            "Epoch: 11, iter 3500: loss = 0.34942182898521423\n",
            "Epoch: 11, iter 3600: loss = 0.3482409715652466\n",
            "Epoch: 11, iter 3700: loss = 0.24764025211334229\n",
            "Epoch: 11, iter 3800: loss = 0.8648313879966736\n",
            "Epoch: 11, iter 3900: loss = 0.010140778496861458\n",
            "Epoch: 11, iter 4000: loss = 0.04409857094287872\n",
            "Epoch: 11, iter 4100: loss = 0.6071158647537231\n",
            "Epoch: 11, iter 4200: loss = 0.586112916469574\n",
            "Epoch: 11, iter 4300: loss = 0.21665087342262268\n",
            "Epoch: 11, iter 4400: loss = 0.06825222074985504\n",
            "Epoch: 11, iter 4500: loss = 0.2557820677757263\n",
            "Epoch: 11, iter 4600: loss = 0.5921629071235657\n",
            "Epoch: 11, iter 4700: loss = 0.3429587781429291\n",
            "Epoch: 11, iter 4800: loss = 0.3661569654941559\n",
            "Epoch: 11, iter 4900: loss = 0.38531121611595154\n",
            "Epoch: 11, iter 5000: loss = 0.15541194379329681\n",
            "Epoch: 11, iter 5100: loss = 0.11822250485420227\n",
            "Epoch: 11, iter 5200: loss = 0.00587161909788847\n",
            "Epoch: 11, iter 5300: loss = 0.18454806506633759\n",
            "Epoch: 11, iter 5400: loss = 0.009058275260031223\n",
            "Epoch: 11, iter 5500: loss = 0.39004647731781006\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.86it/s]\n",
            "Epoch 11 loss average: 0.225\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8401    0.7721    0.8047     11182\n",
            "           2     0.6702    0.5222    0.5870       969\n",
            "           3     0.7809    0.6881    0.7316      1600\n",
            "           4     0.6826    0.5514    0.6100       827\n",
            "           5     0.5217    0.0822    0.1420       146\n",
            "           6     0.5636    0.4092    0.4742       303\n",
            "\n",
            "   micro avg     0.8113    0.7209    0.7634     15027\n",
            "   macro avg     0.6765    0.5042    0.5582     15027\n",
            "weighted avg     0.8055    0.7209    0.7591     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5954    0.5633    0.5789      1019\n",
            "           2     0.3415    0.2745    0.3043       102\n",
            "           3     0.3216    0.5517    0.4063       116\n",
            "           4     0.4265    0.2458    0.3118       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.6000    0.1915    0.2903        47\n",
            "\n",
            "   micro avg     0.5297    0.4961    0.5124      1419\n",
            "   macro avg     0.3808    0.3045    0.3153      1419\n",
            "weighted avg     0.5338    0.4961    0.5064      1419\n",
            "\n",
            "0.2246643594731571 0.759056617311833 0.5310136260866324 0.5063715075093372\n",
            "Patience counter: 9\n",
            "Epoch: 12, iter 0: loss = 0.0019998783245682716\n",
            "Epoch: 12, iter 100: loss = 0.3694412410259247\n",
            "Epoch: 12, iter 200: loss = 0.046905986964702606\n",
            "Epoch: 12, iter 300: loss = 0.19082456827163696\n",
            "Epoch: 12, iter 400: loss = 0.461896151304245\n",
            "Epoch: 12, iter 500: loss = 0.009016632102429867\n",
            "Epoch: 12, iter 600: loss = 0.10995277762413025\n",
            "Epoch: 12, iter 700: loss = 0.2231530100107193\n",
            "Epoch: 12, iter 800: loss = 0.16306175291538239\n",
            "Epoch: 12, iter 900: loss = 0.0005622843746095896\n",
            "Epoch: 12, iter 1000: loss = 0.002115773968398571\n",
            "Epoch: 12, iter 1100: loss = 0.10356567800045013\n",
            "Epoch: 12, iter 1200: loss = 0.20092840492725372\n",
            "Epoch: 12, iter 1300: loss = 0.000389023101888597\n",
            "Epoch: 12, iter 1400: loss = 0.6724416613578796\n",
            "Epoch: 12, iter 1500: loss = 0.40181875228881836\n",
            "Epoch: 12, iter 1600: loss = 0.009328947402536869\n",
            "Epoch: 12, iter 1700: loss = 0.0037089898250997066\n",
            "Epoch: 12, iter 1800: loss = 0.00040806762990541756\n",
            "Epoch: 12, iter 1900: loss = 1.075413465499878\n",
            "Epoch: 12, iter 2000: loss = 0.0012922906316816807\n",
            "Epoch: 12, iter 2100: loss = 0.2521437108516693\n",
            "Epoch: 12, iter 2200: loss = 0.8205704092979431\n",
            "Epoch: 12, iter 2300: loss = 0.19476856291294098\n",
            "Epoch: 12, iter 2400: loss = 0.10636593401432037\n",
            "Epoch: 12, iter 2500: loss = 0.055877935141325\n",
            "Epoch: 12, iter 2600: loss = 0.02051519602537155\n",
            "Epoch: 12, iter 2700: loss = 0.01868807151913643\n",
            "Epoch: 12, iter 2800: loss = 0.6938065886497498\n",
            "Epoch: 12, iter 2900: loss = 0.5443098545074463\n",
            "Epoch: 12, iter 3000: loss = 0.004513060208410025\n",
            "Epoch: 12, iter 3100: loss = 0.014420021325349808\n",
            "Epoch: 12, iter 3200: loss = 1.0799909830093384\n",
            "Epoch: 12, iter 3300: loss = 0.011409418657422066\n",
            "Epoch: 12, iter 3400: loss = 0.4237087070941925\n",
            "Epoch: 12, iter 3500: loss = 2.177935838699341\n",
            "Epoch: 12, iter 3600: loss = 0.21568211913108826\n",
            "Epoch: 12, iter 3700: loss = 0.11930689215660095\n",
            "Epoch: 12, iter 3800: loss = 0.003921082708984613\n",
            "Epoch: 12, iter 3900: loss = 0.031552884727716446\n",
            "Epoch: 12, iter 4000: loss = 0.005583405494689941\n",
            "Epoch: 12, iter 4100: loss = 0.33097314834594727\n",
            "Epoch: 12, iter 4200: loss = 0.08535440266132355\n",
            "Epoch: 12, iter 4300: loss = 0.42483898997306824\n",
            "Epoch: 12, iter 4400: loss = 0.14653614163398743\n",
            "Epoch: 12, iter 4500: loss = 0.10607784241437912\n",
            "Epoch: 12, iter 4600: loss = 0.028034234419465065\n",
            "Epoch: 12, iter 4700: loss = 0.003864146303385496\n",
            "Epoch: 12, iter 4800: loss = 0.15036414563655853\n",
            "Epoch: 12, iter 4900: loss = 0.0824713483452797\n",
            "Epoch: 12, iter 5000: loss = 0.18067309260368347\n",
            "Epoch: 12, iter 5100: loss = 0.49673977494239807\n",
            "Epoch: 12, iter 5200: loss = 0.002504536882042885\n",
            "Epoch: 12, iter 5300: loss = 0.0018254481256008148\n",
            "Epoch: 12, iter 5400: loss = 0.38548776507377625\n",
            "Epoch: 12, iter 5500: loss = 0.058167971670627594\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.84it/s]\n",
            "Epoch 12 loss average: 0.211\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8504    0.7903    0.8192     11182\n",
            "           2     0.7565    0.5418    0.6314       969\n",
            "           3     0.7912    0.6963    0.7407      1600\n",
            "           4     0.6989    0.5417    0.6104       827\n",
            "           5     0.5588    0.1301    0.2111       146\n",
            "           6     0.5865    0.4026    0.4775       303\n",
            "\n",
            "   micro avg     0.8272    0.7363    0.7791     15027\n",
            "   macro avg     0.7071    0.5171    0.5817     15027\n",
            "weighted avg     0.8215    0.7363    0.7745     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5598    0.5741    0.5669      1019\n",
            "           2     0.3867    0.2843    0.3277       102\n",
            "           3     0.4649    0.4569    0.4609       116\n",
            "           4     0.4286    0.2542    0.3191       118\n",
            "           5     0.3333    0.0588    0.1000        17\n",
            "           6     0.4286    0.1915    0.2647        47\n",
            "\n",
            "   micro avg     0.5324    0.4982    0.5147      1419\n",
            "   macro avg     0.4336    0.3033    0.3399      1419\n",
            "weighted avg     0.5216    0.4982    0.5048      1419\n",
            "\n",
            "0.21058722990160145 0.7744581862912654 0.5314116978732442 0.5048034977244107\n",
            "Patience counter: 10\n",
            "Epoch: 13, iter 0: loss = 0.0018585221841931343\n",
            "Epoch: 13, iter 100: loss = 0.015589644201099873\n",
            "Epoch: 13, iter 200: loss = 0.046546123921871185\n",
            "Epoch: 13, iter 300: loss = 0.06999741494655609\n",
            "Epoch: 13, iter 400: loss = 0.8544918298721313\n",
            "Epoch: 13, iter 500: loss = 0.24373680353164673\n",
            "Epoch: 13, iter 600: loss = 0.2468981146812439\n",
            "Epoch: 13, iter 700: loss = 0.10109691321849823\n",
            "Epoch: 13, iter 800: loss = 0.7357915639877319\n",
            "Epoch: 13, iter 900: loss = 0.10802074521780014\n",
            "Epoch: 13, iter 1000: loss = 0.9485520720481873\n",
            "Epoch: 13, iter 1100: loss = 0.00786344800144434\n",
            "Epoch: 13, iter 1200: loss = 0.15934619307518005\n",
            "Epoch: 13, iter 1300: loss = 0.03956037759780884\n",
            "Epoch: 13, iter 1400: loss = 0.0017197923734784126\n",
            "Epoch: 13, iter 1500: loss = 0.010679940693080425\n",
            "Epoch: 13, iter 1600: loss = 0.8421382904052734\n",
            "Epoch: 13, iter 1700: loss = 0.5049392580986023\n",
            "Epoch: 13, iter 1800: loss = 0.08329930901527405\n",
            "Epoch: 13, iter 1900: loss = 0.00022252554481383413\n",
            "Epoch: 13, iter 2000: loss = 0.21010541915893555\n",
            "Epoch: 13, iter 2100: loss = 0.03647785261273384\n",
            "Epoch: 13, iter 2200: loss = 0.8242422938346863\n",
            "Epoch: 13, iter 2300: loss = 0.059288520365953445\n",
            "Epoch: 13, iter 2400: loss = 0.013962273485958576\n",
            "Epoch: 13, iter 2500: loss = 0.00037399845314212143\n",
            "Epoch: 13, iter 2600: loss = 0.010055476799607277\n",
            "Epoch: 13, iter 2700: loss = 0.0003388350596651435\n",
            "Epoch: 13, iter 2800: loss = 0.008749683387577534\n",
            "Epoch: 13, iter 2900: loss = 0.0011556335957720876\n",
            "Epoch: 13, iter 3000: loss = 0.03149536997079849\n",
            "Epoch: 13, iter 3100: loss = 0.021326592192053795\n",
            "Epoch: 13, iter 3200: loss = 0.001804501167498529\n",
            "Epoch: 13, iter 3300: loss = 0.07917848974466324\n",
            "Epoch: 13, iter 3400: loss = 0.25961145758628845\n",
            "Epoch: 13, iter 3500: loss = 0.15976087749004364\n",
            "Epoch: 13, iter 3600: loss = 0.1943756639957428\n",
            "Epoch: 13, iter 3700: loss = 0.00025461416225880384\n",
            "Epoch: 13, iter 3800: loss = 0.14269091188907623\n",
            "Epoch: 13, iter 3900: loss = 0.3965080976486206\n",
            "Epoch: 13, iter 4000: loss = 0.16045524179935455\n",
            "Epoch: 13, iter 4100: loss = 0.07341248542070389\n",
            "Epoch: 13, iter 4200: loss = 0.4338412880897522\n",
            "Epoch: 13, iter 4300: loss = 0.09601682424545288\n",
            "Epoch: 13, iter 4400: loss = 0.5834358334541321\n",
            "Epoch: 13, iter 4500: loss = 0.12514141201972961\n",
            "Epoch: 13, iter 4600: loss = 0.00013494864106178284\n",
            "Epoch: 13, iter 4700: loss = 0.4088301360607147\n",
            "Epoch: 13, iter 4800: loss = 0.2756499946117401\n",
            "Epoch: 13, iter 4900: loss = 0.21061615645885468\n",
            "Epoch: 13, iter 5000: loss = 0.12621919810771942\n",
            "Epoch: 13, iter 5100: loss = 0.7258186936378479\n",
            "Epoch: 13, iter 5200: loss = 0.022748028859496117\n",
            "Epoch: 13, iter 5300: loss = 0.006614407990127802\n",
            "Epoch: 13, iter 5400: loss = 0.06319371610879898\n",
            "Epoch: 13, iter 5500: loss = 0.6803723573684692\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:11<00:00, 12.87it/s]\n",
            "Epoch 13 loss average: 0.205\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8499    0.7991    0.8237     11182\n",
            "           2     0.7148    0.5872    0.6448       969\n",
            "           3     0.7757    0.7113    0.7421      1600\n",
            "           4     0.7188    0.5502    0.6233       827\n",
            "           5     0.6667    0.1644    0.2637       146\n",
            "           6     0.6422    0.4620    0.5374       303\n",
            "\n",
            "   micro avg     0.8242    0.7494    0.7850     15027\n",
            "   macro avg     0.7280    0.5457    0.6058     15027\n",
            "weighted avg     0.8201    0.7494    0.7812     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6494    0.4308    0.5180      1019\n",
            "           2     0.3210    0.2549    0.2842       102\n",
            "           3     0.4261    0.4224    0.4242       116\n",
            "           4     0.4714    0.2797    0.3511       118\n",
            "           5     0.3333    0.0588    0.1000        17\n",
            "           6     0.4545    0.2128    0.2899        47\n",
            "\n",
            "   micro avg     0.5770    0.3932    0.4677      1419\n",
            "   macro avg     0.4426    0.2766    0.3279      1419\n",
            "weighted avg     0.5825    0.3932    0.4671      1419\n",
            "\n",
            "0.20491699220162402 0.7812238615769984 0.47905498013285364 0.46707571267849124\n",
            "Patience counter: 11\n",
            "Done! It took 6.1e+03 secs\n",
            "\n",
            "Current RUN: 4\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.49387076300848276\n",
            "Best test f1 weighted\n",
            "0.4258507521458706\n",
            "Best epoch\n",
            "2\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/conversation_length.pkl'),\n",
            " 'data': 'dailydialog',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/ubuntu_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 7,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/dailydialog/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [7, 256]\n",
            "\tdecoder2output.linears.0.bias\t [7]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 6.0 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 2.1642050743103027\n",
            "Epoch: 1, iter 100: loss = 0.5298750400543213\n",
            "Epoch: 1, iter 200: loss = 0.06834570318460464\n",
            "Epoch: 1, iter 300: loss = 0.8083975315093994\n",
            "Epoch: 1, iter 400: loss = 0.4312227964401245\n",
            "Epoch: 1, iter 500: loss = 0.3461465537548065\n",
            "Epoch: 1, iter 600: loss = 0.22365787625312805\n",
            "Epoch: 1, iter 700: loss = 0.465177983045578\n",
            "Epoch: 1, iter 800: loss = 0.16318391263484955\n",
            "Epoch: 1, iter 900: loss = 0.4563770592212677\n",
            "Epoch: 1, iter 1000: loss = 0.09737863391637802\n",
            "Epoch: 1, iter 1100: loss = 0.8190577030181885\n",
            "Epoch: 1, iter 1200: loss = 0.061559129506349564\n",
            "Epoch: 1, iter 1300: loss = 1.0719770193099976\n",
            "Epoch: 1, iter 1400: loss = 1.239553451538086\n",
            "Epoch: 1, iter 1500: loss = 0.3768632113933563\n",
            "Epoch: 1, iter 1600: loss = 0.6250063180923462\n",
            "Epoch: 1, iter 1700: loss = 0.20530155301094055\n",
            "Epoch: 1, iter 1800: loss = 0.08300507068634033\n",
            "Epoch: 1, iter 1900: loss = 0.7585074305534363\n",
            "Epoch: 1, iter 2000: loss = 0.09053550660610199\n",
            "Epoch: 1, iter 2100: loss = 0.022214673459529877\n",
            "Epoch: 1, iter 2200: loss = 0.0474541075527668\n",
            "Epoch: 1, iter 2300: loss = 0.12365929037332535\n",
            "Epoch: 1, iter 2400: loss = 0.7011535167694092\n",
            "Epoch: 1, iter 2500: loss = 0.565188467502594\n",
            "Epoch: 1, iter 2600: loss = 0.8290948867797852\n",
            "Epoch: 1, iter 2700: loss = 0.4540333151817322\n",
            "Epoch: 1, iter 2800: loss = 0.49773022532463074\n",
            "Epoch: 1, iter 2900: loss = 0.7294928431510925\n",
            "Epoch: 1, iter 3000: loss = 0.6626695990562439\n",
            "Epoch: 1, iter 3100: loss = 0.6551950573921204\n",
            "Epoch: 1, iter 3200: loss = 0.5313076376914978\n",
            "Epoch: 1, iter 3300: loss = 0.08761437237262726\n",
            "Epoch: 1, iter 3400: loss = 1.0386255979537964\n",
            "Epoch: 1, iter 3500: loss = 0.6341314315795898\n",
            "Epoch: 1, iter 3600: loss = 0.687300443649292\n",
            "Epoch: 1, iter 3700: loss = 0.03750458359718323\n",
            "Epoch: 1, iter 3800: loss = 0.631501317024231\n",
            "Epoch: 1, iter 3900: loss = 0.03619837388396263\n",
            "Epoch: 1, iter 4000: loss = 0.06207829713821411\n",
            "Epoch: 1, iter 4100: loss = 0.0195394828915596\n",
            "Epoch: 1, iter 4200: loss = 0.09879418462514877\n",
            "Epoch: 1, iter 4300: loss = 0.29443588852882385\n",
            "Epoch: 1, iter 4400: loss = 0.5896922945976257\n",
            "Epoch: 1, iter 4500: loss = 0.42061153054237366\n",
            "Epoch: 1, iter 4600: loss = 0.6623626947402954\n",
            "Epoch: 1, iter 4700: loss = 0.19388383626937866\n",
            "Epoch: 1, iter 4800: loss = 0.2475661039352417\n",
            "Epoch: 1, iter 4900: loss = 0.08411842584609985\n",
            "Epoch: 1, iter 5000: loss = 0.018329545855522156\n",
            "Epoch: 1, iter 5100: loss = 0.28018566966056824\n",
            "Epoch: 1, iter 5200: loss = 0.3867484927177429\n",
            "Epoch: 1, iter 5300: loss = 0.16571271419525146\n",
            "Epoch: 1, iter 5400: loss = 0.019550414755940437\n",
            "Epoch: 1, iter 5500: loss = 0.2587854266166687\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:15<00:00, 12.76it/s]\n",
            "Epoch 1 loss average: 0.482\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6382    0.3856    0.4808     11182\n",
            "           2     0.0000    0.0000    0.0000       969\n",
            "           3     0.5677    0.2750    0.3705      1600\n",
            "           4     0.3333    0.0012    0.0024       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.0000    0.0000    0.0000       303\n",
            "\n",
            "   micro avg     0.6294    0.3163    0.4210     15027\n",
            "   macro avg     0.2566    0.1103    0.1423     15027\n",
            "weighted avg     0.5537    0.3163    0.3973     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7336    0.3405    0.4651      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.6462    0.3621    0.4641       116\n",
            "           4     0.0000    0.0000    0.0000       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.7230    0.2741    0.3975      1419\n",
            "   macro avg     0.2300    0.1171    0.1549      1419\n",
            "weighted avg     0.5796    0.2741    0.3720      1419\n",
            "\n",
            "0.4817154132178042 0.39733629061446185 0.4244196984150142 0.3719658272271567\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 0.7904958724975586\n",
            "Epoch: 2, iter 100: loss = 0.02944437600672245\n",
            "Epoch: 2, iter 200: loss = 0.5738941431045532\n",
            "Epoch: 2, iter 300: loss = 0.04418790340423584\n",
            "Epoch: 2, iter 400: loss = 0.47164905071258545\n",
            "Epoch: 2, iter 500: loss = 0.23862406611442566\n",
            "Epoch: 2, iter 600: loss = 0.11048047244548798\n",
            "Epoch: 2, iter 700: loss = 0.03303058445453644\n",
            "Epoch: 2, iter 800: loss = 0.08155246824026108\n",
            "Epoch: 2, iter 900: loss = 0.1063588336110115\n",
            "Epoch: 2, iter 1000: loss = 0.0420580729842186\n",
            "Epoch: 2, iter 1100: loss = 0.011336944997310638\n",
            "Epoch: 2, iter 1200: loss = 0.028090741485357285\n",
            "Epoch: 2, iter 1300: loss = 0.1599600464105606\n",
            "Epoch: 2, iter 1400: loss = 0.8292659521102905\n",
            "Epoch: 2, iter 1500: loss = 0.03875763714313507\n",
            "Epoch: 2, iter 1600: loss = 0.2891606390476227\n",
            "Epoch: 2, iter 1700: loss = 0.6649020314216614\n",
            "Epoch: 2, iter 1800: loss = 0.08232171088457108\n",
            "Epoch: 2, iter 1900: loss = 0.6917575001716614\n",
            "Epoch: 2, iter 2000: loss = 0.2155051976442337\n",
            "Epoch: 2, iter 2100: loss = 0.13847653567790985\n",
            "Epoch: 2, iter 2200: loss = 0.35535913705825806\n",
            "Epoch: 2, iter 2300: loss = 0.1088227853178978\n",
            "Epoch: 2, iter 2400: loss = 0.09954539686441422\n",
            "Epoch: 2, iter 2500: loss = 0.04487346485257149\n",
            "Epoch: 2, iter 2600: loss = 0.5452238321304321\n",
            "Epoch: 2, iter 2700: loss = 0.3344251215457916\n",
            "Epoch: 2, iter 2800: loss = 0.4255176782608032\n",
            "Epoch: 2, iter 2900: loss = 0.10872899740934372\n",
            "Epoch: 2, iter 3000: loss = 1.4268444776535034\n",
            "Epoch: 2, iter 3100: loss = 0.5473405122756958\n",
            "Epoch: 2, iter 3200: loss = 0.2916523814201355\n",
            "Epoch: 2, iter 3300: loss = 0.5702477097511292\n",
            "Epoch: 2, iter 3400: loss = 0.06044698879122734\n",
            "Epoch: 2, iter 3500: loss = 0.015644220635294914\n",
            "Epoch: 2, iter 3600: loss = 0.6964847445487976\n",
            "Epoch: 2, iter 3700: loss = 0.19241544604301453\n",
            "Epoch: 2, iter 3800: loss = 0.3593984842300415\n",
            "Epoch: 2, iter 3900: loss = 0.7379932999610901\n",
            "Epoch: 2, iter 4000: loss = 0.18212272226810455\n",
            "Epoch: 2, iter 4100: loss = 0.2407723069190979\n",
            "Epoch: 2, iter 4200: loss = 0.0601203627884388\n",
            "Epoch: 2, iter 4300: loss = 0.34518489241600037\n",
            "Epoch: 2, iter 4400: loss = 0.2965778410434723\n",
            "Epoch: 2, iter 4500: loss = 0.40464532375335693\n",
            "Epoch: 2, iter 4600: loss = 0.14766308665275574\n",
            "Epoch: 2, iter 4700: loss = 0.05030515417456627\n",
            "Epoch: 2, iter 4800: loss = 0.35120296478271484\n",
            "Epoch: 2, iter 4900: loss = 0.06295758485794067\n",
            "Epoch: 2, iter 5000: loss = 0.04166639223694801\n",
            "Epoch: 2, iter 5100: loss = 0.20509304106235504\n",
            "Epoch: 2, iter 5200: loss = 0.15900921821594238\n",
            "Epoch: 2, iter 5300: loss = 0.14588682353496552\n",
            "Epoch: 2, iter 5400: loss = 0.5096008777618408\n",
            "Epoch: 2, iter 5500: loss = 0.05058954283595085\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:15<00:00, 12.77it/s]\n",
            "Epoch 2 loss average: 0.401\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6904    0.5082    0.5855     11182\n",
            "           2     0.3462    0.0186    0.0353       969\n",
            "           3     0.6376    0.4794    0.5473      1600\n",
            "           4     0.3053    0.0484    0.0835       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3333    0.0165    0.0314       303\n",
            "\n",
            "   micro avg     0.6762    0.4334    0.5282     15027\n",
            "   macro avg     0.3855    0.1785    0.2138     15027\n",
            "weighted avg     0.6275    0.4334    0.5014     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6722    0.4809    0.5606      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.4324    0.5517    0.4848       116\n",
            "           4     0.3000    0.0254    0.0469       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6280    0.3925    0.4831      1419\n",
            "   macro avg     0.2341    0.1763    0.1821      1419\n",
            "weighted avg     0.5430    0.3925    0.4461      1419\n",
            "\n",
            "0.4008277115506881 0.5014487371001759 0.485521636061579 0.44613571558908666\n",
            "Patience counter: 1\n",
            "Epoch: 3, iter 0: loss = 0.02274024486541748\n",
            "Epoch: 3, iter 100: loss = 0.3821248710155487\n",
            "Epoch: 3, iter 200: loss = 0.2724268436431885\n",
            "Epoch: 3, iter 300: loss = 0.05829586088657379\n",
            "Epoch: 3, iter 400: loss = 0.16721126437187195\n",
            "Epoch: 3, iter 500: loss = 0.26504701375961304\n",
            "Epoch: 3, iter 600: loss = 0.2022307813167572\n",
            "Epoch: 3, iter 700: loss = 0.15121804177761078\n",
            "Epoch: 3, iter 800: loss = 0.02286585420370102\n",
            "Epoch: 3, iter 900: loss = 0.3555363714694977\n",
            "Epoch: 3, iter 1000: loss = 0.29375651478767395\n",
            "Epoch: 3, iter 1100: loss = 0.11001309752464294\n",
            "Epoch: 3, iter 1200: loss = 0.23536348342895508\n",
            "Epoch: 3, iter 1300: loss = 0.5245668888092041\n",
            "Epoch: 3, iter 1400: loss = 0.331026166677475\n",
            "Epoch: 3, iter 1500: loss = 0.10380455106496811\n",
            "Epoch: 3, iter 1600: loss = 0.016466863453388214\n",
            "Epoch: 3, iter 1700: loss = 1.5452862977981567\n",
            "Epoch: 3, iter 1800: loss = 0.1616591364145279\n",
            "Epoch: 3, iter 1900: loss = 1.9199681282043457\n",
            "Epoch: 3, iter 2000: loss = 0.28268253803253174\n",
            "Epoch: 3, iter 2100: loss = 0.617560863494873\n",
            "Epoch: 3, iter 2200: loss = 0.010223346762359142\n",
            "Epoch: 3, iter 2300: loss = 0.49639225006103516\n",
            "Epoch: 3, iter 2400: loss = 0.17737771570682526\n",
            "Epoch: 3, iter 2500: loss = 0.42713451385498047\n",
            "Epoch: 3, iter 2600: loss = 0.20700675249099731\n",
            "Epoch: 3, iter 2700: loss = 0.8871908187866211\n",
            "Epoch: 3, iter 2800: loss = 0.5966030359268188\n",
            "Epoch: 3, iter 2900: loss = 0.13094013929367065\n",
            "Epoch: 3, iter 3000: loss = 0.1018211841583252\n",
            "Epoch: 3, iter 3100: loss = 0.7526155710220337\n",
            "Epoch: 3, iter 3200: loss = 0.32153594493865967\n",
            "Epoch: 3, iter 3300: loss = 0.646186888217926\n",
            "Epoch: 3, iter 3400: loss = 0.27518218755722046\n",
            "Epoch: 3, iter 3500: loss = 0.036525797098875046\n",
            "Epoch: 3, iter 3600: loss = 0.13955917954444885\n",
            "Epoch: 3, iter 3700: loss = 0.19845816493034363\n",
            "Epoch: 3, iter 3800: loss = 0.09722249209880829\n",
            "Epoch: 3, iter 3900: loss = 0.03914631903171539\n",
            "Epoch: 3, iter 4000: loss = 0.30329108238220215\n",
            "Epoch: 3, iter 4100: loss = 0.013560923747718334\n",
            "Epoch: 3, iter 4200: loss = 1.3421334028244019\n",
            "Epoch: 3, iter 4300: loss = 0.3481416404247284\n",
            "Epoch: 3, iter 4400: loss = 0.046970248222351074\n",
            "Epoch: 3, iter 4500: loss = 0.16055019199848175\n",
            "Epoch: 3, iter 4600: loss = 0.04778978228569031\n",
            "Epoch: 3, iter 4700: loss = 0.7543174028396606\n",
            "Epoch: 3, iter 4800: loss = 0.31028056144714355\n",
            "Epoch: 3, iter 4900: loss = 0.09451109915971756\n",
            "Epoch: 3, iter 5000: loss = 0.020921682938933372\n",
            "Epoch: 3, iter 5100: loss = 0.10158628970384598\n",
            "Epoch: 3, iter 5200: loss = 1.789080262184143\n",
            "Epoch: 3, iter 5300: loss = 0.36590081453323364\n",
            "Epoch: 3, iter 5400: loss = 0.16943590342998505\n",
            "Epoch: 3, iter 5500: loss = 0.3289514183998108\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:14<00:00, 12.78it/s]\n",
            "Epoch 3 loss average: 0.360\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7271    0.5808    0.6458     11182\n",
            "           2     0.4575    0.0722    0.1248       969\n",
            "           3     0.6698    0.5425    0.5994      1600\n",
            "           4     0.3802    0.1669    0.2319       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2703    0.0330    0.0588       303\n",
            "\n",
            "   micro avg     0.7031    0.5045    0.5875     15027\n",
            "   macro avg     0.4175    0.2326    0.2768     15027\n",
            "weighted avg     0.6682    0.5045    0.5664     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7105    0.3710    0.4874      1019\n",
            "           2     0.0000    0.0000    0.0000       102\n",
            "           3     0.5275    0.4138    0.4638       116\n",
            "           4     0.3780    0.2627    0.3100       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.6482    0.3221    0.4303      1419\n",
            "   macro avg     0.2693    0.1746    0.2102      1419\n",
            "weighted avg     0.5848    0.3221    0.4137      1419\n",
            "\n",
            "0.3599254846192825 0.5663701483684421 0.44968485198041425 0.41371789249965635\n",
            "Patience counter: 2\n",
            "Epoch: 4, iter 0: loss = 0.26341861486434937\n",
            "Epoch: 4, iter 100: loss = 0.06321326643228531\n",
            "Epoch: 4, iter 200: loss = 0.3330494463443756\n",
            "Epoch: 4, iter 300: loss = 0.212825745344162\n",
            "Epoch: 4, iter 400: loss = 0.369463175535202\n",
            "Epoch: 4, iter 500: loss = 0.2117759734392166\n",
            "Epoch: 4, iter 600: loss = 0.8576040863990784\n",
            "Epoch: 4, iter 700: loss = 0.010580264031887054\n",
            "Epoch: 4, iter 800: loss = 0.3642122447490692\n",
            "Epoch: 4, iter 900: loss = 0.169878289103508\n",
            "Epoch: 4, iter 1000: loss = 0.8470584154129028\n",
            "Epoch: 4, iter 1100: loss = 0.2682543992996216\n",
            "Epoch: 4, iter 1200: loss = 0.23167188465595245\n",
            "Epoch: 4, iter 1300: loss = 0.9809450507164001\n",
            "Epoch: 4, iter 1400: loss = 0.38644352555274963\n",
            "Epoch: 4, iter 1500: loss = 0.05853109061717987\n",
            "Epoch: 4, iter 1600: loss = 0.10726355016231537\n",
            "Epoch: 4, iter 1700: loss = 0.684005081653595\n",
            "Epoch: 4, iter 1800: loss = 0.09003739804029465\n",
            "Epoch: 4, iter 1900: loss = 0.12361694127321243\n",
            "Epoch: 4, iter 2000: loss = 0.6768360137939453\n",
            "Epoch: 4, iter 2100: loss = 0.19746287167072296\n",
            "Epoch: 4, iter 2200: loss = 0.09213331341743469\n",
            "Epoch: 4, iter 2300: loss = 0.13226735591888428\n",
            "Epoch: 4, iter 2400: loss = 0.4733748435974121\n",
            "Epoch: 4, iter 2500: loss = 0.03563582897186279\n",
            "Epoch: 4, iter 2600: loss = 0.2064949870109558\n",
            "Epoch: 4, iter 2700: loss = 0.7168781161308289\n",
            "Epoch: 4, iter 2800: loss = 0.031847137957811356\n",
            "Epoch: 4, iter 2900: loss = 0.20755501091480255\n",
            "Epoch: 4, iter 3000: loss = 0.3652828335762024\n",
            "Epoch: 4, iter 3100: loss = 0.2822776436805725\n",
            "Epoch: 4, iter 3200: loss = 0.581519603729248\n",
            "Epoch: 4, iter 3300: loss = 0.146028533577919\n",
            "Epoch: 4, iter 3400: loss = 0.05587601661682129\n",
            "Epoch: 4, iter 3500: loss = 0.0033981213346123695\n",
            "Epoch: 4, iter 3600: loss = 0.7097011804580688\n",
            "Epoch: 4, iter 3700: loss = 0.11922232061624527\n",
            "Epoch: 4, iter 3800: loss = 0.46867361664772034\n",
            "Epoch: 4, iter 3900: loss = 0.0027663393411785364\n",
            "Epoch: 4, iter 4000: loss = 0.6643030643463135\n",
            "Epoch: 4, iter 4100: loss = 0.25702613592147827\n",
            "Epoch: 4, iter 4200: loss = 0.7026473879814148\n",
            "Epoch: 4, iter 4300: loss = 1.4234349727630615\n",
            "Epoch: 4, iter 4400: loss = 0.07907619327306747\n",
            "Epoch: 4, iter 4500: loss = 0.9214345812797546\n",
            "Epoch: 4, iter 4600: loss = 1.6702862977981567\n",
            "Epoch: 4, iter 4700: loss = 0.7259010672569275\n",
            "Epoch: 4, iter 4800: loss = 0.029575126245617867\n",
            "Epoch: 4, iter 4900: loss = 0.3734992444515228\n",
            "Epoch: 4, iter 5000: loss = 0.04551773518323898\n",
            "Epoch: 4, iter 5100: loss = 0.15832316875457764\n",
            "Epoch: 4, iter 5200: loss = 0.019810138270258904\n",
            "Epoch: 4, iter 5300: loss = 0.2016948014497757\n",
            "Epoch: 4, iter 5400: loss = 0.4640531837940216\n",
            "Epoch: 4, iter 5500: loss = 0.08478673547506332\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:15<00:00, 12.77it/s]\n",
            "Epoch 4 loss average: 0.326\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7520    0.6274    0.6841     11182\n",
            "           2     0.4760    0.1641    0.2441       969\n",
            "           3     0.6870    0.5763    0.6268      1600\n",
            "           4     0.5009    0.3229    0.3926       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.2131    0.0429    0.0714       303\n",
            "\n",
            "   micro avg     0.7222    0.5575    0.6292     15027\n",
            "   macro avg     0.4382    0.2889    0.3365     15027\n",
            "weighted avg     0.6953    0.5575    0.6146     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6326    0.5407    0.5831      1019\n",
            "           2     0.3333    0.1078    0.1630       102\n",
            "           3     0.5091    0.4828    0.4956       116\n",
            "           4     0.3418    0.2288    0.2741       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.0000    0.0000    0.0000        47\n",
            "\n",
            "   micro avg     0.5901    0.4545    0.5135      1419\n",
            "   macro avg     0.3028    0.2267    0.2526      1419\n",
            "weighted avg     0.5483    0.4545    0.4937      1419\n",
            "\n",
            "0.3263264997045718 0.6145716805340262 0.545111591852094 0.4937288340366481\n",
            "Patience counter: 3\n",
            "Epoch: 5, iter 0: loss = 0.22171840071678162\n",
            "Epoch: 5, iter 100: loss = 0.14645835757255554\n",
            "Epoch: 5, iter 200: loss = 0.00429800059646368\n",
            "Epoch: 5, iter 300: loss = 0.1464429646730423\n",
            "Epoch: 5, iter 400: loss = 0.02571241557598114\n",
            "Epoch: 5, iter 500: loss = 0.018851516768336296\n",
            "Epoch: 5, iter 600: loss = 0.014057504944503307\n",
            "Epoch: 5, iter 700: loss = 0.1207333356142044\n",
            "Epoch: 5, iter 800: loss = 0.04698417708277702\n",
            "Epoch: 5, iter 900: loss = 0.023103909566998482\n",
            "Epoch: 5, iter 1000: loss = 0.331227570772171\n",
            "Epoch: 5, iter 1100: loss = 0.2706073820590973\n",
            "Epoch: 5, iter 1200: loss = 0.004215359687805176\n",
            "Epoch: 5, iter 1300: loss = 0.34288209676742554\n",
            "Epoch: 5, iter 1400: loss = 0.16793371737003326\n",
            "Epoch: 5, iter 1500: loss = 0.17751118540763855\n",
            "Epoch: 5, iter 1600: loss = 0.019012445583939552\n",
            "Epoch: 5, iter 1700: loss = 0.6105890870094299\n",
            "Epoch: 5, iter 1800: loss = 0.00930375512689352\n",
            "Epoch: 5, iter 1900: loss = 0.29655706882476807\n",
            "Epoch: 5, iter 2000: loss = 0.15711872279644012\n",
            "Epoch: 5, iter 2100: loss = 0.07706952095031738\n",
            "Epoch: 5, iter 2200: loss = 0.16717474162578583\n",
            "Epoch: 5, iter 2300: loss = 0.38488200306892395\n",
            "Epoch: 5, iter 2400: loss = 0.25309544801712036\n",
            "Epoch: 5, iter 2500: loss = 0.569275975227356\n",
            "Epoch: 5, iter 2600: loss = 0.008033046498894691\n",
            "Epoch: 5, iter 2700: loss = 0.25444579124450684\n",
            "Epoch: 5, iter 2800: loss = 0.2016383558511734\n",
            "Epoch: 5, iter 2900: loss = 0.0034553296864032745\n",
            "Epoch: 5, iter 3000: loss = 0.006988985929638147\n",
            "Epoch: 5, iter 3100: loss = 0.19037272036075592\n",
            "Epoch: 5, iter 3200: loss = 0.11235295981168747\n",
            "Epoch: 5, iter 3300: loss = 0.6957297921180725\n",
            "Epoch: 5, iter 3400: loss = 0.8888590335845947\n",
            "Epoch: 5, iter 3500: loss = 0.012989260256290436\n",
            "Epoch: 5, iter 3600: loss = 1.0421345233917236\n",
            "Epoch: 5, iter 3700: loss = 0.3353317975997925\n",
            "Epoch: 5, iter 3800: loss = 0.07817629724740982\n",
            "Epoch: 5, iter 3900: loss = 0.013150757178664207\n",
            "Epoch: 5, iter 4000: loss = 0.5288252234458923\n",
            "Epoch: 5, iter 4100: loss = 0.9937927722930908\n",
            "Epoch: 5, iter 4200: loss = 0.06498649716377258\n",
            "Epoch: 5, iter 4300: loss = 0.25442248582839966\n",
            "Epoch: 5, iter 4400: loss = 0.01139785349369049\n",
            "Epoch: 5, iter 4500: loss = 0.3778477907180786\n",
            "Epoch: 5, iter 4600: loss = 0.11508072167634964\n",
            "Epoch: 5, iter 4700: loss = 0.46385303139686584\n",
            "Epoch: 5, iter 4800: loss = 0.06294367462396622\n",
            "Epoch: 5, iter 4900: loss = 0.21262024343013763\n",
            "Epoch: 5, iter 5000: loss = 0.014213369227945805\n",
            "Epoch: 5, iter 5100: loss = 0.8814892172813416\n",
            "Epoch: 5, iter 5200: loss = 0.11101170629262924\n",
            "Epoch: 5, iter 5300: loss = 0.009332248009741306\n",
            "Epoch: 5, iter 5400: loss = 0.31467872858047485\n",
            "Epoch: 5, iter 5500: loss = 0.8112235069274902\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:15<00:00, 12.77it/s]\n",
            "Epoch 5 loss average: 0.297\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.7795    0.6738    0.7228     11182\n",
            "           2     0.6016    0.3086    0.4079       969\n",
            "           3     0.6997    0.6319    0.6640      1600\n",
            "           4     0.5390    0.3676    0.4371       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3474    0.1089    0.1658       303\n",
            "\n",
            "   micro avg     0.7485    0.6110    0.6728     15027\n",
            "   macro avg     0.4945    0.3485    0.3996     15027\n",
            "weighted avg     0.7300    0.6110    0.6623     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6673    0.3621    0.4695      1019\n",
            "           2     0.2903    0.0882    0.1353       102\n",
            "           3     0.5521    0.4569    0.5000       116\n",
            "           4     0.3878    0.1610    0.2275       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.2727    0.0638    0.1034        47\n",
            "\n",
            "   micro avg     0.6122    0.3192    0.4196      1419\n",
            "   macro avg     0.3617    0.1887    0.2393      1419\n",
            "weighted avg     0.5865    0.3192    0.4101      1419\n",
            "\n",
            "0.2970907764064433 0.6622539997377588 0.4328158557240893 0.41007919367731954\n",
            "Patience counter: 4\n",
            "Epoch: 6, iter 0: loss = 0.006104230415076017\n",
            "Epoch: 6, iter 100: loss = 0.6291106343269348\n",
            "Epoch: 6, iter 200: loss = 0.15442927181720734\n",
            "Epoch: 6, iter 300: loss = 0.42502567172050476\n",
            "Epoch: 6, iter 400: loss = 0.29238778352737427\n",
            "Epoch: 6, iter 500: loss = 0.09322531521320343\n",
            "Epoch: 6, iter 600: loss = 0.22837311029434204\n",
            "Epoch: 6, iter 700: loss = 0.006373368203639984\n",
            "Epoch: 6, iter 800: loss = 0.4980609714984894\n",
            "Epoch: 6, iter 900: loss = 0.07733999937772751\n",
            "Epoch: 6, iter 1000: loss = 1.1546146869659424\n",
            "Epoch: 6, iter 1100: loss = 0.0041276756674051285\n",
            "Epoch: 6, iter 1200: loss = 0.16842858493328094\n",
            "Epoch: 6, iter 1300: loss = 0.1875254213809967\n",
            "Epoch: 6, iter 1400: loss = 0.040786437690258026\n",
            "Epoch: 6, iter 1500: loss = 0.018042856827378273\n",
            "Epoch: 6, iter 1600: loss = 0.012155195698142052\n",
            "Epoch: 6, iter 1700: loss = 0.010829748585820198\n",
            "Epoch: 6, iter 1800: loss = 0.8008602261543274\n",
            "Epoch: 6, iter 1900: loss = 0.7309335470199585\n",
            "Epoch: 6, iter 2000: loss = 0.006487712264060974\n",
            "Epoch: 6, iter 2100: loss = 0.14346309006214142\n",
            "Epoch: 6, iter 2200: loss = 0.029753943905234337\n",
            "Epoch: 6, iter 2300: loss = 0.06291887909173965\n",
            "Epoch: 6, iter 2400: loss = 0.37585997581481934\n",
            "Epoch: 6, iter 2500: loss = 0.002925737528130412\n",
            "Epoch: 6, iter 2600: loss = 0.7996660470962524\n",
            "Epoch: 6, iter 2700: loss = 0.5534167885780334\n",
            "Epoch: 6, iter 2800: loss = 0.22113825380802155\n",
            "Epoch: 6, iter 2900: loss = 0.6320328116416931\n",
            "Epoch: 6, iter 3000: loss = 0.4346068203449249\n",
            "Epoch: 6, iter 3100: loss = 0.0010398271260783076\n",
            "Epoch: 6, iter 3200: loss = 0.0062933675944805145\n",
            "Epoch: 6, iter 3300: loss = 0.20637767016887665\n",
            "Epoch: 6, iter 3400: loss = 0.01729443483054638\n",
            "Epoch: 6, iter 3500: loss = 0.06503044068813324\n",
            "Epoch: 6, iter 3600: loss = 0.008685298264026642\n",
            "Epoch: 6, iter 3700: loss = 0.3592631220817566\n",
            "Epoch: 6, iter 3800: loss = 0.7365622520446777\n",
            "Epoch: 6, iter 3900: loss = 0.02823157235980034\n",
            "Epoch: 6, iter 4000: loss = 0.13106724619865417\n",
            "Epoch: 6, iter 4100: loss = 0.01420445553958416\n",
            "Epoch: 6, iter 4200: loss = 0.001733247423544526\n",
            "Epoch: 6, iter 4300: loss = 0.0015349120367318392\n",
            "Epoch: 6, iter 4400: loss = 0.411859393119812\n",
            "Epoch: 6, iter 4500: loss = 0.0045213233679533005\n",
            "Epoch: 6, iter 4600: loss = 0.029407186433672905\n",
            "Epoch: 6, iter 4700: loss = 4.962282180786133\n",
            "Epoch: 6, iter 4800: loss = 1.115103006362915\n",
            "Epoch: 6, iter 4900: loss = 0.016067905351519585\n",
            "Epoch: 6, iter 5000: loss = 0.16272132098674774\n",
            "Epoch: 6, iter 5100: loss = 0.2951103448867798\n",
            "Epoch: 6, iter 5200: loss = 0.07978558540344238\n",
            "Epoch: 6, iter 5300: loss = 0.006106983870267868\n",
            "Epoch: 6, iter 5400: loss = 0.17385418713092804\n",
            "Epoch: 6, iter 5500: loss = 0.1122736930847168\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:16<00:00, 12.74it/s]\n",
            "Epoch 6 loss average: 0.276\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8020    0.7023    0.7488     11182\n",
            "           2     0.6271    0.3870    0.4786       969\n",
            "           3     0.7111    0.6475    0.6778      1600\n",
            "           4     0.5667    0.4111    0.4765       827\n",
            "           5     0.0000    0.0000    0.0000       146\n",
            "           6     0.3869    0.1749    0.2409       303\n",
            "\n",
            "   micro avg     0.7673    0.6426    0.6995     15027\n",
            "   macro avg     0.5156    0.3871    0.4371     15027\n",
            "weighted avg     0.7519    0.6426    0.6913     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5986    0.5692    0.5835      1019\n",
            "           2     0.4359    0.1667    0.2411       102\n",
            "           3     0.5098    0.4483    0.4771       116\n",
            "           4     0.4189    0.2627    0.3229       118\n",
            "           5     1.0000    0.0588    0.1111        17\n",
            "           6     0.5000    0.0213    0.0408        47\n",
            "\n",
            "   micro avg     0.5746    0.4806    0.5234      1419\n",
            "   macro avg     0.5772    0.2545    0.2961      1419\n",
            "weighted avg     0.5662    0.4806    0.5049      1419\n",
            "\n",
            "0.2761878825333121 0.6913399415769811 0.5284917886815003 0.5048866400814201\n",
            "Patience counter: 5\n",
            "Epoch: 7, iter 0: loss = 0.026422064751386642\n",
            "Epoch: 7, iter 100: loss = 0.011655998416244984\n",
            "Epoch: 7, iter 200: loss = 0.03471861407160759\n",
            "Epoch: 7, iter 300: loss = 0.12664121389389038\n",
            "Epoch: 7, iter 400: loss = 0.042366109788417816\n",
            "Epoch: 7, iter 500: loss = 0.009494401514530182\n",
            "Epoch: 7, iter 600: loss = 0.30024096369743347\n",
            "Epoch: 7, iter 700: loss = 0.29663342237472534\n",
            "Epoch: 7, iter 800: loss = 0.25726479291915894\n",
            "Epoch: 7, iter 900: loss = 0.8772886991500854\n",
            "Epoch: 7, iter 1000: loss = 0.2195759266614914\n",
            "Epoch: 7, iter 1100: loss = 0.25023719668388367\n",
            "Epoch: 7, iter 1200: loss = 0.24349574744701385\n",
            "Epoch: 7, iter 1300: loss = 0.0006180337513796985\n",
            "Epoch: 7, iter 1400: loss = 0.07267973572015762\n",
            "Epoch: 7, iter 1500: loss = 0.3335720896720886\n",
            "Epoch: 7, iter 1600: loss = 0.3119274377822876\n",
            "Epoch: 7, iter 1700: loss = 0.007913416251540184\n",
            "Epoch: 7, iter 1800: loss = 0.1889735758304596\n",
            "Epoch: 7, iter 1900: loss = 0.002319548511877656\n",
            "Epoch: 7, iter 2000: loss = 0.04694214090704918\n",
            "Epoch: 7, iter 2100: loss = 0.16606485843658447\n",
            "Epoch: 7, iter 2200: loss = 0.006038060411810875\n",
            "Epoch: 7, iter 2300: loss = 0.07932053506374359\n",
            "Epoch: 7, iter 2400: loss = 0.6799160838127136\n",
            "Epoch: 7, iter 2500: loss = 0.0820486918091774\n",
            "Epoch: 7, iter 2600: loss = 0.6525998711585999\n",
            "Epoch: 7, iter 2700: loss = 0.009460139088332653\n",
            "Epoch: 7, iter 2800: loss = 0.00438601104542613\n",
            "Epoch: 7, iter 2900: loss = 0.00938422791659832\n",
            "Epoch: 7, iter 3000: loss = 0.6905343532562256\n",
            "Epoch: 7, iter 3100: loss = 0.4183772802352905\n",
            "Epoch: 7, iter 3200: loss = 0.3803541660308838\n",
            "Epoch: 7, iter 3300: loss = 0.06288092583417892\n",
            "Epoch: 7, iter 3400: loss = 0.16073918342590332\n",
            "Epoch: 7, iter 3500: loss = 0.00978031475096941\n",
            "Epoch: 7, iter 3600: loss = 0.0265898946672678\n",
            "Epoch: 7, iter 3700: loss = 1.229201316833496\n",
            "Epoch: 7, iter 3800: loss = 0.0011608218774199486\n",
            "Epoch: 7, iter 3900: loss = 0.009768461808562279\n",
            "Epoch: 7, iter 4000: loss = 0.5499305129051208\n",
            "Epoch: 7, iter 4100: loss = 0.13123749196529388\n",
            "Epoch: 7, iter 4200: loss = 0.003412639256566763\n",
            "Epoch: 7, iter 4300: loss = 0.5386852025985718\n",
            "Epoch: 7, iter 4400: loss = 0.9690155386924744\n",
            "Epoch: 7, iter 4500: loss = 0.41104698181152344\n",
            "Epoch: 7, iter 4600: loss = 0.48305144906044006\n",
            "Epoch: 7, iter 4700: loss = 0.6305471062660217\n",
            "Epoch: 7, iter 4800: loss = 0.003531455295160413\n",
            "Epoch: 7, iter 4900: loss = 0.4483138918876648\n",
            "Epoch: 7, iter 5000: loss = 0.06433966010808945\n",
            "Epoch: 7, iter 5100: loss = 0.06668482720851898\n",
            "Epoch: 7, iter 5200: loss = 0.3924553096294403\n",
            "Epoch: 7, iter 5300: loss = 0.9320387840270996\n",
            "Epoch: 7, iter 5400: loss = 0.7046303153038025\n",
            "Epoch: 7, iter 5500: loss = 0.2203301042318344\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:17<00:00, 12.71it/s]\n",
            "Epoch 7 loss average: 0.258\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8135    0.7284    0.7686     11182\n",
            "           2     0.6596    0.4479    0.5335       969\n",
            "           3     0.7458    0.6656    0.7034      1600\n",
            "           4     0.6276    0.4667    0.5354       827\n",
            "           5     0.2727    0.0205    0.0382       146\n",
            "           6     0.4305    0.2145    0.2863       303\n",
            "\n",
            "   micro avg     0.7843    0.6720    0.7238     15027\n",
            "   macro avg     0.5916    0.4240    0.4776     15027\n",
            "weighted avg     0.7732    0.6720    0.7169     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5925    0.5564    0.5739      1019\n",
            "           2     0.2857    0.0980    0.1460       102\n",
            "           3     0.3904    0.4914    0.4351       116\n",
            "           4     0.3667    0.2797    0.3173       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.6000    0.0638    0.1154        47\n",
            "\n",
            "   micro avg     0.5434    0.4722    0.5053      1419\n",
            "   macro avg     0.3725    0.2482    0.2646      1419\n",
            "weighted avg     0.5283    0.4722    0.4884      1419\n",
            "\n",
            "0.25768933960645163 0.7168552563610435 0.5206146946767507 0.48838595055461415\n",
            "Patience counter: 6\n",
            "Epoch: 8, iter 0: loss = 0.025447357445955276\n",
            "Epoch: 8, iter 100: loss = 0.025248011574149132\n",
            "Epoch: 8, iter 200: loss = 0.3927096128463745\n",
            "Epoch: 8, iter 300: loss = 0.09467414021492004\n",
            "Epoch: 8, iter 400: loss = 0.07514779269695282\n",
            "Epoch: 8, iter 500: loss = 0.01369834877550602\n",
            "Epoch: 8, iter 600: loss = 0.014460386708378792\n",
            "Epoch: 8, iter 700: loss = 0.0321243517100811\n",
            "Epoch: 8, iter 800: loss = 0.08538101613521576\n",
            "Epoch: 8, iter 900: loss = 0.15283909440040588\n",
            "Epoch: 8, iter 1000: loss = 0.08962048590183258\n",
            "Epoch: 8, iter 1100: loss = 0.144672229886055\n",
            "Epoch: 8, iter 1200: loss = 0.22608189284801483\n",
            "Epoch: 8, iter 1300: loss = 0.20682445168495178\n",
            "Epoch: 8, iter 1400: loss = 0.14594481885433197\n",
            "Epoch: 8, iter 1500: loss = 0.004792701452970505\n",
            "Epoch: 8, iter 1600: loss = 0.029115881770849228\n",
            "Epoch: 8, iter 1700: loss = 0.04761606827378273\n",
            "Epoch: 8, iter 1800: loss = 0.14272654056549072\n",
            "Epoch: 8, iter 1900: loss = 0.11661677062511444\n",
            "Epoch: 8, iter 2000: loss = 0.5570947527885437\n",
            "Epoch: 8, iter 2100: loss = 0.21732664108276367\n",
            "Epoch: 8, iter 2200: loss = 0.43471845984458923\n",
            "Epoch: 8, iter 2300: loss = 0.2953539788722992\n",
            "Epoch: 8, iter 2400: loss = 0.5852570533752441\n",
            "Epoch: 8, iter 2500: loss = 0.6062889099121094\n",
            "Epoch: 8, iter 2600: loss = 0.23825231194496155\n",
            "Epoch: 8, iter 2700: loss = 0.012003565207123756\n",
            "Epoch: 8, iter 2800: loss = 0.3600698709487915\n",
            "Epoch: 8, iter 2900: loss = 0.09122144430875778\n",
            "Epoch: 8, iter 3000: loss = 0.0037029986269772053\n",
            "Epoch: 8, iter 3100: loss = 0.8805548548698425\n",
            "Epoch: 8, iter 3200: loss = 0.4729439318180084\n",
            "Epoch: 8, iter 3300: loss = 0.0013077203184366226\n",
            "Epoch: 8, iter 3400: loss = 0.39747729897499084\n",
            "Epoch: 8, iter 3500: loss = 0.04250502958893776\n",
            "Epoch: 8, iter 3600: loss = 0.06685526669025421\n",
            "Epoch: 8, iter 3700: loss = 0.0013231702614575624\n",
            "Epoch: 8, iter 3800: loss = 0.04131477326154709\n",
            "Epoch: 8, iter 3900: loss = 0.15012510120868683\n",
            "Epoch: 8, iter 4000: loss = 0.1897657811641693\n",
            "Epoch: 8, iter 4100: loss = 0.17289386689662933\n",
            "Epoch: 8, iter 4200: loss = 0.43850985169410706\n",
            "Epoch: 8, iter 4300: loss = 0.9640023708343506\n",
            "Epoch: 8, iter 4400: loss = 0.4342585504055023\n",
            "Epoch: 8, iter 4500: loss = 0.017305705696344376\n",
            "Epoch: 8, iter 4600: loss = 0.010972254909574986\n",
            "Epoch: 8, iter 4700: loss = 0.0023599436972290277\n",
            "Epoch: 8, iter 4800: loss = 0.06838396936655045\n",
            "Epoch: 8, iter 4900: loss = 1.0100823640823364\n",
            "Epoch: 8, iter 5000: loss = 0.16797101497650146\n",
            "Epoch: 8, iter 5100: loss = 0.17335020005702972\n",
            "Epoch: 8, iter 5200: loss = 0.06261894106864929\n",
            "Epoch: 8, iter 5300: loss = 0.0039369803853333\n",
            "Epoch: 8, iter 5400: loss = 0.46066004037857056\n",
            "Epoch: 8, iter 5500: loss = 0.016879837960004807\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:15<00:00, 12.77it/s]\n",
            "Epoch 8 loss average: 0.242\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8224    0.7585    0.7892     11182\n",
            "           2     0.7025    0.4995    0.5838       969\n",
            "           3     0.7405    0.6975    0.7184      1600\n",
            "           4     0.6630    0.5163    0.5806       827\n",
            "           5     0.4167    0.0342    0.0633       146\n",
            "           6     0.4557    0.2376    0.3124       303\n",
            "\n",
            "   micro avg     0.7946    0.7044    0.7468     15027\n",
            "   macro avg     0.6335    0.4573    0.5079     15027\n",
            "weighted avg     0.7859    0.7044    0.7402     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.5911    0.5633    0.5769      1019\n",
            "           2     0.2903    0.1765    0.2195       102\n",
            "           3     0.5340    0.4741    0.5023       116\n",
            "           4     0.4000    0.1864    0.2543       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.7500    0.2553    0.3810        47\n",
            "\n",
            "   micro avg     0.5642    0.4799    0.5187      1419\n",
            "   macro avg     0.4276    0.2759    0.3223      1419\n",
            "weighted avg     0.5471    0.4799    0.5049      1419\n",
            "\n",
            "0.24219571163225592 0.7402295858470996 0.5321000909443975 0.5048743007851615\n",
            "Patience counter: 7\n",
            "Epoch: 9, iter 0: loss = 0.23663347959518433\n",
            "Epoch: 9, iter 100: loss = 0.007429650519043207\n",
            "Epoch: 9, iter 200: loss = 0.28609880805015564\n",
            "Epoch: 9, iter 300: loss = 0.04949445649981499\n",
            "Epoch: 9, iter 400: loss = 0.7103019952774048\n",
            "Epoch: 9, iter 500: loss = 0.11067960411310196\n",
            "Epoch: 9, iter 600: loss = 0.01241757906973362\n",
            "Epoch: 9, iter 700: loss = 0.3194381594657898\n",
            "Epoch: 9, iter 800: loss = 0.1731371134519577\n",
            "Epoch: 9, iter 900: loss = 0.01247307937592268\n",
            "Epoch: 9, iter 1000: loss = 0.19689331948757172\n",
            "Epoch: 9, iter 1100: loss = 0.8128163814544678\n",
            "Epoch: 9, iter 1200: loss = 0.7044904828071594\n",
            "Epoch: 9, iter 1300: loss = 0.05190775543451309\n",
            "Epoch: 9, iter 1400: loss = 0.5399707555770874\n",
            "Epoch: 9, iter 1500: loss = 0.7541378140449524\n",
            "Epoch: 9, iter 1600: loss = 0.029412494972348213\n",
            "Epoch: 9, iter 1700: loss = 0.011779026128351688\n",
            "Epoch: 9, iter 1800: loss = 0.2312742918729782\n",
            "Epoch: 9, iter 1900: loss = 0.00522625120356679\n",
            "Epoch: 9, iter 2000: loss = 0.3785596787929535\n",
            "Epoch: 9, iter 2100: loss = 0.008572986349463463\n",
            "Epoch: 9, iter 2200: loss = 0.6547959446907043\n",
            "Epoch: 9, iter 2300: loss = 0.30806419253349304\n",
            "Epoch: 9, iter 2400: loss = 0.5238273739814758\n",
            "Epoch: 9, iter 2500: loss = 0.008987569250166416\n",
            "Epoch: 9, iter 2600: loss = 1.4384456872940063\n",
            "Epoch: 9, iter 2700: loss = 0.0020389403216540813\n",
            "Epoch: 9, iter 2800: loss = 0.6591640114784241\n",
            "Epoch: 9, iter 2900: loss = 0.0009083690820261836\n",
            "Epoch: 9, iter 3000: loss = 0.18148967623710632\n",
            "Epoch: 9, iter 3100: loss = 0.9531426429748535\n",
            "Epoch: 9, iter 3200: loss = 0.0032412828877568245\n",
            "Epoch: 9, iter 3300: loss = 0.0009052866953425109\n",
            "Epoch: 9, iter 3400: loss = 0.8090484142303467\n",
            "Epoch: 9, iter 3500: loss = 0.9466522336006165\n",
            "Epoch: 9, iter 3600: loss = 0.10330130159854889\n",
            "Epoch: 9, iter 3700: loss = 0.3280307650566101\n",
            "Epoch: 9, iter 3800: loss = 0.015157514251768589\n",
            "Epoch: 9, iter 3900: loss = 0.075322724878788\n",
            "Epoch: 9, iter 4000: loss = 0.048994094133377075\n",
            "Epoch: 9, iter 4100: loss = 0.002118003321811557\n",
            "Epoch: 9, iter 4200: loss = 1.5877825021743774\n",
            "Epoch: 9, iter 4300: loss = 1.1939409971237183\n",
            "Epoch: 9, iter 4400: loss = 0.040491536259651184\n",
            "Epoch: 9, iter 4500: loss = 0.2483445256948471\n",
            "Epoch: 9, iter 4600: loss = 0.3741215467453003\n",
            "Epoch: 9, iter 4700: loss = 0.4735320806503296\n",
            "Epoch: 9, iter 4800: loss = 0.022617444396018982\n",
            "Epoch: 9, iter 4900: loss = 0.866310179233551\n",
            "Epoch: 9, iter 5000: loss = 0.1592119038105011\n",
            "Epoch: 9, iter 5100: loss = 0.04000017046928406\n",
            "Epoch: 9, iter 5200: loss = 0.0017226412892341614\n",
            "Epoch: 9, iter 5300: loss = 0.13607020676136017\n",
            "Epoch: 9, iter 5400: loss = 0.030518118292093277\n",
            "Epoch: 9, iter 5500: loss = 0.4458199143409729\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:13<00:00, 12.83it/s]\n",
            "Epoch 9 loss average: 0.231\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8327    0.7684    0.7993     11182\n",
            "           2     0.7182    0.5129    0.5984       969\n",
            "           3     0.7503    0.7063    0.7276      1600\n",
            "           4     0.6530    0.5187    0.5782       827\n",
            "           5     0.4167    0.0342    0.0633       146\n",
            "           6     0.4873    0.3168    0.3840       303\n",
            "\n",
            "   micro avg     0.8032    0.7153    0.7567     15027\n",
            "   macro avg     0.6430    0.4762    0.5251     15027\n",
            "weighted avg     0.7957    0.7153    0.7510     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6419    0.4838    0.5518      1019\n",
            "           2     0.3196    0.3039    0.3116       102\n",
            "           3     0.5275    0.4138    0.4638       116\n",
            "           4     0.4225    0.2542    0.3175       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.6667    0.1702    0.2712        47\n",
            "\n",
            "   micro avg     0.5871    0.4299    0.4963      1419\n",
            "   macro avg     0.4297    0.2710    0.3193      1419\n",
            "weighted avg     0.5843    0.4299    0.4919      1419\n",
            "\n",
            "0.23138789096365098 0.7509880050255932 0.535238235227173 0.4919156439841162\n",
            "Patience counter: 8\n",
            "Epoch: 10, iter 0: loss = 0.7224828600883484\n",
            "Epoch: 10, iter 100: loss = 0.2209310382604599\n",
            "Epoch: 10, iter 200: loss = 0.012070367112755775\n",
            "Epoch: 10, iter 300: loss = 0.01842113956809044\n",
            "Epoch: 10, iter 400: loss = 0.0034105228260159492\n",
            "Epoch: 10, iter 500: loss = 0.12750579416751862\n",
            "Epoch: 10, iter 600: loss = 1.857447862625122\n",
            "Epoch: 10, iter 700: loss = 0.047949016094207764\n",
            "Epoch: 10, iter 800: loss = 0.22240720689296722\n",
            "Epoch: 10, iter 900: loss = 0.0008116042008623481\n",
            "Epoch: 10, iter 1000: loss = 0.04947963356971741\n",
            "Epoch: 10, iter 1100: loss = 0.2596948444843292\n",
            "Epoch: 10, iter 1200: loss = 0.1617755889892578\n",
            "Epoch: 10, iter 1300: loss = 0.07743028551340103\n",
            "Epoch: 10, iter 1400: loss = 0.02939947508275509\n",
            "Epoch: 10, iter 1500: loss = 0.011869717389345169\n",
            "Epoch: 10, iter 1600: loss = 0.12613405287265778\n",
            "Epoch: 10, iter 1700: loss = 0.052368562668561935\n",
            "Epoch: 10, iter 1800: loss = 0.28807565569877625\n",
            "Epoch: 10, iter 1900: loss = 0.1835101842880249\n",
            "Epoch: 10, iter 2000: loss = 0.322215735912323\n",
            "Epoch: 10, iter 2100: loss = 0.002231423743069172\n",
            "Epoch: 10, iter 2200: loss = 0.013811293989419937\n",
            "Epoch: 10, iter 2300: loss = 0.07351047545671463\n",
            "Epoch: 10, iter 2400: loss = 0.001991362776607275\n",
            "Epoch: 10, iter 2500: loss = 1.1348187923431396\n",
            "Epoch: 10, iter 2600: loss = 0.0010179554810747504\n",
            "Epoch: 10, iter 2700: loss = 0.014336347579956055\n",
            "Epoch: 10, iter 2800: loss = 0.006409043911844492\n",
            "Epoch: 10, iter 2900: loss = 0.08694350719451904\n",
            "Epoch: 10, iter 3000: loss = 0.029578272253274918\n",
            "Epoch: 10, iter 3100: loss = 0.005462648347020149\n",
            "Epoch: 10, iter 3200: loss = 0.0405585840344429\n",
            "Epoch: 10, iter 3300: loss = 0.093144990503788\n",
            "Epoch: 10, iter 3400: loss = 0.09125502407550812\n",
            "Epoch: 10, iter 3500: loss = 0.1206682026386261\n",
            "Epoch: 10, iter 3600: loss = 0.002691018395125866\n",
            "Epoch: 10, iter 3700: loss = 0.008335202001035213\n",
            "Epoch: 10, iter 3800: loss = 0.14710819721221924\n",
            "Epoch: 10, iter 3900: loss = 0.00044037445331923664\n",
            "Epoch: 10, iter 4000: loss = 0.36491233110427856\n",
            "Epoch: 10, iter 4100: loss = 0.2627560496330261\n",
            "Epoch: 10, iter 4200: loss = 0.49322938919067383\n",
            "Epoch: 10, iter 4300: loss = 0.02883705124258995\n",
            "Epoch: 10, iter 4400: loss = 0.09851469844579697\n",
            "Epoch: 10, iter 4500: loss = 0.0032440824434161186\n",
            "Epoch: 10, iter 4600: loss = 0.010996311902999878\n",
            "Epoch: 10, iter 4700: loss = 0.02179001457989216\n",
            "Epoch: 10, iter 4800: loss = 0.02822611853480339\n",
            "Epoch: 10, iter 4900: loss = 0.2163119614124298\n",
            "Epoch: 10, iter 5000: loss = 0.34010937809944153\n",
            "Epoch: 10, iter 5100: loss = 0.5116069316864014\n",
            "Epoch: 10, iter 5200: loss = 0.002766098128631711\n",
            "Epoch: 10, iter 5300: loss = 0.0049132611602544785\n",
            "Epoch: 10, iter 5400: loss = 0.50461345911026\n",
            "Epoch: 10, iter 5500: loss = 0.006765714846551418\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.85it/s]\n",
            "Epoch 10 loss average: 0.217\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8395    0.7881    0.8130     11182\n",
            "           2     0.7339    0.5408    0.6227       969\n",
            "           3     0.7684    0.7131    0.7397      1600\n",
            "           4     0.6888    0.5780    0.6285       827\n",
            "           5     0.4688    0.1027    0.1685       146\n",
            "           6     0.5000    0.3234    0.3928       303\n",
            "\n",
            "   micro avg     0.8127    0.7365    0.7728     15027\n",
            "   macro avg     0.6665    0.5077    0.5609     15027\n",
            "weighted avg     0.8064    0.7365    0.7680     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6225    0.5260    0.5702      1019\n",
            "           2     0.3433    0.2255    0.2722       102\n",
            "           3     0.4786    0.4828    0.4807       116\n",
            "           4     0.3735    0.2627    0.3085       118\n",
            "           5     0.4000    0.1176    0.1818        17\n",
            "           6     0.7778    0.1489    0.2500        47\n",
            "\n",
            "   micro avg     0.5736    0.4616    0.5115      1419\n",
            "   macro avg     0.4993    0.2939    0.3439      1419\n",
            "weighted avg     0.5725    0.4616    0.5044      1419\n",
            "\n",
            "0.21663760862627002 0.7680030419036596 0.553475845747367 0.5044458758376491\n",
            "Patience counter: 9\n",
            "Epoch: 11, iter 0: loss = 0.006843399256467819\n",
            "Epoch: 11, iter 100: loss = 0.5240699052810669\n",
            "Epoch: 11, iter 200: loss = 0.07799641042947769\n",
            "Epoch: 11, iter 300: loss = 0.25755974650382996\n",
            "Epoch: 11, iter 400: loss = 0.005715629085898399\n",
            "Epoch: 11, iter 500: loss = 0.0062636686488986015\n",
            "Epoch: 11, iter 600: loss = 0.4798315167427063\n",
            "Epoch: 11, iter 700: loss = 0.003483022330328822\n",
            "Epoch: 11, iter 800: loss = 0.066373810172081\n",
            "Epoch: 11, iter 900: loss = 0.0019188736332580447\n",
            "Epoch: 11, iter 1000: loss = 0.05190317705273628\n",
            "Epoch: 11, iter 1100: loss = 0.02058093436062336\n",
            "Epoch: 11, iter 1200: loss = 0.1439189314842224\n",
            "Epoch: 11, iter 1300: loss = 0.1670193076133728\n",
            "Epoch: 11, iter 1400: loss = 0.0021295221522450447\n",
            "Epoch: 11, iter 1500: loss = 0.015180031768977642\n",
            "Epoch: 11, iter 1600: loss = 0.2102397233247757\n",
            "Epoch: 11, iter 1700: loss = 0.012639092281460762\n",
            "Epoch: 11, iter 1800: loss = 0.05174034461379051\n",
            "Epoch: 11, iter 1900: loss = 0.05934901162981987\n",
            "Epoch: 11, iter 2000: loss = 0.05604865029454231\n",
            "Epoch: 11, iter 2100: loss = 0.04458342492580414\n",
            "Epoch: 11, iter 2200: loss = 0.028705835342407227\n",
            "Epoch: 11, iter 2300: loss = 0.002981120953336358\n",
            "Epoch: 11, iter 2400: loss = 0.031224632635712624\n",
            "Epoch: 11, iter 2500: loss = 0.053071241825819016\n",
            "Epoch: 11, iter 2600: loss = 0.0007094590691849589\n",
            "Epoch: 11, iter 2700: loss = 0.11917936056852341\n",
            "Epoch: 11, iter 2800: loss = 0.0005057979724369943\n",
            "Epoch: 11, iter 2900: loss = 0.003813213435932994\n",
            "Epoch: 11, iter 3000: loss = 0.36233973503112793\n",
            "Epoch: 11, iter 3100: loss = 0.003396477783098817\n",
            "Epoch: 11, iter 3200: loss = 1.0162688493728638\n",
            "Epoch: 11, iter 3300: loss = 0.4966675043106079\n",
            "Epoch: 11, iter 3400: loss = 0.5275471806526184\n",
            "Epoch: 11, iter 3500: loss = 0.03467120975255966\n",
            "Epoch: 11, iter 3600: loss = 0.0012222633231431246\n",
            "Epoch: 11, iter 3700: loss = 0.14140309393405914\n",
            "Epoch: 11, iter 3800: loss = 0.0614447221159935\n",
            "Epoch: 11, iter 3900: loss = 0.27993106842041016\n",
            "Epoch: 11, iter 4000: loss = 0.23849685490131378\n",
            "Epoch: 11, iter 4100: loss = 0.04995540902018547\n",
            "Epoch: 11, iter 4200: loss = 0.2806416153907776\n",
            "Epoch: 11, iter 4300: loss = 0.33819663524627686\n",
            "Epoch: 11, iter 4400: loss = 0.09216932207345963\n",
            "Epoch: 11, iter 4500: loss = 0.006156620103865862\n",
            "Epoch: 11, iter 4600: loss = 0.4145491123199463\n",
            "Epoch: 11, iter 4700: loss = 0.7808529138565063\n",
            "Epoch: 11, iter 4800: loss = 0.16820649802684784\n",
            "Epoch: 11, iter 4900: loss = 0.051798414438962936\n",
            "Epoch: 11, iter 5000: loss = 0.048400525003671646\n",
            "Epoch: 11, iter 5100: loss = 0.9961291551589966\n",
            "Epoch: 11, iter 5200: loss = 0.09252502769231796\n",
            "Epoch: 11, iter 5300: loss = 0.06855256110429764\n",
            "Epoch: 11, iter 5400: loss = 0.0755169689655304\n",
            "Epoch: 11, iter 5500: loss = 0.001798416255041957\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.86it/s]\n",
            "Epoch 11 loss average: 0.210\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8496    0.8008    0.8245     11182\n",
            "           2     0.7666    0.5831    0.6624       969\n",
            "           3     0.7683    0.7069    0.7363      1600\n",
            "           4     0.7047    0.5599    0.6240       827\n",
            "           5     0.5000    0.1096    0.1798       146\n",
            "           6     0.5636    0.4092    0.4742       303\n",
            "\n",
            "   micro avg     0.8240    0.7489    0.7846     15027\n",
            "   macro avg     0.6922    0.5282    0.5835     15027\n",
            "weighted avg     0.8185    0.7489    0.7803     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6474    0.4504    0.5312      1019\n",
            "           2     0.2708    0.1275    0.1733       102\n",
            "           3     0.4400    0.3793    0.4074       116\n",
            "           4     0.4459    0.2797    0.3437       118\n",
            "           5     0.0000    0.0000    0.0000        17\n",
            "           6     0.4000    0.1277    0.1935        47\n",
            "\n",
            "   micro avg     0.5867    0.3911    0.4693      1419\n",
            "   macro avg     0.3674    0.2274    0.2749      1419\n",
            "weighted avg     0.5707    0.3911    0.4623      1419\n",
            "\n",
            "0.20985441708997735 0.7802613446475224 0.4932284982712836 0.46225671843044935\n",
            "Patience counter: 10\n",
            "Epoch: 12, iter 0: loss = 0.0021958667784929276\n",
            "Epoch: 12, iter 100: loss = 0.812244713306427\n",
            "Epoch: 12, iter 200: loss = 0.49358105659484863\n",
            "Epoch: 12, iter 300: loss = 0.4732029139995575\n",
            "Epoch: 12, iter 400: loss = 1.0091228485107422\n",
            "Epoch: 12, iter 500: loss = 0.7561226487159729\n",
            "Epoch: 12, iter 600: loss = 0.05776190012693405\n",
            "Epoch: 12, iter 700: loss = 0.004841564688831568\n",
            "Epoch: 12, iter 800: loss = 0.46364614367485046\n",
            "Epoch: 12, iter 900: loss = 0.3244185447692871\n",
            "Epoch: 12, iter 1000: loss = 0.0009410043712705374\n",
            "Epoch: 12, iter 1100: loss = 0.32282447814941406\n",
            "Epoch: 12, iter 1200: loss = 0.2433457374572754\n",
            "Epoch: 12, iter 1300: loss = 0.7140809297561646\n",
            "Epoch: 12, iter 1400: loss = 0.013772116042673588\n",
            "Epoch: 12, iter 1500: loss = 0.40195339918136597\n",
            "Epoch: 12, iter 1600: loss = 0.38482752442359924\n",
            "Epoch: 12, iter 1700: loss = 0.6369930505752563\n",
            "Epoch: 12, iter 1800: loss = 0.006375334691256285\n",
            "Epoch: 12, iter 1900: loss = 0.11315096169710159\n",
            "Epoch: 12, iter 2000: loss = 0.30195552110671997\n",
            "Epoch: 12, iter 2100: loss = 0.05076245218515396\n",
            "Epoch: 12, iter 2200: loss = 0.0010182103142142296\n",
            "Epoch: 12, iter 2300: loss = 0.2000485211610794\n",
            "Epoch: 12, iter 2400: loss = 0.3022942841053009\n",
            "Epoch: 12, iter 2500: loss = 0.020767150446772575\n",
            "Epoch: 12, iter 2600: loss = 0.0006008236086927354\n",
            "Epoch: 12, iter 2700: loss = 0.07611005753278732\n",
            "Epoch: 12, iter 2800: loss = 0.015357901342213154\n",
            "Epoch: 12, iter 2900: loss = 0.3871312439441681\n",
            "Epoch: 12, iter 3000: loss = 0.007020400371402502\n",
            "Epoch: 12, iter 3100: loss = 0.007717397063970566\n",
            "Epoch: 12, iter 3200: loss = 0.0665520653128624\n",
            "Epoch: 12, iter 3300: loss = 0.11369770765304565\n",
            "Epoch: 12, iter 3400: loss = 0.13604377210140228\n",
            "Epoch: 12, iter 3500: loss = 0.09907914698123932\n",
            "Epoch: 12, iter 3600: loss = 0.002285397844389081\n",
            "Epoch: 12, iter 3700: loss = 0.9446662068367004\n",
            "Epoch: 12, iter 3800: loss = 0.19768083095550537\n",
            "Epoch: 12, iter 3900: loss = 0.009011226706206799\n",
            "Epoch: 12, iter 4000: loss = 0.8579486608505249\n",
            "Epoch: 12, iter 4100: loss = 0.16637884080410004\n",
            "Epoch: 12, iter 4200: loss = 0.0006536621949635446\n",
            "Epoch: 12, iter 4300: loss = 0.0023783580400049686\n",
            "Epoch: 12, iter 4400: loss = 0.0014825384132564068\n",
            "Epoch: 12, iter 4500: loss = 0.05547787994146347\n",
            "Epoch: 12, iter 4600: loss = 0.066173255443573\n",
            "Epoch: 12, iter 4700: loss = 0.47189009189605713\n",
            "Epoch: 12, iter 4800: loss = 0.431723952293396\n",
            "Epoch: 12, iter 4900: loss = 0.38179081678390503\n",
            "Epoch: 12, iter 5000: loss = 0.011685704812407494\n",
            "Epoch: 12, iter 5100: loss = 0.5258613228797913\n",
            "Epoch: 12, iter 5200: loss = 0.016622165217995644\n",
            "Epoch: 12, iter 5300: loss = 1.3895256519317627\n",
            "Epoch: 12, iter 5400: loss = 0.03383613005280495\n",
            "Epoch: 12, iter 5500: loss = 0.07720344513654709\n",
            "100%|███████████████████████████████████████| 5559/5559 [07:12<00:00, 12.87it/s]\n",
            "Epoch 12 loss average: 0.199\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.8553    0.8132    0.8337     11182\n",
            "           2     0.7542    0.5986    0.6674       969\n",
            "           3     0.7865    0.7275    0.7558      1600\n",
            "           4     0.7352    0.5840    0.6509       827\n",
            "           5     0.5833    0.1438    0.2308       146\n",
            "           6     0.5536    0.4257    0.4813       303\n",
            "\n",
            "   micro avg     0.8308    0.7633    0.7956     15027\n",
            "   macro avg     0.7114    0.5488    0.6033     15027\n",
            "weighted avg     0.8261    0.7633    0.7917     15027\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           1     0.6718    0.4720    0.5545      1019\n",
            "           2     0.2899    0.1961    0.2339       102\n",
            "           3     0.4630    0.4310    0.4464       116\n",
            "           4     0.2993    0.3475    0.3216       118\n",
            "           5     0.5000    0.0588    0.1053        17\n",
            "           6     0.5000    0.0851    0.1455        47\n",
            "\n",
            "   micro avg     0.5740    0.4207    0.4856      1419\n",
            "   macro avg     0.4540    0.2651    0.3012      1419\n",
            "weighted avg     0.5885    0.4207    0.4843      1419\n",
            "\n",
            "0.1987123036631555 0.7916852487446133 0.5182722221907372 0.4842974121740236\n",
            "Patience counter: 11\n",
            "Done! It took 5.6e+03 secs\n",
            "\n",
            "Current RUN: 5\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.5111739759389311\n",
            "Best test f1 weighted\n",
            "0.3719658272271567\n",
            "Best epoch\n",
            "1\n",
            "\n",
            "\n",
            "Average across runs:\n",
            "Best epoch\n",
            "[1, 3, 1, 2, 1]\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.4934322524454444\n",
            "Overall test f1 weighted\n",
            "[0.46253776 0.47192822 0.39508275 0.42585075 0.37196583]\n",
            "Best test f1 weighted\n",
            "0.4254730616134094\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERC_IEMOCAP_UBUNTU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakinavohracs/EmotionDetection/blob/master/ERC_IEMOCAP_UBUNTU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAhzlpZSUe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "1227a297-f28d-41cb-917c-73e8c77e0d9a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/Sakina Model/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf1dFoIeVoJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb35fd81-b57d-4302-96e4-f2dac492e17a"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/Sakina\\ Model/TL-ERC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NINDhn63WDwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATASET_DIRECTORY_PATH = \"./datasets/\"\n",
        "GENERATIVE_WEIGHTS_DIRECTORY_PATH = \"./generative_weights/\"\n",
        "\n",
        "if not os.path.exists(DATASET_DIRECTORY_PATH):\n",
        "    os.makedirs(DATASET_DIRECTORY_PATH)\n",
        "\n",
        "\n",
        "if not os.path.exists(GENERATIVE_WEIGHTS_DIRECTORY_PATH):\n",
        "    os.makedirs(GENERATIVE_WEIGHTS_DIRECTORY_PATH)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_ZDpZEX4nC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "222b64a8-3723-4794-fc48-f9bcede52021"
      },
      "source": [
        "%cd bert_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/bert_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2R2ZXYTYAPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "040558d1-f787-41cd-80d6-3fde281ef2a6"
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg9iv36QYG66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "c628df26-6502-43d0-bdd2-e13fea64bfcf"
      },
      "source": [
        "pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 18.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.37)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.37)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9ZTFX6LYWyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9d59aede-50ef-4521-fca9-bba25438f621"
      },
      "source": [
        "!python train.py --load_checkpoint=../generative_weights/ubuntu_weights.pkl --data=iemocap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 4.4e+01 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.7459849119186401\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 1 loss average: 1.786\n",
            "train\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1190    0.0982    0.1076       764\n",
            "           2     0.1553    0.0296    0.0498      1080\n",
            "           3     0.1836    0.0748    0.1063       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2610    0.7669    0.3895      1210\n",
            "\n",
            "    accuracy                         0.2322      4699\n",
            "   macro avg     0.1198    0.1616    0.1089      4699\n",
            "weighted avg     0.1515    0.2322    0.1462      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3091    0.0443    0.0774       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2430    1.0000    0.3910       381\n",
            "\n",
            "    accuracy                         0.2452      1623\n",
            "   macro avg     0.0920    0.1740    0.0781      1623\n",
            "weighted avg     0.1302    0.2452    0.1101      1623\n",
            "\n",
            "1.7859422465165455 0.1461695915841716 0.10513067935339203 0.11010461201455822\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.9491512775421143\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 2 loss average: 1.690\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.2910    0.0510    0.0869       764\n",
            "           2     0.3123    0.3028    0.3075      1080\n",
            "           3     0.1190    0.0267    0.0436       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.3147    0.8711    0.4624      1210\n",
            "\n",
            "    accuracy                         0.3064      4699\n",
            "   macro avg     0.1729    0.2086    0.1501      4699\n",
            "weighted avg     0.2191    0.3064    0.2108      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.2086    0.8082    0.3317       245\n",
            "           2     0.3699    0.5443    0.4405       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.6972    0.1995    0.3102       381\n",
            "\n",
            "    accuracy                         0.2976      1623\n",
            "   macro avg     0.2126    0.2587    0.1804      1623\n",
            "weighted avg     0.2827    0.2976    0.2271      1623\n",
            "\n",
            "1.6897210975488026 0.21080830875977266 0.09611918110380013 0.2270992461806689\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.55022394657135\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 3 loss average: 1.393\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2973    0.0585    0.0978       376\n",
            "           1     0.5590    0.4215    0.4806       764\n",
            "           2     0.3562    0.4083    0.3805      1080\n",
            "           3     0.7254    0.2363    0.3565       749\n",
            "           4     0.3845    0.3712    0.3777       520\n",
            "           5     0.4446    0.7587    0.5606      1210\n",
            "\n",
            "    accuracy                         0.4412      4699\n",
            "   macro avg     0.4612    0.3757    0.3756      4699\n",
            "weighted avg     0.4692    0.4412    0.4164      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3286    0.4861    0.3922       144\n",
            "           1     0.8188    0.4980    0.6193       245\n",
            "           2     0.4493    0.5312    0.4869       384\n",
            "           3     0.6341    0.4588    0.5324       170\n",
            "           4     0.7019    0.2441    0.3623       299\n",
            "           5     0.4966    0.7559    0.5994       381\n",
            "\n",
            "    accuracy                         0.5145      1623\n",
            "   macro avg     0.5716    0.4957    0.4987      1623\n",
            "weighted avg     0.5714    0.5145    0.5067      1623\n",
            "\n",
            "1.3930944601694744 0.4163937612311878 0.4036041856381392 0.5066867237179413\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.3223174810409546\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 4 loss average: 1.112\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5212    0.3271    0.4020       376\n",
            "           1     0.7287    0.7487    0.7385       764\n",
            "           2     0.4768    0.4463    0.4610      1080\n",
            "           3     0.7418    0.3605    0.4852       749\n",
            "           4     0.5268    0.6058    0.5635       520\n",
            "           5     0.5308    0.7479    0.6209      1210\n",
            "\n",
            "    accuracy                         0.5676      4699\n",
            "   macro avg     0.5877    0.5394    0.5452      4699\n",
            "weighted avg     0.5830    0.5676    0.5578      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3739    0.2986    0.3320       144\n",
            "           1     0.3645    0.8898    0.5172       245\n",
            "           2     0.5097    0.4115    0.4553       384\n",
            "           3     0.7115    0.4353    0.5401       170\n",
            "           4     0.8167    0.1639    0.2730       299\n",
            "           5     0.5596    0.6404    0.5973       381\n",
            "\n",
            "    accuracy                         0.4843      1623\n",
            "   macro avg     0.5560    0.4732    0.4525      1623\n",
            "weighted avg     0.5652    0.4843    0.4624      1623\n",
            "\n",
            "1.112463615834713 0.557784234916758 0.35845189260588256 0.46235125641174657\n",
            "Patience counter: 1\n",
            "Epoch: 5, iter 0: loss = 1.1266826391220093\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 5 loss average: 0.961\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5375    0.4761    0.5049       376\n",
            "           1     0.7254    0.8194    0.7695       764\n",
            "           2     0.5391    0.5037    0.5208      1080\n",
            "           3     0.7315    0.4873    0.5849       749\n",
            "           4     0.6130    0.7250    0.6643       520\n",
            "           5     0.5986    0.6826    0.6378      1210\n",
            "\n",
            "    accuracy                         0.6208      4699\n",
            "   macro avg     0.6242    0.6157    0.6137      4699\n",
            "weighted avg     0.6234    0.6208    0.6162      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5915    0.2917    0.3907       144\n",
            "           1     0.7778    0.5429    0.6394       245\n",
            "           2     0.5216    0.5026    0.5119       384\n",
            "           3     0.6725    0.6765    0.6745       170\n",
            "           4     0.7070    0.6054    0.6523       299\n",
            "           5     0.5086    0.7795    0.6155       381\n",
            "\n",
            "    accuracy                         0.5921      1623\n",
            "   macro avg     0.6298    0.5664    0.5807      1623\n",
            "weighted avg     0.6134    0.5921    0.5876      1623\n",
            "\n",
            "0.9607884430636963 0.6162162384796911 0.458755638643213 0.587622386416205\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.756390392780304\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 6 loss average: 0.818\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5683    0.4761    0.5181       376\n",
            "           1     0.8225    0.8613    0.8414       764\n",
            "           2     0.6341    0.6241    0.6290      1080\n",
            "           3     0.7161    0.6128    0.6604       749\n",
            "           4     0.6235    0.6942    0.6570       520\n",
            "           5     0.6610    0.7107    0.6850      1210\n",
            "\n",
            "    accuracy                         0.6791      4699\n",
            "   macro avg     0.6709    0.6632    0.6652      4699\n",
            "weighted avg     0.6783    0.6791    0.6772      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6774    0.1458    0.2400       144\n",
            "           1     0.7269    0.6735    0.6992       245\n",
            "           2     0.6690    0.5052    0.5757       384\n",
            "           3     0.7299    0.5882    0.6515       170\n",
            "           4     0.5889    0.8528    0.6967       299\n",
            "           5     0.5545    0.7349    0.6321       381\n",
            "\n",
            "    accuracy                         0.6254      1623\n",
            "   macro avg     0.6578    0.5834    0.5825      1623\n",
            "weighted avg     0.6432    0.6254    0.6080      1623\n",
            "\n",
            "0.8180593401193619 0.6771906287667349 0.4402117982851018 0.6080038518704473\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 0.6731771230697632\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 7 loss average: 0.717\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5844    0.6170    0.6003       376\n",
            "           1     0.8612    0.8691    0.8651       764\n",
            "           2     0.7048    0.7074    0.7061      1080\n",
            "           3     0.7572    0.6702    0.7110       749\n",
            "           4     0.7143    0.6635    0.6879       520\n",
            "           5     0.7110    0.7645    0.7368      1210\n",
            "\n",
            "    accuracy                         0.7304      4699\n",
            "   macro avg     0.7221    0.7153    0.7179      4699\n",
            "weighted avg     0.7316    0.7304    0.7302      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5833    0.3403    0.4298       144\n",
            "           1     0.6960    0.7102    0.7030       245\n",
            "           2     0.5498    0.5755    0.5623       384\n",
            "           3     0.6203    0.6824    0.6499       170\n",
            "           4     0.7070    0.6455    0.6748       299\n",
            "           5     0.5738    0.6430    0.6064       381\n",
            "\n",
            "    accuracy                         0.6149      1623\n",
            "   macro avg     0.6217    0.5995    0.6044      1623\n",
            "weighted avg     0.6168    0.6149    0.6121      1623\n",
            "\n",
            "0.7165825876096884 0.7301631113044112 0.48659900872553224 0.6120622244788521\n",
            "Patience counter: 0\n",
            "Epoch: 8, iter 0: loss = 0.8629183769226074\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 8 loss average: 0.607\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6803    0.7074    0.6936       376\n",
            "           1     0.8541    0.8887    0.8711       764\n",
            "           2     0.7521    0.7306    0.7412      1080\n",
            "           3     0.7906    0.7610    0.7755       749\n",
            "           4     0.7249    0.7500    0.7372       520\n",
            "           5     0.7734    0.7702    0.7718      1210\n",
            "\n",
            "    accuracy                         0.7717      4699\n",
            "   macro avg     0.7626    0.7680    0.7651      4699\n",
            "weighted avg     0.7716    0.7717    0.7714      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4224    0.3403    0.3769       144\n",
            "           1     0.6354    0.7184    0.6743       245\n",
            "           2     0.4640    0.6719    0.5489       384\n",
            "           3     0.6831    0.5706    0.6218       170\n",
            "           4     0.7836    0.3512    0.4850       299\n",
            "           5     0.5879    0.6142    0.6008       381\n",
            "\n",
            "    accuracy                         0.5662      1623\n",
            "   macro avg     0.5961    0.5444    0.5513      1623\n",
            "weighted avg     0.5971    0.5662    0.5606      1623\n",
            "\n",
            "0.6071307474436859 0.7714271954480028 0.48879580382940396 0.5606218540371354\n",
            "Patience counter: 1\n",
            "Epoch: 9, iter 0: loss = 0.48510661721229553\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 9 loss average: 0.512\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7759    0.7181    0.7459       376\n",
            "           1     0.8997    0.9045    0.9021       764\n",
            "           2     0.7981    0.7870    0.7925      1080\n",
            "           3     0.8266    0.7891    0.8074       749\n",
            "           4     0.7889    0.8481    0.8174       520\n",
            "           5     0.7982    0.8207    0.8093      1210\n",
            "\n",
            "    accuracy                         0.8163      4699\n",
            "   macro avg     0.8146    0.8112    0.8124      4699\n",
            "weighted avg     0.8164    0.8163    0.8160      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4744    0.2569    0.3333       144\n",
            "           1     0.5944    0.7837    0.6761       245\n",
            "           2     0.4226    0.6901    0.5242       384\n",
            "           3     0.5160    0.6647    0.5810       170\n",
            "           4     0.8986    0.2074    0.3370       299\n",
            "           5     0.6254    0.5039    0.5581       381\n",
            "\n",
            "    accuracy                         0.5305      1623\n",
            "   macro avg     0.5886    0.5178    0.5016      1623\n",
            "weighted avg     0.5982    0.5305    0.5096      1623\n",
            "\n",
            "0.512387183184425 0.8160480042430928 0.4457599218753676 0.5096159398816028\n",
            "Patience counter: 2\n",
            "Epoch: 10, iter 0: loss = 0.5007840394973755\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 10 loss average: 0.451\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8298    0.7261    0.7745       376\n",
            "           1     0.9135    0.9267    0.9201       764\n",
            "           2     0.8198    0.8296    0.8247      1080\n",
            "           3     0.8299    0.8531    0.8413       749\n",
            "           4     0.7885    0.8673    0.8260       520\n",
            "           5     0.8500    0.8149    0.8321      1210\n",
            "\n",
            "    accuracy                         0.8412      4699\n",
            "   macro avg     0.8386    0.8363    0.8364      4699\n",
            "weighted avg     0.8417    0.8412    0.8409      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4602    0.3611    0.4047       144\n",
            "           1     0.6860    0.6776    0.6817       245\n",
            "           2     0.4553    0.6771    0.5445       384\n",
            "           3     0.6471    0.5824    0.6130       170\n",
            "           4     0.7903    0.3278    0.4634       299\n",
            "           5     0.5762    0.6352    0.6042       381\n",
            "\n",
            "    accuracy                         0.5650      1623\n",
            "   macro avg     0.6025    0.5435    0.5519      1623\n",
            "weighted avg     0.6007    0.5650    0.5591      1623\n",
            "\n",
            "0.45059853062654537 0.8408747894692423 0.4820982586273117 0.5590606630007684\n",
            "Patience counter: 3\n",
            "Epoch: 11, iter 0: loss = 0.4315654933452606\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 11 loss average: 0.384\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8352    0.7819    0.8077       376\n",
            "           1     0.9201    0.9346    0.9273       764\n",
            "           2     0.8695    0.8639    0.8667      1080\n",
            "           3     0.8855    0.8465    0.8655       749\n",
            "           4     0.8306    0.8769    0.8531       520\n",
            "           5     0.8605    0.8769    0.8686      1210\n",
            "\n",
            "    accuracy                         0.8708      4699\n",
            "   macro avg     0.8669    0.8634    0.8648      4699\n",
            "weighted avg     0.8709    0.8708    0.8706      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6744    0.2014    0.3102       144\n",
            "           1     0.7708    0.6041    0.6773       245\n",
            "           2     0.4696    0.5026    0.4855       384\n",
            "           3     0.4718    0.6882    0.5598       170\n",
            "           4     0.7143    0.5686    0.6331       299\n",
            "           5     0.4766    0.6142    0.5367       381\n",
            "\n",
            "    accuracy                         0.5490      1623\n",
            "   macro avg     0.5962    0.5298    0.5338      1623\n",
            "weighted avg     0.5802    0.5490    0.5459      1623\n",
            "\n",
            "0.3838893075784047 0.8706286346092538 0.4632199866859994 0.5459138286070954\n",
            "Patience counter: 4\n",
            "Epoch: 12, iter 0: loss = 0.10302920639514923\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 12 loss average: 0.342\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8369    0.8324    0.8347       376\n",
            "           1     0.9252    0.9385    0.9318       764\n",
            "           2     0.8648    0.8769    0.8708      1080\n",
            "           3     0.8648    0.8798    0.8723       749\n",
            "           4     0.8800    0.8885    0.8842       520\n",
            "           5     0.8844    0.8537    0.8688      1210\n",
            "\n",
            "    accuracy                         0.8791      4699\n",
            "   macro avg     0.8760    0.8783    0.8771      4699\n",
            "weighted avg     0.8791    0.8791    0.8790      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6154    0.2222    0.3265       144\n",
            "           1     0.7104    0.6408    0.6738       245\n",
            "           2     0.4857    0.5755    0.5268       384\n",
            "           3     0.5260    0.5941    0.5580       170\n",
            "           4     0.6780    0.5987    0.6359       299\n",
            "           5     0.5467    0.6299    0.5854       381\n",
            "\n",
            "    accuracy                         0.5730      1623\n",
            "   macro avg     0.5937    0.5435    0.5511      1623\n",
            "weighted avg     0.5851    0.5730    0.5683      1623\n",
            "\n",
            "0.34184569601590437 0.8790260348767184 0.4586172037500613 0.568341573550002\n",
            "Patience counter: 5\n",
            "Epoch: 13, iter 0: loss = 0.40930625796318054\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 13 loss average: 0.305\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8289    0.8245    0.8267       376\n",
            "           1     0.9489    0.9476    0.9483       764\n",
            "           2     0.8915    0.8824    0.8869      1080\n",
            "           3     0.8887    0.8852    0.8870       749\n",
            "           4     0.8596    0.8827    0.8710       520\n",
            "           5     0.8912    0.8934    0.8923      1210\n",
            "\n",
            "    accuracy                         0.8917      4699\n",
            "   macro avg     0.8848    0.8860    0.8853      4699\n",
            "weighted avg     0.8918    0.8917    0.8917      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4471    0.2639    0.3319       144\n",
            "           1     0.6667    0.5796    0.6201       245\n",
            "           2     0.4606    0.7005    0.5558       384\n",
            "           3     0.5347    0.6353    0.5806       170\n",
            "           4     0.7037    0.4448    0.5451       299\n",
            "           5     0.5571    0.5118    0.5335       381\n",
            "\n",
            "    accuracy                         0.5453      1623\n",
            "   macro avg     0.5616    0.5227    0.5278      1623\n",
            "weighted avg     0.5657    0.5453    0.5410      1623\n",
            "\n",
            "0.3051514105560879 0.891694853996268 0.4809950177916715 0.5410301647914014\n",
            "Patience counter: 6\n",
            "Epoch: 14, iter 0: loss = 0.31161925196647644\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 14 loss average: 0.270\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8280    0.8191    0.8235       376\n",
            "           1     0.9418    0.9529    0.9473       764\n",
            "           2     0.9027    0.9102    0.9064      1080\n",
            "           3     0.9155    0.8972    0.9063       749\n",
            "           4     0.8867    0.8731    0.8798       520\n",
            "           5     0.9032    0.9099    0.9065      1210\n",
            "\n",
            "    accuracy                         0.9036      4699\n",
            "   macro avg     0.8963    0.8937    0.8950      4699\n",
            "weighted avg     0.9035    0.9036    0.9035      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4805    0.2569    0.3348       144\n",
            "           1     0.7231    0.5755    0.6409       245\n",
            "           2     0.4766    0.5573    0.5138       384\n",
            "           3     0.5022    0.6824    0.5786       170\n",
            "           4     0.5865    0.6120    0.5990       299\n",
            "           5     0.5599    0.5276    0.5432       381\n",
            "\n",
            "    accuracy                         0.5496      1623\n",
            "   macro avg     0.5548    0.5353    0.5351      1623\n",
            "weighted avg     0.5566    0.5496    0.5465      1623\n",
            "\n",
            "0.27014251953611773 0.9034991119943903 0.47554378892860255 0.5465048772265498\n",
            "Patience counter: 7\n",
            "Epoch: 15, iter 0: loss = 0.1902431845664978\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 15 loss average: 0.230\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8743    0.8511    0.8625       376\n",
            "           1     0.9622    0.9673    0.9648       764\n",
            "           2     0.9191    0.9157    0.9174      1080\n",
            "           3     0.9241    0.9266    0.9253       749\n",
            "           4     0.9051    0.9173    0.9112       520\n",
            "           5     0.9216    0.9223    0.9219      1210\n",
            "\n",
            "    accuracy                         0.9225      4699\n",
            "   macro avg     0.9177    0.9167    0.9172      4699\n",
            "weighted avg     0.9224    0.9225    0.9225      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5244    0.2986    0.3805       144\n",
            "           1     0.6682    0.6000    0.6323       245\n",
            "           2     0.4512    0.5781    0.5068       384\n",
            "           3     0.5419    0.5706    0.5559       170\n",
            "           4     0.7382    0.4716    0.5755       299\n",
            "           5     0.5207    0.6273    0.5690       381\n",
            "\n",
            "    accuracy                         0.5478      1623\n",
            "   macro avg     0.5741    0.5244    0.5367      1623\n",
            "weighted avg     0.5691    0.5478    0.5470      1623\n",
            "\n",
            "0.23035103365934143 0.9224606580824721 0.48439119056461594 0.5469581541482061\n",
            "Patience counter: 8\n",
            "Epoch: 16, iter 0: loss = 0.23150669038295746\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 16 loss average: 0.206\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9141    0.8777    0.8955       376\n",
            "           1     0.9562    0.9712    0.9636       764\n",
            "           2     0.9202    0.9185    0.9194      1080\n",
            "           3     0.9208    0.9319    0.9263       749\n",
            "           4     0.9249    0.9231    0.9240       520\n",
            "           5     0.9312    0.9289    0.9301      1210\n",
            "\n",
            "    accuracy                         0.9291      4699\n",
            "   macro avg     0.9279    0.9252    0.9265      4699\n",
            "weighted avg     0.9290    0.9291    0.9290      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4149    0.2708    0.3277       144\n",
            "           1     0.6698    0.5878    0.6261       245\n",
            "           2     0.4482    0.6641    0.5352       384\n",
            "           3     0.5632    0.6294    0.5944       170\n",
            "           4     0.6667    0.4749    0.5547       299\n",
            "           5     0.5760    0.5171    0.5450       381\n",
            "\n",
            "    accuracy                         0.5447      1623\n",
            "   macro avg     0.5564    0.5240    0.5305      1623\n",
            "weighted avg     0.5610    0.5447    0.5426      1623\n",
            "\n",
            "0.206176236194248 0.929036502090701 0.4968564537393158 0.5425857558558155\n",
            "Patience counter: 9\n",
            "Epoch: 17, iter 0: loss = 0.3561519980430603\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 17 loss average: 0.175\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9138    0.9309    0.9223       376\n",
            "           1     0.9723    0.9647    0.9685       764\n",
            "           2     0.9265    0.9343    0.9304      1080\n",
            "           3     0.9291    0.9279    0.9285       749\n",
            "           4     0.9517    0.9481    0.9499       520\n",
            "           5     0.9335    0.9281    0.9308      1210\n",
            "\n",
            "    accuracy                         0.9379      4699\n",
            "   macro avg     0.9378    0.9390    0.9384      4699\n",
            "weighted avg     0.9380    0.9379    0.9379      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6136    0.1875    0.2872       144\n",
            "           1     0.7882    0.5469    0.6458       245\n",
            "           2     0.4682    0.5938    0.5235       384\n",
            "           3     0.4483    0.6118    0.5174       170\n",
            "           4     0.6269    0.5452    0.5832       299\n",
            "           5     0.5047    0.5696    0.5351       381\n",
            "\n",
            "    accuracy                         0.5379      1623\n",
            "   macro avg     0.5750    0.5091    0.5154      1623\n",
            "weighted avg     0.5651    0.5379    0.5341      1623\n",
            "\n",
            "0.17510179554422697 0.9378937369856438 0.4679939175244035 0.5340959796705957\n",
            "Patience counter: 10\n",
            "Epoch: 18, iter 0: loss = 0.08052905648946762\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 18 loss average: 0.168\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9256    0.8936    0.9093       376\n",
            "           1     0.9699    0.9712    0.9706       764\n",
            "           2     0.9290    0.9324    0.9307      1080\n",
            "           3     0.9357    0.9319    0.9338       749\n",
            "           4     0.9274    0.9577    0.9423       520\n",
            "           5     0.9410    0.9364    0.9387      1210\n",
            "\n",
            "    accuracy                         0.9393      4699\n",
            "   macro avg     0.9381    0.9372    0.9376      4699\n",
            "weighted avg     0.9394    0.9393    0.9393      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4000    0.3056    0.3465       144\n",
            "           1     0.6500    0.6367    0.6433       245\n",
            "           2     0.5274    0.5260    0.5267       384\n",
            "           3     0.5070    0.6412    0.5662       170\n",
            "           4     0.5947    0.5987    0.5967       299\n",
            "           5     0.5428    0.5328    0.5377       381\n",
            "\n",
            "    accuracy                         0.5502      1623\n",
            "   macro avg     0.5370    0.5402    0.5362      1623\n",
            "weighted avg     0.5485    0.5502    0.5479      1623\n",
            "\n",
            "0.1682321085827425 0.939300131698488 0.4861711622332815 0.5479399678099006\n",
            "Patience counter: 11\n",
            "Done! It took 7.5e+02 secs\n",
            "\n",
            "Current RUN: 1\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.9884893335402012\n",
            "Best test f1 weighted\n",
            "0.6120622244788521\n",
            "Best epoch\n",
            "7\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/ubuntu_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.1 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.7982709407806396\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 1 loss average: 1.783\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2500    0.0160    0.0300       376\n",
            "           1     0.0455    0.0013    0.0025       764\n",
            "           2     0.2706    0.3806    0.3163      1080\n",
            "           3     0.2143    0.0040    0.0079       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2876    0.7388    0.4140      1210\n",
            "\n",
            "    accuracy                         0.2798      4699\n",
            "   macro avg     0.1780    0.1901    0.1284      4699\n",
            "weighted avg     0.1978    0.2798    0.1834      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3548    0.0286    0.0530       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2387    0.9974    0.3852       381\n",
            "\n",
            "    accuracy                         0.2409      1623\n",
            "   macro avg     0.0989    0.1710    0.0730      1623\n",
            "weighted avg     0.1400    0.2409    0.1030      1623\n",
            "\n",
            "1.782810407380263 0.18336092105575272 0.0985138476672946 0.1029685174052998\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.6481343507766724\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 2 loss average: 1.713\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.2941    0.0065    0.0128       764\n",
            "           2     0.3119    0.1750    0.2242      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.5714    0.0231    0.0444       520\n",
            "           5     0.2715    0.9099    0.4182      1210\n",
            "\n",
            "    accuracy                         0.2781      4699\n",
            "   macro avg     0.2415    0.1858    0.1166      4699\n",
            "weighted avg     0.2527    0.2781    0.1662      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.2155    0.3633    0.2705       245\n",
            "           2     0.5018    0.3672    0.4241       384\n",
            "           3     0.8750    0.1235    0.2165       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3525    0.8373    0.4961       381\n",
            "\n",
            "    accuracy                         0.3512      1623\n",
            "   macro avg     0.3241    0.2819    0.2345      1623\n",
            "weighted avg     0.3256    0.3512    0.2803      1623\n",
            "\n",
            "1.7130124767621357 0.16621592500362373 0.1830236496659784 0.2803071347626753\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.5178645849227905\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 3 loss average: 1.469\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3077    0.0745    0.1199       376\n",
            "           1     0.4974    0.2474    0.3304       764\n",
            "           2     0.3686    0.5352    0.4366      1080\n",
            "           3     0.6550    0.1749    0.2761       749\n",
            "           4     0.4700    0.4519    0.4608       520\n",
            "           5     0.3898    0.6314    0.4820      1210\n",
            "\n",
            "    accuracy                         0.4097      4699\n",
            "   macro avg     0.4481    0.3525    0.3510      4699\n",
            "weighted avg     0.4470    0.4097    0.3828      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9000    0.0625    0.1169       144\n",
            "           1     0.6204    0.6204    0.6204       245\n",
            "           2     0.5216    0.6589    0.5823       384\n",
            "           3     0.5917    0.5882    0.5900       170\n",
            "           4     0.6171    0.7224    0.6656       299\n",
            "           5     0.5412    0.5171    0.5289       381\n",
            "\n",
            "    accuracy                         0.5712      1623\n",
            "   macro avg     0.6320    0.5282    0.5173      1623\n",
            "weighted avg     0.5996    0.5712    0.5504      1623\n",
            "\n",
            "1.468803068002065 0.3827715354110399 0.41717556282944346 0.5503651182680395\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.0347551107406616\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 4 loss average: 1.094\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5060    0.2261    0.3125       376\n",
            "           1     0.7145    0.6976    0.7060       764\n",
            "           2     0.5271    0.5130    0.5199      1080\n",
            "           3     0.6562    0.4459    0.5310       749\n",
            "           4     0.5409    0.7750    0.6372       520\n",
            "           5     0.5101    0.6240    0.5613      1210\n",
            "\n",
            "    accuracy                         0.5669      4699\n",
            "   macro avg     0.5758    0.5469    0.5446      4699\n",
            "weighted avg     0.5736    0.5669    0.5590      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5278    0.2639    0.3519       144\n",
            "           1     0.6718    0.7184    0.6943       245\n",
            "           2     0.4130    0.5130    0.4576       384\n",
            "           3     0.5879    0.5706    0.5791       170\n",
            "           4     0.7752    0.3344    0.4673       299\n",
            "           5     0.5232    0.7113    0.6029       381\n",
            "\n",
            "    accuracy                         0.5416      1623\n",
            "   macro avg     0.5831    0.5186    0.5255      1623\n",
            "weighted avg     0.5731    0.5416    0.5326      1623\n",
            "\n",
            "1.093823319921891 0.5589811673979228 0.4238448403980793 0.5325667398051134\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.8229241371154785\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 5 loss average: 0.950\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5864    0.4601    0.5156       376\n",
            "           1     0.7563    0.8207    0.7872       764\n",
            "           2     0.5662    0.5704    0.5683      1080\n",
            "           3     0.7079    0.5113    0.5938       749\n",
            "           4     0.6417    0.6923    0.6660       520\n",
            "           5     0.6144    0.7033    0.6559      1210\n",
            "\n",
            "    accuracy                         0.6406      4699\n",
            "   macro avg     0.6455    0.6264    0.6311      4699\n",
            "weighted avg     0.6421    0.6406    0.6371      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5571    0.2708    0.3645       144\n",
            "           1     0.7989    0.6163    0.6959       245\n",
            "           2     0.4824    0.5703    0.5227       384\n",
            "           3     0.7021    0.5824    0.6367       170\n",
            "           4     0.7093    0.5385    0.6122       299\n",
            "           5     0.5351    0.7612    0.6284       381\n",
            "\n",
            "    accuracy                         0.5909      1623\n",
            "   macro avg     0.6308    0.5566    0.5767      1623\n",
            "weighted avg     0.6140    0.5909    0.5880      1623\n",
            "\n",
            "0.9495954389373461 0.6371012329013701 0.4571595429228621 0.5880226626641275\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.907461941242218\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 6 loss average: 0.795\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6923    0.4548    0.5490       376\n",
            "           1     0.7957    0.8259    0.8105       764\n",
            "           2     0.6486    0.6324    0.6404      1080\n",
            "           3     0.7318    0.6449    0.6856       749\n",
            "           4     0.7043    0.7923    0.7457       520\n",
            "           5     0.6627    0.7455    0.7017      1210\n",
            "\n",
            "    accuracy                         0.6984      4699\n",
            "   macro avg     0.7059    0.6826    0.6888      4699\n",
            "weighted avg     0.6991    0.6984    0.6954      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5326    0.3403    0.4153       144\n",
            "           1     0.7571    0.6490    0.6989       245\n",
            "           2     0.4839    0.6667    0.5608       384\n",
            "           3     0.6358    0.5647    0.5981       170\n",
            "           4     0.7553    0.4749    0.5832       299\n",
            "           5     0.5430    0.6457    0.5899       381\n",
            "\n",
            "    accuracy                         0.5841      1623\n",
            "   macro avg     0.6180    0.5569    0.5744      1623\n",
            "weighted avg     0.6093    0.5841    0.5836      1623\n",
            "\n",
            "0.7952252297351757 0.6953815382562173 0.5100573765352957 0.5835986082858315\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.6751348972320557\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 7 loss average: 0.671\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7139    0.6702    0.6914       376\n",
            "           1     0.8265    0.8665    0.8460       764\n",
            "           2     0.7548    0.7269    0.7406      1080\n",
            "           3     0.7837    0.7303    0.7560       749\n",
            "           4     0.7623    0.8077    0.7843       520\n",
            "           5     0.7357    0.7636    0.7494      1210\n",
            "\n",
            "    accuracy                         0.7640      4699\n",
            "   macro avg     0.7628    0.7609    0.7613      4699\n",
            "weighted avg     0.7637    0.7640    0.7634      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6765    0.1597    0.2584       144\n",
            "           1     0.7130    0.6286    0.6681       245\n",
            "           2     0.4587    0.7240    0.5616       384\n",
            "           3     0.6369    0.5882    0.6116       170\n",
            "           4     0.6783    0.6488    0.6632       299\n",
            "           5     0.6049    0.5144    0.5560       381\n",
            "\n",
            "    accuracy                         0.5823      1623\n",
            "   macro avg     0.6281    0.5440    0.5532      1623\n",
            "weighted avg     0.6099    0.5823    0.5734      1623\n",
            "\n",
            "0.6709478708604971 0.7633532976573192 0.4889360582001928 0.5734412689904226\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.4494135081768036\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 8 loss average: 0.569\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7327    0.5904    0.6539       376\n",
            "           1     0.8774    0.8246    0.8502       764\n",
            "           2     0.7870    0.7935    0.7902      1080\n",
            "           3     0.8221    0.7837    0.8025       749\n",
            "           4     0.7230    0.8231    0.7698       520\n",
            "           5     0.7693    0.8157    0.7918      1210\n",
            "\n",
            "    accuracy                         0.7897      4699\n",
            "   macro avg     0.7852    0.7718    0.7764      4699\n",
            "weighted avg     0.7913    0.7897    0.7892      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5158    0.3403    0.4100       144\n",
            "           1     0.5731    0.8000    0.6678       245\n",
            "           2     0.5746    0.5417    0.5576       384\n",
            "           3     0.5410    0.5824    0.5609       170\n",
            "           4     0.5843    0.6488    0.6149       299\n",
            "           5     0.6084    0.4934    0.5449       381\n",
            "\n",
            "    accuracy                         0.5755      1623\n",
            "   macro avg     0.5662    0.5678    0.5594      1623\n",
            "weighted avg     0.5754    0.5755    0.5691      1623\n",
            "\n",
            "0.5694599502409498 0.789166971629534 0.4943882300864093 0.5690803181301157\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.7941340208053589\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 9 loss average: 0.511\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7880    0.6622    0.7197       376\n",
            "           1     0.8471    0.9136    0.8791       764\n",
            "           2     0.8143    0.8037    0.8089      1080\n",
            "           3     0.8392    0.8011    0.8197       749\n",
            "           4     0.7838    0.8577    0.8191       520\n",
            "           5     0.8180    0.8174    0.8177      1210\n",
            "\n",
            "    accuracy                         0.8193      4699\n",
            "   macro avg     0.8151    0.8093    0.8107      4699\n",
            "weighted avg     0.8191    0.8193    0.8183      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4583    0.3819    0.4167       144\n",
            "           1     0.6140    0.7143    0.6604       245\n",
            "           2     0.5109    0.6094    0.5558       384\n",
            "           3     0.5190    0.6412    0.5737       170\n",
            "           4     0.6857    0.4816    0.5658       299\n",
            "           5     0.5824    0.5197    0.5492       381\n",
            "\n",
            "    accuracy                         0.5638      1623\n",
            "   macro avg     0.5617    0.5580    0.5536      1623\n",
            "weighted avg     0.5716    0.5638    0.5614      1623\n",
            "\n",
            "0.5106814928973714 0.8182921362336482 0.49968439806497783 0.5614242710272022\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.32769396901130676\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 10 loss average: 0.421\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7825    0.7846    0.7835       376\n",
            "           1     0.9046    0.9058    0.9052       764\n",
            "           2     0.8475    0.8491    0.8483      1080\n",
            "           3     0.8633    0.8344    0.8486       749\n",
            "           4     0.8278    0.8135    0.8206       520\n",
            "           5     0.8371    0.8579    0.8473      1210\n",
            "\n",
            "    accuracy                         0.8491      4699\n",
            "   macro avg     0.8438    0.8409    0.8423      4699\n",
            "weighted avg     0.8492    0.8491    0.8491      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5059    0.2986    0.3755       144\n",
            "           1     0.7039    0.6694    0.6862       245\n",
            "           2     0.4699    0.6510    0.5459       384\n",
            "           3     0.5340    0.6000    0.5651       170\n",
            "           4     0.6389    0.6154    0.6269       299\n",
            "           5     0.6122    0.4724    0.5333       381\n",
            "\n",
            "    accuracy                         0.5687      1623\n",
            "   macro avg     0.5775    0.5511    0.5555      1623\n",
            "weighted avg     0.5797    0.5687    0.5659      1623\n",
            "\n",
            "0.42140147897104424 0.8490949760999424 0.5143421324764922 0.565937934203134\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.20175640285015106\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 11 loss average: 0.372\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8071    0.7899    0.7984       376\n",
            "           1     0.9172    0.9280    0.9226       764\n",
            "           2     0.8658    0.8722    0.8690      1080\n",
            "           3     0.8560    0.8812    0.8684       749\n",
            "           4     0.8404    0.8404    0.8404       520\n",
            "           5     0.8796    0.8570    0.8681      1210\n",
            "\n",
            "    accuracy                         0.8687      4699\n",
            "   macro avg     0.8610    0.8615    0.8612      4699\n",
            "weighted avg     0.8686    0.8687    0.8686      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5056    0.3125    0.3863       144\n",
            "           1     0.6802    0.5469    0.6063       245\n",
            "           2     0.4415    0.7266    0.5492       384\n",
            "           3     0.5568    0.5765    0.5665       170\n",
            "           4     0.7356    0.4281    0.5412       299\n",
            "           5     0.5803    0.5407    0.5598       381\n",
            "\n",
            "    accuracy                         0.5484      1623\n",
            "   macro avg     0.5833    0.5219    0.5349      1623\n",
            "weighted avg     0.5821    0.5484    0.5462      1623\n",
            "\n",
            "0.37196471154068905 0.8685825847802001 0.4983916201886942 0.5461961688333111\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.364691823720932\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 12 loss average: 0.326\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8596    0.8138    0.8361       376\n",
            "           1     0.9350    0.9411    0.9380       764\n",
            "           2     0.8655    0.8639    0.8647      1080\n",
            "           3     0.9007    0.8959    0.8983       749\n",
            "           4     0.8598    0.8962    0.8776       520\n",
            "           5     0.8850    0.8843    0.8847      1210\n",
            "\n",
            "    accuracy                         0.8864      4699\n",
            "   macro avg     0.8843    0.8825    0.8832      4699\n",
            "weighted avg     0.8863    0.8864    0.8862      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4865    0.3750    0.4235       144\n",
            "           1     0.5820    0.7388    0.6511       245\n",
            "           2     0.5278    0.4453    0.4831       384\n",
            "           3     0.4517    0.6882    0.5455       170\n",
            "           4     0.6915    0.4649    0.5560       299\n",
            "           5     0.5324    0.5827    0.5564       381\n",
            "\n",
            "    accuracy                         0.5447      1623\n",
            "   macro avg     0.5453    0.5491    0.5359      1623\n",
            "weighted avg     0.5556    0.5447    0.5403      1623\n",
            "\n",
            "0.3260736418887973 0.8862450813384138 0.47800323936269773 0.5403267925695273\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.38849321007728577\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 13 loss average: 0.259\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8667    0.8644    0.8655       376\n",
            "           1     0.9351    0.9437    0.9394       764\n",
            "           2     0.8943    0.8935    0.8939      1080\n",
            "           3     0.9061    0.9146    0.9103       749\n",
            "           4     0.9015    0.8981    0.8998       520\n",
            "           5     0.9142    0.9066    0.9104      1210\n",
            "\n",
            "    accuracy                         0.9066      4699\n",
            "   macro avg     0.9030    0.9035    0.9032      4699\n",
            "weighted avg     0.9065    0.9066    0.9065      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5000    0.2361    0.3208       144\n",
            "           1     0.7090    0.5469    0.6175       245\n",
            "           2     0.4527    0.6354    0.5287       384\n",
            "           3     0.5046    0.6471    0.5670       170\n",
            "           4     0.6409    0.5552    0.5950       299\n",
            "           5     0.5571    0.5118    0.5335       381\n",
            "\n",
            "    accuracy                         0.5441      1623\n",
            "   macro avg     0.5607    0.5221    0.5271      1623\n",
            "weighted avg     0.5602    0.5441    0.5410      1623\n",
            "\n",
            "0.2586712957515071 0.9065455208896096 0.4869090782408025 0.5410134371950626\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.18173356354236603\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 14 loss average: 0.242\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8832    0.8644    0.8737       376\n",
            "           1     0.9543    0.9568    0.9556       764\n",
            "           2     0.9128    0.9009    0.9068      1080\n",
            "           3     0.9074    0.9159    0.9116       749\n",
            "           4     0.9049    0.9154    0.9101       520\n",
            "           5     0.9088    0.9140    0.9114      1210\n",
            "\n",
            "    accuracy                         0.9144      4699\n",
            "   macro avg     0.9119    0.9112    0.9115      4699\n",
            "weighted avg     0.9144    0.9144    0.9144      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4375    0.3889    0.4118       144\n",
            "           1     0.6529    0.6449    0.6489       245\n",
            "           2     0.4874    0.6562    0.5594       384\n",
            "           3     0.6012    0.6118    0.6064       170\n",
            "           4     0.6413    0.4783    0.5479       299\n",
            "           5     0.5912    0.5276    0.5576       381\n",
            "\n",
            "    accuracy                         0.5632      1623\n",
            "   macro avg     0.5686    0.5513    0.5553      1623\n",
            "weighted avg     0.5726    0.5632    0.5622      1623\n",
            "\n",
            "0.24177494505420327 0.9144021378901742 0.5053909192896062 0.5621743784515698\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.19713521003723145\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 15 loss average: 0.208\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8957    0.8910    0.8933       376\n",
            "           1     0.9537    0.9699    0.9617       764\n",
            "           2     0.9260    0.9157    0.9209      1080\n",
            "           3     0.9318    0.9119    0.9217       749\n",
            "           4     0.9328    0.9346    0.9337       520\n",
            "           5     0.9144    0.9264    0.9204      1210\n",
            "\n",
            "    accuracy                         0.9268      4699\n",
            "   macro avg     0.9257    0.9249    0.9253      4699\n",
            "weighted avg     0.9268    0.9268    0.9267      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4240    0.3681    0.3941       144\n",
            "           1     0.6159    0.6939    0.6526       245\n",
            "           2     0.4876    0.6146    0.5438       384\n",
            "           3     0.5152    0.6000    0.5543       170\n",
            "           4     0.6818    0.4515    0.5433       299\n",
            "           5     0.5760    0.5171    0.5450       381\n",
            "\n",
            "    accuracy                         0.5502      1623\n",
            "   macro avg     0.5501    0.5408    0.5388      1623\n",
            "weighted avg     0.5608    0.5502    0.5482      1623\n",
            "\n",
            "0.2077048916835338 0.9267315189959585 0.47938699725803285 0.5482068301177379\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.13043707609176636\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 16 loss average: 0.181\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9235    0.8989    0.9111       376\n",
            "           1     0.9607    0.9594    0.9601       764\n",
            "           2     0.9336    0.9241    0.9288      1080\n",
            "           3     0.9212    0.9212    0.9212       749\n",
            "           4     0.9287    0.9519    0.9402       520\n",
            "           5     0.9262    0.9331    0.9296      1210\n",
            "\n",
            "    accuracy                         0.9328      4699\n",
            "   macro avg     0.9323    0.9314    0.9318      4699\n",
            "weighted avg     0.9328    0.9328    0.9327      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3671    0.5278    0.4330       144\n",
            "           1     0.5476    0.7510    0.6334       245\n",
            "           2     0.5316    0.5703    0.5503       384\n",
            "           3     0.5921    0.5294    0.5590       170\n",
            "           4     0.6606    0.3645    0.4698       299\n",
            "           5     0.5812    0.5354    0.5574       381\n",
            "\n",
            "    accuracy                         0.5434      1623\n",
            "   macro avg     0.5467    0.5464    0.5338      1623\n",
            "weighted avg     0.5612    0.5434    0.5402      1623\n",
            "\n",
            "0.181380626008225 0.9327195979371469 0.45779166465034493 0.5401764261101597\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.14378531277179718\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 17 loss average: 0.153\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9251    0.9202    0.9227       376\n",
            "           1     0.9675    0.9751    0.9713       764\n",
            "           2     0.9442    0.9407    0.9425      1080\n",
            "           3     0.9377    0.9453    0.9415       749\n",
            "           4     0.9412    0.9538    0.9475       520\n",
            "           5     0.9515    0.9413    0.9464      1210\n",
            "\n",
            "    accuracy                         0.9470      4699\n",
            "   macro avg     0.9446    0.9461    0.9453      4699\n",
            "weighted avg     0.9470    0.9470    0.9470      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5443    0.2986    0.3857       144\n",
            "           1     0.6480    0.6612    0.6545       245\n",
            "           2     0.4817    0.6510    0.5537       384\n",
            "           3     0.5365    0.6059    0.5691       170\n",
            "           4     0.6382    0.5251    0.5761       299\n",
            "           5     0.5757    0.5092    0.5404       381\n",
            "\n",
            "    accuracy                         0.5601      1623\n",
            "   macro avg     0.5707    0.5418    0.5466      1623\n",
            "weighted avg     0.5690    0.5601    0.5566      1623\n",
            "\n",
            "0.15316958849628767 0.9469897254706564 0.47477013882480945 0.5566350302867504\n",
            "Patience counter: 11\n",
            "Done! It took 7.1e+02 secs\n",
            "\n",
            "Current RUN: 2\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0493544302880764\n",
            "Best test f1 weighted\n",
            "0.5835986082858315\n",
            "Best epoch\n",
            "6\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/ubuntu_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.1 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.7584577798843384\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 1 loss average: 1.784\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.1815    0.2648    0.2154      1080\n",
            "           3     0.1698    0.0240    0.0421       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2592    0.6463    0.3700      1210\n",
            "\n",
            "    accuracy                         0.2311      4699\n",
            "   macro avg     0.1017    0.1559    0.1046      4699\n",
            "weighted avg     0.1355    0.2311    0.1515      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.1416    0.3796    0.2062       245\n",
            "           2     0.3102    0.4661    0.3725       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3882    0.3963    0.3922       381\n",
            "\n",
            "    accuracy                         0.2606      1623\n",
            "   macro avg     0.1400    0.2070    0.1618      1623\n",
            "weighted avg     0.1859    0.2606    0.2113      1623\n",
            "\n",
            "1.7837825293342273 0.15148543649740762 0.1562069327282688 0.2113390152102963\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.586609125137329\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 2 loss average: 1.697\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1014    0.0851    0.0925       764\n",
            "           2     0.2926    0.3926    0.3353      1080\n",
            "           3     0.1905    0.0160    0.0296       749\n",
            "           4     0.0663    0.0212    0.0321       520\n",
            "           5     0.3101    0.6099    0.4111      1210\n",
            "\n",
            "    accuracy                         0.2660      4699\n",
            "   macro avg     0.1601    0.1875    0.1501      4699\n",
            "weighted avg     0.2013    0.2660    0.2062      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.1754    0.3429    0.2320       245\n",
            "           2     0.3516    0.0833    0.1347       384\n",
            "           3     0.3726    0.5765    0.4527       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.4013    0.8320    0.5414       381\n",
            "\n",
            "    accuracy                         0.3272      1623\n",
            "   macro avg     0.2168    0.3058    0.2268      1623\n",
            "weighted avg     0.2429    0.3272    0.2414      1623\n",
            "\n",
            "1.6968533520897229 0.20624001549273283 0.2144447840768786 0.2414179788841362\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.4424055814743042\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 3 loss average: 1.459\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.0027    0.0053       376\n",
            "           1     0.3056    0.2356    0.2661       764\n",
            "           2     0.3154    0.3898    0.3487      1080\n",
            "           3     0.5000    0.3004    0.3753       749\n",
            "           4     0.4288    0.5096    0.4657       520\n",
            "           5     0.4636    0.6529    0.5422      1210\n",
            "\n",
            "    accuracy                         0.4005      4699\n",
            "   macro avg     0.3911    0.3485    0.3339      4699\n",
            "weighted avg     0.3954    0.4005    0.3748      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.1667    0.0069    0.0133       144\n",
            "           1     0.6207    0.2939    0.3989       245\n",
            "           2     0.4541    0.5156    0.4829       384\n",
            "           3     0.7634    0.4176    0.5399       170\n",
            "           4     0.3940    0.9197    0.5517       299\n",
            "           5     0.5949    0.4278    0.4977       381\n",
            "\n",
            "    accuracy                         0.4806      1623\n",
            "   macro avg     0.4990    0.4303    0.4141      1623\n",
            "weighted avg     0.5081    0.4806    0.4507      1623\n",
            "\n",
            "1.4593780376017094 0.3747978279762891 0.3723399139485638 0.4506788798261659\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.1744918823242188\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 4 loss average: 1.185\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3514    0.3112    0.3300       376\n",
            "           1     0.6547    0.7369    0.6933       764\n",
            "           2     0.4762    0.3991    0.4343      1080\n",
            "           3     0.6301    0.4139    0.4996       749\n",
            "           4     0.4913    0.4885    0.4899       520\n",
            "           5     0.4987    0.6562    0.5667      1210\n",
            "\n",
            "    accuracy                         0.5254      4699\n",
            "   macro avg     0.5171    0.5010    0.5023      4699\n",
            "weighted avg     0.5272    0.5254    0.5187      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.0069    0.0136       144\n",
            "           1     0.8231    0.4939    0.6173       245\n",
            "           2     0.4751    0.6458    0.5475       384\n",
            "           3     0.6277    0.6941    0.6592       170\n",
            "           4     0.5381    0.7793    0.6366       299\n",
            "           5     0.6333    0.5486    0.5879       381\n",
            "\n",
            "    accuracy                         0.5730      1623\n",
            "   macro avg     0.5718    0.5281    0.5104      1623\n",
            "weighted avg     0.5798    0.5730    0.5483      1623\n",
            "\n",
            "1.185433531800906 0.5187271069172431 0.421840829671927 0.5482686038567358\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.9745271801948547\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 5 loss average: 1.002\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5185    0.2979    0.3784       376\n",
            "           1     0.7113    0.7997    0.7529       764\n",
            "           2     0.5553    0.4324    0.4862      1080\n",
            "           3     0.6741    0.5274    0.5918       749\n",
            "           4     0.5496    0.7346    0.6288       520\n",
            "           5     0.5593    0.6942    0.6195      1210\n",
            "\n",
            "    accuracy                         0.5974      4699\n",
            "   macro avg     0.5947    0.5810    0.5763      4699\n",
            "weighted avg     0.5970    0.5974    0.5879      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6557    0.2778    0.3902       144\n",
            "           1     0.7534    0.6735    0.7112       245\n",
            "           2     0.4812    0.5990    0.5336       384\n",
            "           3     0.6710    0.6118    0.6400       170\n",
            "           4     0.7100    0.5485    0.6189       299\n",
            "           5     0.5491    0.6903    0.6116       381\n",
            "\n",
            "    accuracy                         0.5952      1623\n",
            "   macro avg     0.6367    0.5668    0.5843      1623\n",
            "weighted avg     0.6157    0.5952    0.5929      1623\n",
            "\n",
            "1.001560989767313 0.5878640504491063 0.4517838314566108 0.5928720561318387\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.7628366351127625\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 6 loss average: 0.838\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5605    0.5665    0.5635       376\n",
            "           1     0.7832    0.8416    0.8114       764\n",
            "           2     0.6112    0.5546    0.5816      1080\n",
            "           3     0.7420    0.6182    0.6744       749\n",
            "           4     0.6517    0.6981    0.6741       520\n",
            "           5     0.6387    0.7058    0.6706      1210\n",
            "\n",
            "    accuracy                         0.6672      4699\n",
            "   macro avg     0.6646    0.6641    0.6626      4699\n",
            "weighted avg     0.6675    0.6672    0.6654      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6207    0.2500    0.3564       144\n",
            "           1     0.6799    0.7714    0.7228       245\n",
            "           2     0.4601    0.7214    0.5619       384\n",
            "           3     0.7600    0.5588    0.6441       170\n",
            "           4     0.7021    0.4415    0.5421       299\n",
            "           5     0.6102    0.5958    0.6029       381\n",
            "\n",
            "    accuracy                         0.5890      1623\n",
            "   macro avg     0.6388    0.5565    0.5717      1623\n",
            "weighted avg     0.6188    0.5890    0.5825      1623\n",
            "\n",
            "0.8384212826689085 0.6654449562764998 0.4859029692384138 0.5825316142260711\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 0.7631512880325317\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 7 loss average: 0.712\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6119    0.6037    0.6078       376\n",
            "           1     0.8193    0.8665    0.8422       764\n",
            "           2     0.6932    0.6861    0.6896      1080\n",
            "           3     0.7666    0.6622    0.7106       749\n",
            "           4     0.7211    0.7308    0.7259       520\n",
            "           5     0.7063    0.7455    0.7254      1210\n",
            "\n",
            "    accuracy                         0.7253      4699\n",
            "   macro avg     0.7197    0.7158    0.7169      4699\n",
            "weighted avg     0.7254    0.7253    0.7244      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6176    0.2917    0.3962       144\n",
            "           1     0.7742    0.6857    0.7273       245\n",
            "           2     0.4831    0.6693    0.5611       384\n",
            "           3     0.5909    0.6118    0.6012       170\n",
            "           4     0.6511    0.6054    0.6274       299\n",
            "           5     0.6051    0.5591    0.5812       381\n",
            "\n",
            "    accuracy                         0.5946      1623\n",
            "   macro avg     0.6203    0.5705    0.5824      1623\n",
            "weighted avg     0.6099    0.5946    0.5927      1623\n",
            "\n",
            "0.7116595941285292 0.7244484382961401 0.5175712831234254 0.5926835922502454\n",
            "Patience counter: 0\n",
            "Epoch: 8, iter 0: loss = 0.3026943504810333\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 8 loss average: 0.609\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7291    0.6729    0.6999       376\n",
            "           1     0.8680    0.8953    0.8814       764\n",
            "           2     0.7453    0.7370    0.7412      1080\n",
            "           3     0.8176    0.7183    0.7647       749\n",
            "           4     0.7586    0.8096    0.7833       520\n",
            "           5     0.7545    0.8000    0.7766      1210\n",
            "\n",
            "    accuracy                         0.7789      4699\n",
            "   macro avg     0.7789    0.7722    0.7745      4699\n",
            "weighted avg     0.7793    0.7789    0.7782      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4839    0.3125    0.3797       144\n",
            "           1     0.8011    0.5755    0.6698       245\n",
            "           2     0.4481    0.6745    0.5385       384\n",
            "           3     0.5924    0.6412    0.6158       170\n",
            "           4     0.6731    0.4682    0.5523       299\n",
            "           5     0.5391    0.5433    0.5412       381\n",
            "\n",
            "    accuracy                         0.5551      1623\n",
            "   macro avg     0.5896    0.5359    0.5496      1623\n",
            "weighted avg     0.5825    0.5551    0.5555      1623\n",
            "\n",
            "0.6087573493520418 0.7781999743575119 0.5273123335319592 0.5554946054775302\n",
            "Patience counter: 1\n",
            "Epoch: 9, iter 0: loss = 0.7008218765258789\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 9 loss average: 0.509\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7787    0.7207    0.7486       376\n",
            "           1     0.8906    0.9058    0.8981       764\n",
            "           2     0.7824    0.7991    0.7907      1080\n",
            "           3     0.8340    0.7917    0.8123       749\n",
            "           4     0.8092    0.8481    0.8282       520\n",
            "           5     0.8082    0.8116    0.8099      1210\n",
            "\n",
            "    accuracy                         0.8176      4699\n",
            "   macro avg     0.8172    0.8128    0.8146      4699\n",
            "weighted avg     0.8175    0.8176    0.8173      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5441    0.2569    0.3491       144\n",
            "           1     0.6441    0.7388    0.6882       245\n",
            "           2     0.4989    0.5911    0.5411       384\n",
            "           3     0.4615    0.7412    0.5688       170\n",
            "           4     0.6448    0.5585    0.5986       299\n",
            "           5     0.5923    0.4462    0.5090       381\n",
            "\n",
            "    accuracy                         0.5595      1623\n",
            "   macro avg     0.5643    0.5555    0.5425      1623\n",
            "weighted avg     0.5697    0.5595    0.5522      1623\n",
            "\n",
            "0.5091052381321788 0.8173244847244459 0.49252998834961365 0.5522269349946994\n",
            "Patience counter: 2\n",
            "Epoch: 10, iter 0: loss = 0.46419641375541687\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 10 loss average: 0.432\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8323    0.6995    0.7601       376\n",
            "           1     0.8943    0.9084    0.9013       764\n",
            "           2     0.8236    0.8213    0.8224      1080\n",
            "           3     0.8581    0.8398    0.8489       749\n",
            "           4     0.8050    0.8731    0.8376       520\n",
            "           5     0.8370    0.8529    0.8449      1210\n",
            "\n",
            "    accuracy                         0.8425      4699\n",
            "   macro avg     0.8417    0.8325    0.8359      4699\n",
            "weighted avg     0.8427    0.8425    0.8419      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5140    0.3819    0.4382       144\n",
            "           1     0.7608    0.6490    0.7004       245\n",
            "           2     0.4509    0.6693    0.5388       384\n",
            "           3     0.5955    0.6235    0.6092       170\n",
            "           4     0.7086    0.4147    0.5232       299\n",
            "           5     0.5547    0.5591    0.5569       381\n",
            "\n",
            "    accuracy                         0.5632      1623\n",
            "   macro avg     0.5974    0.5496    0.5611      1623\n",
            "weighted avg     0.5903    0.5632    0.5630      1623\n",
            "\n",
            "0.4321549590677023 0.8419400276110733 0.5086296806225781 0.563016219245902\n",
            "Patience counter: 3\n",
            "Epoch: 11, iter 0: loss = 0.2561509311199188\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 11 loss average: 0.367\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7995    0.8059    0.8026       376\n",
            "           1     0.9183    0.9411    0.9295       764\n",
            "           2     0.8491    0.8648    0.8569      1080\n",
            "           3     0.8721    0.8652    0.8686       749\n",
            "           4     0.8664    0.8481    0.8571       520\n",
            "           5     0.8768    0.8587    0.8676      1210\n",
            "\n",
            "    accuracy                         0.8691      4699\n",
            "   macro avg     0.8637    0.8639    0.8637      4699\n",
            "weighted avg     0.8691    0.8691    0.8690      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4643    0.3611    0.4063       144\n",
            "           1     0.6459    0.6776    0.6614       245\n",
            "           2     0.4776    0.6667    0.5565       384\n",
            "           3     0.6538    0.6000    0.6258       170\n",
            "           4     0.6604    0.4682    0.5479       299\n",
            "           5     0.5914    0.5433    0.5663       381\n",
            "\n",
            "    accuracy                         0.5687      1623\n",
            "   macro avg     0.5822    0.5528    0.5607      1623\n",
            "weighted avg     0.5807    0.5687    0.5670      1623\n",
            "\n",
            "0.36684056278318167 0.8690279366933804 0.5072078739432633 0.5669935863496468\n",
            "Patience counter: 4\n",
            "Epoch: 12, iter 0: loss = 0.2521473169326782\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 12 loss average: 0.314\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8459    0.8032    0.8240       376\n",
            "           1     0.9332    0.9516    0.9423       764\n",
            "           2     0.8840    0.8824    0.8832      1080\n",
            "           3     0.8719    0.8999    0.8857       749\n",
            "           4     0.8780    0.9000    0.8889       520\n",
            "           5     0.9033    0.8802    0.8916      1210\n",
            "\n",
            "    accuracy                         0.8915      4699\n",
            "   macro avg     0.8861    0.8862    0.8860      4699\n",
            "weighted avg     0.8914    0.8915    0.8913      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4322    0.3542    0.3893       144\n",
            "           1     0.7085    0.6449    0.6752       245\n",
            "           2     0.4483    0.7109    0.5498       384\n",
            "           3     0.5327    0.6706    0.5937       170\n",
            "           4     0.6791    0.4247    0.5226       299\n",
            "           5     0.5882    0.4199    0.4900       381\n",
            "\n",
            "    accuracy                         0.5441      1623\n",
            "   macro avg     0.5648    0.5375    0.5368      1623\n",
            "weighted avg     0.5704    0.5441    0.5401      1623\n",
            "\n",
            "0.3143029750014345 0.8912657201193112 0.4967296614465843 0.5400757280516096\n",
            "Patience counter: 5\n",
            "Epoch: 13, iter 0: loss = 0.2186238169670105\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 13 loss average: 0.267\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8418    0.8777    0.8594       376\n",
            "           1     0.9375    0.9620    0.9496       764\n",
            "           2     0.8996    0.8880    0.8938      1080\n",
            "           3     0.8946    0.9065    0.9005       749\n",
            "           4     0.9160    0.8808    0.8980       520\n",
            "           5     0.9107    0.9017    0.9061      1210\n",
            "\n",
            "    accuracy                         0.9049      4699\n",
            "   macro avg     0.9000    0.9028    0.9012      4699\n",
            "weighted avg     0.9050    0.9049    0.9048      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5412    0.3194    0.4017       144\n",
            "           1     0.6798    0.6327    0.6554       245\n",
            "           2     0.4412    0.7135    0.5453       384\n",
            "           3     0.7266    0.5941    0.6537       170\n",
            "           4     0.6512    0.4682    0.5447       299\n",
            "           5     0.6060    0.5328    0.5670       381\n",
            "\n",
            "    accuracy                         0.5662      1623\n",
            "   macro avg     0.6077    0.5435    0.5613      1623\n",
            "weighted avg     0.5934    0.5662    0.5655      1623\n",
            "\n",
            "0.26745431070836884 0.904830768984019 0.4950351819492498 0.5655338203899247\n",
            "Patience counter: 6\n",
            "Epoch: 14, iter 0: loss = 0.3079431653022766\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 14 loss average: 0.230\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8983    0.8457    0.8712       376\n",
            "           1     0.9474    0.9660    0.9566       764\n",
            "           2     0.9001    0.9093    0.9047      1080\n",
            "           3     0.9258    0.9159    0.9208       749\n",
            "           4     0.8883    0.9327    0.9099       520\n",
            "           5     0.9234    0.9066    0.9149      1210\n",
            "\n",
            "    accuracy                         0.9164      4699\n",
            "   macro avg     0.9139    0.9127    0.9130      4699\n",
            "weighted avg     0.9164    0.9164    0.9162      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4407    0.3611    0.3969       144\n",
            "           1     0.7236    0.5878    0.6486       245\n",
            "           2     0.4904    0.5339    0.5112       384\n",
            "           3     0.4930    0.6176    0.5483       170\n",
            "           4     0.6435    0.4950    0.5595       299\n",
            "           5     0.5124    0.5984    0.5521       381\n",
            "\n",
            "    accuracy                         0.5434      1623\n",
            "   macro avg     0.5506    0.5323    0.5361      1623\n",
            "weighted avg     0.5548    0.5434    0.5442      1623\n",
            "\n",
            "0.22961744638935974 0.9162272423950899 0.48217691207987673 0.544201126129189\n",
            "Patience counter: 7\n",
            "Epoch: 15, iter 0: loss = 0.13593734800815582\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 15 loss average: 0.194\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9058    0.9202    0.9129       376\n",
            "           1     0.9451    0.9686    0.9567       764\n",
            "           2     0.9164    0.9028    0.9095      1080\n",
            "           3     0.9332    0.9332    0.9332       749\n",
            "           4     0.9471    0.9288    0.9379       520\n",
            "           5     0.9240    0.9248    0.9244      1210\n",
            "\n",
            "    accuracy                         0.9283      4699\n",
            "   macro avg     0.9286    0.9297    0.9291      4699\n",
            "weighted avg     0.9282    0.9283    0.9282      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4519    0.3264    0.3790       144\n",
            "           1     0.6653    0.6408    0.6528       245\n",
            "           2     0.4669    0.5885    0.5207       384\n",
            "           3     0.5347    0.6353    0.5806       170\n",
            "           4     0.6109    0.5251    0.5647       299\n",
            "           5     0.5618    0.5013    0.5298       381\n",
            "\n",
            "    accuracy                         0.5459      1623\n",
            "   macro avg     0.5486    0.5362    0.5380      1623\n",
            "weighted avg     0.5514    0.5459    0.5446      1623\n",
            "\n",
            "0.1939564705826342 0.9282136389255596 0.4887881799904232 0.5446162058742099\n",
            "Patience counter: 8\n",
            "Epoch: 16, iter 0: loss = 0.14760169386863708\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 16 loss average: 0.179\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9120    0.9096    0.9108       376\n",
            "           1     0.9596    0.9647    0.9621       764\n",
            "           2     0.9270    0.9287    0.9278      1080\n",
            "           3     0.9135    0.9306    0.9220       749\n",
            "           4     0.9394    0.9538    0.9466       520\n",
            "           5     0.9434    0.9223    0.9327      1210\n",
            "\n",
            "    accuracy                         0.9345      4699\n",
            "   macro avg     0.9325    0.9349    0.9337      4699\n",
            "weighted avg     0.9345    0.9345    0.9344      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4875    0.2708    0.3482       144\n",
            "           1     0.6444    0.6286    0.6364       245\n",
            "           2     0.4658    0.5859    0.5190       384\n",
            "           3     0.5073    0.6118    0.5547       170\n",
            "           4     0.6745    0.4783    0.5597       299\n",
            "           5     0.5198    0.5512    0.5350       381\n",
            "\n",
            "    accuracy                         0.5391      1623\n",
            "   macro avg     0.5499    0.5211    0.5255      1623\n",
            "weighted avg     0.5502    0.5391    0.5366      1623\n",
            "\n",
            "0.17868397249064097 0.9344445688142867 0.4690796862394121 0.5365660833447329\n",
            "Patience counter: 9\n",
            "Epoch: 17, iter 0: loss = 0.044303085654973984\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 17 loss average: 0.161\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9350    0.9176    0.9262       376\n",
            "           1     0.9647    0.9647    0.9647       764\n",
            "           2     0.9329    0.9398    0.9363      1080\n",
            "           3     0.9360    0.9372    0.9366       749\n",
            "           4     0.9520    0.9538    0.9529       520\n",
            "           5     0.9370    0.9347    0.9359      1210\n",
            "\n",
            "    accuracy                         0.9419      4699\n",
            "   macro avg     0.9429    0.9413    0.9421      4699\n",
            "weighted avg     0.9419    0.9419    0.9419      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4314    0.3056    0.3577       144\n",
            "           1     0.6727    0.6041    0.6366       245\n",
            "           2     0.4545    0.6901    0.5481       384\n",
            "           3     0.6569    0.5294    0.5863       170\n",
            "           4     0.6250    0.4849    0.5461       299\n",
            "           5     0.5645    0.5171    0.5397       381\n",
            "\n",
            "    accuracy                         0.5478      1623\n",
            "   macro avg     0.5675    0.5219    0.5358      1623\n",
            "weighted avg     0.5638    0.5478    0.5462      1623\n",
            "\n",
            "0.16138947095411518 0.9418930096850856 0.47903722549792416 0.5462354240493091\n",
            "Patience counter: 10\n",
            "Epoch: 18, iter 0: loss = 0.11903414130210876\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 18 loss average: 0.141\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9351    0.9202    0.9276       376\n",
            "           1     0.9698    0.9673    0.9685       764\n",
            "           2     0.9401    0.9454    0.9428      1080\n",
            "           3     0.9532    0.9519    0.9526       749\n",
            "           4     0.9538    0.9538    0.9538       520\n",
            "           5     0.9522    0.9545    0.9534      1210\n",
            "\n",
            "    accuracy                         0.9513      4699\n",
            "   macro avg     0.9507    0.9489    0.9498      4699\n",
            "weighted avg     0.9513    0.9513    0.9513      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3974    0.4167    0.4068       144\n",
            "           1     0.6134    0.6735    0.6420       245\n",
            "           2     0.4730    0.5938    0.5266       384\n",
            "           3     0.5487    0.6294    0.5863       170\n",
            "           4     0.6613    0.4114    0.5072       299\n",
            "           5     0.5559    0.4961    0.5243       381\n",
            "\n",
            "    accuracy                         0.5373      1623\n",
            "   macro avg     0.5416    0.5368    0.5322      1623\n",
            "weighted avg     0.5496    0.5373    0.5355      1623\n",
            "\n",
            "0.14111809368478134 0.9512597090228855 0.4878565791499467 0.5355188804685856\n",
            "Patience counter: 11\n",
            "Done! It took 7.5e+02 secs\n",
            "\n",
            "Current RUN: 3\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0126450024545193\n",
            "Best test f1 weighted\n",
            "0.5926835922502454\n",
            "Best epoch\n",
            "7\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/ubuntu_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.1 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.6267752647399902\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 1 loss average: 1.793\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1699    0.1034    0.1286       764\n",
            "           2     0.2608    0.2296    0.2442      1080\n",
            "           3     0.1553    0.0668    0.0934       749\n",
            "           4     0.1936    0.1404    0.1628       520\n",
            "           5     0.2411    0.5149    0.3284      1210\n",
            "\n",
            "    accuracy                         0.2283      4699\n",
            "   macro avg     0.1701    0.1758    0.1596      4699\n",
            "weighted avg     0.1958    0.2283    0.1945      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3295    0.1484    0.2047       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2462    0.9370    0.3900       381\n",
            "\n",
            "    accuracy                         0.2551      1623\n",
            "   macro avg     0.0959    0.1809    0.0991      1623\n",
            "weighted avg     0.1358    0.2551    0.1400      1623\n",
            "\n",
            "1.7931777437527974 0.19449328211739947 0.12463349335884899 0.13996533095113972\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.5137362480163574\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 2 loss average: 1.732\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2771    0.2935    0.2851      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2568    0.7545    0.3832      1210\n",
            "\n",
            "    accuracy                         0.2618      4699\n",
            "   macro avg     0.0890    0.1747    0.1114      4699\n",
            "weighted avg     0.1298    0.2618    0.1642      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3986    0.4557    0.4253       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2796    0.8688    0.4230       381\n",
            "\n",
            "    accuracy                         0.3118      1623\n",
            "   macro avg     0.1130    0.2207    0.1414      1623\n",
            "weighted avg     0.1599    0.3118    0.1999      1623\n",
            "\n",
            "1.7320612370967865 0.16419725525829446 0.1445057762078729 0.19991940790289195\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.5305181741714478\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 3 loss average: 1.499\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.1818    0.0053    0.0103       376\n",
            "           1     0.5021    0.1597    0.2423       764\n",
            "           2     0.3308    0.4361    0.3762      1080\n",
            "           3     0.3464    0.0708    0.1175       749\n",
            "           4     0.5234    0.3442    0.4153       520\n",
            "           5     0.3678    0.7678    0.4973      1210\n",
            "\n",
            "    accuracy                         0.3737      4699\n",
            "   macro avg     0.3754    0.2973    0.2765      4699\n",
            "weighted avg     0.3800    0.3737    0.3194      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4000    0.0139    0.0268       144\n",
            "           1     0.5633    0.6898    0.6202       245\n",
            "           2     0.6066    0.4818    0.5370       384\n",
            "           3     0.8780    0.2118    0.3412       170\n",
            "           4     0.5506    0.8194    0.6586       299\n",
            "           5     0.4744    0.6562    0.5507       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5788    0.4788    0.4558      1623\n",
            "weighted avg     0.5688    0.5465    0.5094      1623\n",
            "\n",
            "1.4992626706759136 0.31943912921134116 0.34274475433875096 0.5093998219624386\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.0992063283920288\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 4 loss average: 1.195\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.1920    0.0638    0.0958       376\n",
            "           1     0.6103    0.7317    0.6655       764\n",
            "           2     0.5000    0.3843    0.4346      1080\n",
            "           3     0.7017    0.2764    0.3966       749\n",
            "           4     0.4709    0.6058    0.5299       520\n",
            "           5     0.4705    0.7248    0.5706      1210\n",
            "\n",
            "    accuracy                         0.5101      4699\n",
            "   macro avg     0.4909    0.4644    0.4488      4699\n",
            "weighted avg     0.5146    0.5101    0.4845      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0139    0.0274       144\n",
            "           1     0.6355    0.8327    0.7208       245\n",
            "           2     0.5749    0.5495    0.5619       384\n",
            "           3     0.6387    0.5824    0.6092       170\n",
            "           4     0.5424    0.8127    0.6506       299\n",
            "           5     0.6152    0.5328    0.5710       381\n",
            "\n",
            "    accuracy                         0.5927      1623\n",
            "   macro avg     0.6678    0.5540    0.5235      1623\n",
            "weighted avg     0.6319    0.5927    0.5619      1623\n",
            "\n",
            "1.1950262474517028 0.48451296490970763 0.40076786171883677 0.5619160876683762\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.8395865559577942\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 5 loss average: 0.965\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6277    0.2287    0.3353       376\n",
            "           1     0.7570    0.8115    0.7833       764\n",
            "           2     0.5533    0.5574    0.5554      1080\n",
            "           3     0.7140    0.4633    0.5619       749\n",
            "           4     0.5472    0.7808    0.6434       520\n",
            "           5     0.5872    0.6926    0.6356      1210\n",
            "\n",
            "    accuracy                         0.6169      4699\n",
            "   macro avg     0.6311    0.5890    0.5858      4699\n",
            "weighted avg     0.6261    0.6169    0.6063      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3643    0.3542    0.3592       144\n",
            "           1     0.5588    0.8531    0.6753       245\n",
            "           2     0.4871    0.6406    0.5534       384\n",
            "           3     0.6776    0.6059    0.6398       170\n",
            "           4     0.8025    0.2174    0.3421       299\n",
            "           5     0.6065    0.5906    0.5984       381\n",
            "\n",
            "    accuracy                         0.5539      1623\n",
            "   macro avg     0.5828    0.5436    0.5280      1623\n",
            "weighted avg     0.5931    0.5539    0.5353      1623\n",
            "\n",
            "0.965004701167345 0.6062608325439635 0.45003075374413015 0.5352552489582\n",
            "Patience counter: 1\n",
            "Epoch: 6, iter 0: loss = 0.6844266057014465\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 6 loss average: 0.834\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5934    0.4814    0.5316       376\n",
            "           1     0.7882    0.8717    0.8278       764\n",
            "           2     0.6315    0.6491    0.6402      1080\n",
            "           3     0.7474    0.5688    0.6459       749\n",
            "           4     0.6697    0.7019    0.6854       520\n",
            "           5     0.6556    0.7174    0.6851      1210\n",
            "\n",
            "    accuracy                         0.6825      4699\n",
            "   macro avg     0.6810    0.6650    0.6693      4699\n",
            "weighted avg     0.6828    0.6825    0.6795      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8000    0.0833    0.1509       144\n",
            "           1     0.7381    0.6327    0.6813       245\n",
            "           2     0.4601    0.6615    0.5427       384\n",
            "           3     0.6337    0.6412    0.6374       170\n",
            "           4     0.6693    0.5686    0.6148       299\n",
            "           5     0.5571    0.6142    0.5843       381\n",
            "\n",
            "    accuracy                         0.5755      1623\n",
            "   macro avg     0.6431    0.5336    0.5353      1623\n",
            "weighted avg     0.6117    0.5755    0.5618      1623\n",
            "\n",
            "0.8343355941275755 0.6794923518852897 0.4553595489028234 0.5618435808885377\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.7414025068283081\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 7 loss average: 0.748\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6701    0.5239    0.5881       376\n",
            "           1     0.8229    0.8272    0.8251       764\n",
            "           2     0.6946    0.6676    0.6808      1080\n",
            "           3     0.7462    0.6595    0.7002       749\n",
            "           4     0.6795    0.7827    0.7274       520\n",
            "           5     0.6839    0.7562    0.7182      1210\n",
            "\n",
            "    accuracy                         0.7163      4699\n",
            "   macro avg     0.7162    0.7029    0.7066      4699\n",
            "weighted avg     0.7173    0.7163    0.7147      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3077    0.0556    0.0941       144\n",
            "           1     0.7183    0.7388    0.7284       245\n",
            "           2     0.5994    0.5339    0.5647       384\n",
            "           3     0.5330    0.7118    0.6096       170\n",
            "           4     0.5295    0.8094    0.6402       299\n",
            "           5     0.6113    0.5118    0.5571       381\n",
            "\n",
            "    accuracy                         0.5866      1623\n",
            "   macro avg     0.5499    0.5602    0.5324      1623\n",
            "weighted avg     0.5744    0.5866    0.5645      1623\n",
            "\n",
            "0.7481975120802721 0.7147303389005941 0.44793999544982793 0.564500962759698\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.5983486175537109\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 8 loss average: 0.626\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6918    0.6090    0.6478       376\n",
            "           1     0.8519    0.8887    0.8700       764\n",
            "           2     0.7416    0.7546    0.7480      1080\n",
            "           3     0.7983    0.7343    0.7650       749\n",
            "           4     0.7349    0.8212    0.7757       520\n",
            "           5     0.7562    0.7512    0.7537      1210\n",
            "\n",
            "    accuracy                         0.7680      4699\n",
            "   macro avg     0.7625    0.7599    0.7600      4699\n",
            "weighted avg     0.7676    0.7680    0.7671      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5636    0.2153    0.3116       144\n",
            "           1     0.7225    0.6163    0.6652       245\n",
            "           2     0.5249    0.5495    0.5369       384\n",
            "           3     0.5441    0.6529    0.5936       170\n",
            "           4     0.6807    0.5418    0.6034       299\n",
            "           5     0.4835    0.6535    0.5558       381\n",
            "\n",
            "    accuracy                         0.5638      1623\n",
            "   macro avg     0.5865    0.5382    0.5444      1623\n",
            "weighted avg     0.5791    0.5638    0.5589      1623\n",
            "\n",
            "0.6259465056161085 0.7670612517348394 0.4774317671454535 0.5588899045480742\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.5222868919372559\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 9 loss average: 0.534\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7695    0.6569    0.7088       376\n",
            "           1     0.8821    0.8914    0.8867       764\n",
            "           2     0.8004    0.8093    0.8048      1080\n",
            "           3     0.8031    0.8331    0.8178       749\n",
            "           4     0.7748    0.8269    0.8000       520\n",
            "           5     0.8113    0.7926    0.8018      1210\n",
            "\n",
            "    accuracy                         0.8119      4699\n",
            "   macro avg     0.8069    0.8017    0.8033      4699\n",
            "weighted avg     0.8116    0.8119    0.8112      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5610    0.3194    0.4071       144\n",
            "           1     0.6618    0.7347    0.6963       245\n",
            "           2     0.4277    0.7474    0.5441       384\n",
            "           3     0.6667    0.5294    0.5902       170\n",
            "           4     0.7368    0.4682    0.5726       299\n",
            "           5     0.6264    0.4488    0.5229       381\n",
            "\n",
            "    accuracy                         0.5632      1623\n",
            "   macro avg     0.6134    0.5413    0.5555      1623\n",
            "weighted avg     0.6035    0.5632    0.5600      1623\n",
            "\n",
            "0.5337793612852693 0.811213296714343 0.494591074002782 0.5600228869948223\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.1910228580236435\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 10 loss average: 0.464\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7895    0.6782    0.7296       376\n",
            "           1     0.8927    0.9149    0.9037       764\n",
            "           2     0.7946    0.8380    0.8157      1080\n",
            "           3     0.8522    0.8158    0.8336       749\n",
            "           4     0.7953    0.8519    0.8227       520\n",
            "           5     0.8314    0.8107    0.8209      1210\n",
            "\n",
            "    accuracy                         0.8287      4699\n",
            "   macro avg     0.8259    0.8182    0.8210      4699\n",
            "weighted avg     0.8289    0.8287    0.8281      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3632    0.5903    0.4497       144\n",
            "           1     0.6184    0.7143    0.6629       245\n",
            "           2     0.5718    0.5286    0.5494       384\n",
            "           3     0.6024    0.5882    0.5952       170\n",
            "           4     0.6609    0.3846    0.4863       299\n",
            "           5     0.5596    0.6037    0.5808       381\n",
            "\n",
            "    accuracy                         0.5595      1623\n",
            "   macro avg     0.5627    0.5683    0.5541      1623\n",
            "weighted avg     0.5771    0.5595    0.5582      1623\n",
            "\n",
            "0.4635921564574043 0.8280737968853008 0.4964565379043899 0.5582272666535051\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.6148830056190491\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 11 loss average: 0.395\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7949    0.7527    0.7732       376\n",
            "           1     0.9171    0.9267    0.9219       764\n",
            "           2     0.8486    0.8667    0.8575      1080\n",
            "           3     0.8529    0.8438    0.8483       749\n",
            "           4     0.8280    0.8423    0.8351       520\n",
            "           5     0.8639    0.8554    0.8596      1210\n",
            "\n",
            "    accuracy                         0.8581      4699\n",
            "   macro avg     0.8509    0.8479    0.8493      4699\n",
            "weighted avg     0.8578    0.8581    0.8578      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6557    0.2778    0.3902       144\n",
            "           1     0.6906    0.6286    0.6581       245\n",
            "           2     0.5126    0.5286    0.5205       384\n",
            "           3     0.4167    0.7059    0.5240       170\n",
            "           4     0.6564    0.6388    0.6475       299\n",
            "           5     0.5714    0.5459    0.5584       381\n",
            "\n",
            "    accuracy                         0.5644      1623\n",
            "   macro avg     0.5839    0.5543    0.5498      1623\n",
            "weighted avg     0.5824    0.5644    0.5624      1623\n",
            "\n",
            "0.3954036380164325 0.8578370532954481 0.4934780013775885 0.5623724398848305\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.18551720678806305\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 12 loss average: 0.357\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8338    0.8138    0.8237       376\n",
            "           1     0.9382    0.9332    0.9357       764\n",
            "           2     0.8623    0.8870    0.8745      1080\n",
            "           3     0.8877    0.8652    0.8763       749\n",
            "           4     0.8443    0.8654    0.8547       520\n",
            "           5     0.8940    0.8851    0.8895      1210\n",
            "\n",
            "    accuracy                         0.8823      4699\n",
            "   macro avg     0.8767    0.8750    0.8757      4699\n",
            "weighted avg     0.8826    0.8823    0.8823      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5833    0.3403    0.4298       144\n",
            "           1     0.7080    0.7224    0.7152       245\n",
            "           2     0.5144    0.5599    0.5362       384\n",
            "           3     0.5233    0.5941    0.5565       170\n",
            "           4     0.6493    0.5819    0.6138       299\n",
            "           5     0.5585    0.6010    0.5790       381\n",
            "\n",
            "    accuracy                         0.5823      1623\n",
            "   macro avg     0.5895    0.5666    0.5717      1623\n",
            "weighted avg     0.5859    0.5823    0.5802      1623\n",
            "\n",
            "0.35726719178880256 0.8823429755296215 0.503880290192771 0.580228109470853\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.19773396849632263\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 13 loss average: 0.296\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8714    0.8112    0.8402       376\n",
            "           1     0.9436    0.9424    0.9430       764\n",
            "           2     0.8808    0.8963    0.8885      1080\n",
            "           3     0.8760    0.8959    0.8858       749\n",
            "           4     0.8774    0.8808    0.8791       520\n",
            "           5     0.8882    0.8802    0.8842      1210\n",
            "\n",
            "    accuracy                         0.8910      4699\n",
            "   macro avg     0.8896    0.8844    0.8868      4699\n",
            "weighted avg     0.8910    0.8910    0.8909      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4407    0.3611    0.3969       144\n",
            "           1     0.7044    0.5837    0.6384       245\n",
            "           2     0.4485    0.7031    0.5477       384\n",
            "           3     0.5550    0.6235    0.5873       170\n",
            "           4     0.6204    0.5686    0.5934       299\n",
            "           5     0.6085    0.3753    0.4643       381\n",
            "\n",
            "    accuracy                         0.5447      1623\n",
            "   macro avg     0.5629    0.5359    0.5380      1623\n",
            "weighted avg     0.5668    0.5447    0.5410      1623\n",
            "\n",
            "0.2957871834902714 0.8909147416007768 0.4941616236603217 0.540982483118884\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.11533959954977036\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 14 loss average: 0.273\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8518    0.8404    0.8461       376\n",
            "           1     0.9463    0.9463    0.9463       764\n",
            "           2     0.9037    0.8954    0.8995      1080\n",
            "           3     0.8954    0.8919    0.8936       749\n",
            "           4     0.8736    0.9173    0.8949       520\n",
            "           5     0.9085    0.9025    0.9055      1210\n",
            "\n",
            "    accuracy                         0.9030      4699\n",
            "   macro avg     0.8966    0.8990    0.8977      4699\n",
            "weighted avg     0.9031    0.9030    0.9029      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4427    0.4028    0.4218       144\n",
            "           1     0.6830    0.6245    0.6525       245\n",
            "           2     0.5099    0.5391    0.5241       384\n",
            "           3     0.5674    0.5941    0.5805       170\n",
            "           4     0.6520    0.4448    0.5288       299\n",
            "           5     0.4917    0.6194    0.5482       381\n",
            "\n",
            "    accuracy                         0.5471      1623\n",
            "   macro avg     0.5578    0.5374    0.5426      1623\n",
            "weighted avg     0.5580    0.5471    0.5468      1623\n",
            "\n",
            "0.2727227684420844 0.9029455233086183 0.5066856613480042 0.5468204338881592\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.2612234055995941\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 15 loss average: 0.234\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9076    0.8617    0.8840       376\n",
            "           1     0.9581    0.9568    0.9574       764\n",
            "           2     0.9023    0.9231    0.9126      1080\n",
            "           3     0.9040    0.9052    0.9046       749\n",
            "           4     0.9087    0.9192    0.9140       520\n",
            "           5     0.9182    0.9091    0.9136      1210\n",
            "\n",
            "    accuracy                         0.9168      4699\n",
            "   macro avg     0.9165    0.9125    0.9144      4699\n",
            "weighted avg     0.9169    0.9168    0.9167      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5111    0.3194    0.3932       144\n",
            "           1     0.7232    0.5224    0.6066       245\n",
            "           2     0.4522    0.6042    0.5173       384\n",
            "           3     0.5397    0.6000    0.5682       170\n",
            "           4     0.6524    0.5084    0.5714       299\n",
            "           5     0.5178    0.5722    0.5436       381\n",
            "\n",
            "    accuracy                         0.5410      1623\n",
            "   macro avg     0.5661    0.5211    0.5334      1623\n",
            "weighted avg     0.5598    0.5410    0.5413      1623\n",
            "\n",
            "0.23350772013266882 0.9167391591637888 0.5025505203438587 0.5412584255999296\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.26597851514816284\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.52it/s]\n",
            "Epoch 16 loss average: 0.210\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8939    0.8963    0.8951       376\n",
            "           1     0.9516    0.9516    0.9516       764\n",
            "           2     0.9187    0.9204    0.9195      1080\n",
            "           3     0.9128    0.9359    0.9242       749\n",
            "           4     0.9223    0.9135    0.9179       520\n",
            "           5     0.9296    0.9165    0.9230      1210\n",
            "\n",
            "    accuracy                         0.9242      4699\n",
            "   macro avg     0.9215    0.9224    0.9219      4699\n",
            "weighted avg     0.9243    0.9242    0.9242      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4779    0.3750    0.4202       144\n",
            "           1     0.6963    0.5429    0.6101       245\n",
            "           2     0.4819    0.5885    0.5299       384\n",
            "           3     0.5350    0.4941    0.5138       170\n",
            "           4     0.6727    0.4950    0.5703       299\n",
            "           5     0.4841    0.6010    0.5363       381\n",
            "\n",
            "    accuracy                         0.5385      1623\n",
            "   macro avg     0.5580    0.5161    0.5301      1623\n",
            "weighted avg     0.5552    0.5385    0.5395      1623\n",
            "\n",
            "0.21010899830920002 0.9242377963207549 0.5049438057251733 0.5395336898162992\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.13292017579078674\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 17 loss average: 0.186\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9254    0.8910    0.9079       376\n",
            "           1     0.9583    0.9620    0.9602       764\n",
            "           2     0.9248    0.9333    0.9290      1080\n",
            "           3     0.9286    0.9199    0.9242       749\n",
            "           4     0.9228    0.9423    0.9324       520\n",
            "           5     0.9221    0.9198    0.9210      1210\n",
            "\n",
            "    accuracy                         0.9300      4699\n",
            "   macro avg     0.9303    0.9281    0.9291      4699\n",
            "weighted avg     0.9300    0.9300    0.9299      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4722    0.3542    0.4048       144\n",
            "           1     0.5678    0.7347    0.6406       245\n",
            "           2     0.5013    0.5000    0.5007       384\n",
            "           3     0.5231    0.6000    0.5589       170\n",
            "           4     0.6310    0.5318    0.5771       299\n",
            "           5     0.5380    0.5197    0.5287       381\n",
            "\n",
            "    accuracy                         0.5434      1623\n",
            "   macro avg     0.5389    0.5401    0.5351      1623\n",
            "weighted avg     0.5436    0.5434    0.5400      1623\n",
            "\n",
            "0.18575033518330505 0.9299334697665225 0.49397855210445146 0.5400421664695368\n",
            "Patience counter: 11\n",
            "Done! It took 7.1e+02 secs\n",
            "\n",
            "Current RUN: 4\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0717289634048939\n",
            "Best test f1 weighted\n",
            "0.5618435808885377\n",
            "Best epoch\n",
            "6\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/ubuntu_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/ubuntu_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.1 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8280147314071655\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 1 loss average: 1.775\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1537    0.1217    0.1359       764\n",
            "           2     0.2714    0.2639    0.2676      1080\n",
            "           3     0.0969    0.0841    0.0901       749\n",
            "           4     0.1230    0.0577    0.0785       520\n",
            "           5     0.2544    0.4521    0.3256      1210\n",
            "\n",
            "    accuracy                         0.2166      4699\n",
            "   macro avg     0.1499    0.1632    0.1496      4699\n",
            "weighted avg     0.1819    0.2166    0.1905      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3230    0.2161    0.2590       384\n",
            "           3     0.1415    0.7824    0.2396       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.4316    0.4803    0.4547       381\n",
            "\n",
            "    accuracy                         0.2458      1623\n",
            "   macro avg     0.1493    0.2465    0.1589      1623\n",
            "weighted avg     0.1926    0.2458    0.1931      1623\n",
            "\n",
            "1.7747837156057358 0.19048340459313506 0.17637917637917636 0.1931042521527708\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.6361687183380127\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 2 loss average: 1.681\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.4014    0.0746    0.1258       764\n",
            "           2     0.2689    0.2870    0.2777      1080\n",
            "           3     0.2865    0.0681    0.1100       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.3087    0.8231    0.4491      1210\n",
            "\n",
            "    accuracy                         0.3009      4699\n",
            "   macro avg     0.2109    0.2088    0.1604      4699\n",
            "weighted avg     0.2522    0.3009    0.2174      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.2049    0.3429    0.2565       245\n",
            "           2     0.3968    0.2604    0.3145       384\n",
            "           3     1.0000    0.0235    0.0460       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3751    0.9423    0.5366       381\n",
            "\n",
            "    accuracy                         0.3370      1623\n",
            "   macro avg     0.3295    0.2615    0.1923      1623\n",
            "weighted avg     0.3176    0.3370    0.2439      1623\n",
            "\n",
            "1.6809278105696042 0.21744343790432086 0.17749076823748156 0.24390845248912504\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.7674957513809204\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 3 loss average: 1.440\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4167    0.0665    0.1147       376\n",
            "           1     0.3469    0.5458    0.4242       764\n",
            "           2     0.3216    0.2528    0.2830      1080\n",
            "           3     0.5266    0.2644    0.3520       749\n",
            "           4     0.5149    0.2327    0.3205       520\n",
            "           5     0.4325    0.7066    0.5366      1210\n",
            "\n",
            "    accuracy                         0.4020      4699\n",
            "   macro avg     0.4265    0.3448    0.3385      4699\n",
            "weighted avg     0.4159    0.4020    0.3729      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.5833    0.6286    0.6051       245\n",
            "           2     0.5163    0.2474    0.3345       384\n",
            "           3     0.6618    0.5294    0.5882       170\n",
            "           4     0.6136    0.7860    0.6891       299\n",
            "           5     0.4573    0.7874    0.5786       381\n",
            "\n",
            "    accuracy                         0.5385      1623\n",
            "   macro avg     0.4720    0.4965    0.4659      1623\n",
            "weighted avg     0.4999    0.5385    0.4949      1623\n",
            "\n",
            "1.440180964767933 0.37294397602850016 0.3863392598359813 0.49488692757166114\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.2003710269927979\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 4 loss average: 1.176\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4118    0.1489    0.2188       376\n",
            "           1     0.6164    0.6688    0.6416       764\n",
            "           2     0.4486    0.3519    0.3944      1080\n",
            "           3     0.6111    0.4112    0.4916       749\n",
            "           4     0.5033    0.7365    0.5980       520\n",
            "           5     0.5185    0.6950    0.5939      1210\n",
            "\n",
            "    accuracy                         0.5276      4699\n",
            "   macro avg     0.5183    0.5021    0.4897      4699\n",
            "weighted avg     0.5229    0.5276    0.5099      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6944    0.1736    0.2778       144\n",
            "           1     0.5888    0.7714    0.6678       245\n",
            "           2     0.4561    0.4740    0.4649       384\n",
            "           3     0.5081    0.7412    0.6029       170\n",
            "           4     0.7110    0.5184    0.5996       299\n",
            "           5     0.5312    0.5591    0.5448       381\n",
            "\n",
            "    accuracy                         0.5484      1623\n",
            "   macro avg     0.5816    0.5396    0.5263      1623\n",
            "weighted avg     0.5673    0.5484    0.5369      1623\n",
            "\n",
            "1.1762839381893475 0.5099309725056465 0.3908609804649212 0.5369440041420293\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.0041769742965698\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 5 loss average: 0.987\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5685    0.2979    0.3909       376\n",
            "           1     0.7120    0.8154    0.7602       764\n",
            "           2     0.5432    0.3722    0.4418      1080\n",
            "           3     0.6593    0.4780    0.5542       749\n",
            "           4     0.5558    0.8135    0.6604       520\n",
            "           5     0.5546    0.7256    0.6287      1210\n",
            "\n",
            "    accuracy                         0.5950      4699\n",
            "   macro avg     0.5989    0.5838    0.5727      4699\n",
            "weighted avg     0.5955    0.5950    0.5797      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5811    0.2986    0.3945       144\n",
            "           1     0.6440    0.8490    0.7324       245\n",
            "           2     0.4420    0.7344    0.5519       384\n",
            "           3     0.6687    0.6529    0.6607       170\n",
            "           4     0.7363    0.4482    0.5572       299\n",
            "           5     0.6792    0.4278    0.5250       381\n",
            "\n",
            "    accuracy                         0.5798      1623\n",
            "   macro avg     0.6252    0.5685    0.5703      1623\n",
            "weighted avg     0.6185    0.5798    0.5712      1623\n",
            "\n",
            "0.9865142280856768 0.5797273701590457 0.4478263812866609 0.5712159818203797\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 1.0098450183868408\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 6 loss average: 0.870\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6511    0.6303    0.6405       376\n",
            "           1     0.7716    0.8665    0.8163       764\n",
            "           2     0.5948    0.5343    0.5629      1080\n",
            "           3     0.7091    0.5728    0.6337       749\n",
            "           4     0.6816    0.7038    0.6925       520\n",
            "           5     0.6256    0.7058    0.6633      1210\n",
            "\n",
            "    accuracy                         0.6650      4699\n",
            "   macro avg     0.6723    0.6689    0.6682      4699\n",
            "weighted avg     0.6638    0.6650    0.6618      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6250    0.2083    0.3125       144\n",
            "           1     0.8105    0.6286    0.7080       245\n",
            "           2     0.5259    0.6615    0.5859       384\n",
            "           3     0.7124    0.6412    0.6749       170\n",
            "           4     0.6135    0.7592    0.6786       299\n",
            "           5     0.5805    0.5774    0.5789       381\n",
            "\n",
            "    accuracy                         0.6124      1623\n",
            "   macro avg     0.6446    0.5794    0.5898      1623\n",
            "weighted avg     0.6261    0.6124    0.6049      1623\n",
            "\n",
            "0.8702888737122217 0.6617943724884244 0.4763664529466558 0.6048628544466557\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.6857128143310547\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 7 loss average: 0.764\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7349    0.5824    0.6499       376\n",
            "           1     0.8051    0.8704    0.8365       764\n",
            "           2     0.6600    0.6380    0.6488      1080\n",
            "           3     0.7447    0.6075    0.6691       749\n",
            "           4     0.6966    0.7769    0.7345       520\n",
            "           5     0.6672    0.7388    0.7012      1210\n",
            "\n",
            "    accuracy                         0.7078      4699\n",
            "   macro avg     0.7181    0.7023    0.7067      4699\n",
            "weighted avg     0.7090    0.7078    0.7056      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6094    0.2708    0.3750       144\n",
            "           1     0.7760    0.5796    0.6636       245\n",
            "           2     0.4936    0.6016    0.5423       384\n",
            "           3     0.7385    0.5647    0.6400       170\n",
            "           4     0.6611    0.5284    0.5874       299\n",
            "           5     0.5121    0.7244    0.6000       381\n",
            "\n",
            "    accuracy                         0.5804      1623\n",
            "   macro avg     0.6318    0.5449    0.5680      1623\n",
            "weighted avg     0.6073    0.5804    0.5778      1623\n",
            "\n",
            "0.7635969171921412 0.7056070750295688 0.478994144915332 0.5778288745622219\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 1.0359288454055786\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 8 loss average: 0.658\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7119    0.6835    0.6974       376\n",
            "           1     0.8458    0.8901    0.8673       764\n",
            "           2     0.7198    0.6685    0.6932      1080\n",
            "           3     0.7390    0.7143    0.7264       749\n",
            "           4     0.7473    0.7904    0.7682       520\n",
            "           5     0.7263    0.7545    0.7402      1210\n",
            "\n",
            "    accuracy                         0.7487      4699\n",
            "   macro avg     0.7483    0.7502    0.7488      4699\n",
            "weighted avg     0.7474    0.7487    0.7475      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5968    0.2569    0.3592       144\n",
            "           1     0.8065    0.6122    0.6961       245\n",
            "           2     0.4885    0.6615    0.5619       384\n",
            "           3     0.5507    0.7353    0.6297       170\n",
            "           4     0.6719    0.5686    0.6159       299\n",
            "           5     0.5813    0.5722    0.5767       381\n",
            "\n",
            "    accuracy                         0.5878      1623\n",
            "   macro avg     0.6159    0.5678    0.5733      1623\n",
            "weighted avg     0.6082    0.5878    0.5847      1623\n",
            "\n",
            "0.6579999128977457 0.7475496615683436 0.5020659707839454 0.58471911939184\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.8727002739906311\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 9 loss average: 0.577\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7712    0.7261    0.7479       376\n",
            "           1     0.8842    0.8796    0.8819       764\n",
            "           2     0.7352    0.7454    0.7402      1080\n",
            "           3     0.7865    0.7477    0.7666       749\n",
            "           4     0.8205    0.8173    0.8189       520\n",
            "           5     0.7611    0.7926    0.7765      1210\n",
            "\n",
            "    accuracy                         0.7861      4699\n",
            "   macro avg     0.7931    0.7848    0.7887      4699\n",
            "weighted avg     0.7866    0.7861    0.7861      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5114    0.3125    0.3879       144\n",
            "           1     0.5497    0.8122    0.6557       245\n",
            "           2     0.5612    0.5495    0.5553       384\n",
            "           3     0.5370    0.6824    0.6010       170\n",
            "           4     0.6787    0.5017    0.5769       299\n",
            "           5     0.5972    0.5643    0.5803       381\n",
            "\n",
            "    accuracy                         0.5767      1623\n",
            "   macro avg     0.5725    0.5704    0.5595      1623\n",
            "weighted avg     0.5826    0.5767    0.5702      1623\n",
            "\n",
            "0.5773910364756981 0.786130548356659 0.4855724285140552 0.57023715892035\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.43363890051841736\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 10 loss average: 0.489\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8036    0.7074    0.7525       376\n",
            "           1     0.8890    0.9123    0.9005       764\n",
            "           2     0.7941    0.8037    0.7989      1080\n",
            "           3     0.8103    0.7984    0.8043       749\n",
            "           4     0.7935    0.8423    0.8172       520\n",
            "           5     0.8085    0.8025    0.8055      1210\n",
            "\n",
            "    accuracy                         0.8168      4699\n",
            "   macro avg     0.8165    0.8111    0.8131      4699\n",
            "weighted avg     0.8165    0.8168    0.8163      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5507    0.2639    0.3568       144\n",
            "           1     0.6653    0.6816    0.6734       245\n",
            "           2     0.4774    0.6589    0.5536       384\n",
            "           3     0.6149    0.5824    0.5982       170\n",
            "           4     0.6439    0.5987    0.6205       299\n",
            "           5     0.6287    0.5512    0.5874       381\n",
            "\n",
            "    accuracy                         0.5829      1623\n",
            "   macro avg     0.5968    0.5561    0.5650      1623\n",
            "weighted avg     0.5929    0.5829    0.5791      1623\n",
            "\n",
            "0.48890079526851576 0.8162814186343215 0.4990347364867025 0.5791480737731737\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.2658836245536804\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 11 loss average: 0.421\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8115    0.7899    0.8005       376\n",
            "           1     0.9160    0.9280    0.9220       764\n",
            "           2     0.8183    0.8380    0.8280      1080\n",
            "           3     0.8535    0.8398    0.8466       749\n",
            "           4     0.8618    0.8635    0.8626       520\n",
            "           5     0.8477    0.8372    0.8424      1210\n",
            "\n",
            "    accuracy                         0.8517      4699\n",
            "   macro avg     0.8515    0.8494    0.8504      4699\n",
            "weighted avg     0.8516    0.8517    0.8516      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6212    0.2847    0.3905       144\n",
            "           1     0.5082    0.7592    0.6088       245\n",
            "           2     0.5291    0.5443    0.5366       384\n",
            "           3     0.5899    0.6176    0.6034       170\n",
            "           4     0.6985    0.4649    0.5582       299\n",
            "           5     0.5465    0.6010    0.5725       381\n",
            "\n",
            "    accuracy                         0.5601      1623\n",
            "   macro avg     0.5822    0.5453    0.5450      1623\n",
            "weighted avg     0.5758    0.5601    0.5540      1623\n",
            "\n",
            "0.4214723367864887 0.8515843879783765 0.4692611720135003 0.5539513297483657\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.4866574704647064\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 12 loss average: 0.365\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8579    0.8191    0.8381       376\n",
            "           1     0.9190    0.9202    0.9196       764\n",
            "           2     0.8455    0.8463    0.8459      1080\n",
            "           3     0.8535    0.8865    0.8697       749\n",
            "           4     0.8773    0.9077    0.8922       520\n",
            "           5     0.8701    0.8471    0.8585      1210\n",
            "\n",
            "    accuracy                         0.8695      4699\n",
            "   macro avg     0.8706    0.8712    0.8707      4699\n",
            "weighted avg     0.8696    0.8695    0.8694      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5075    0.2361    0.3223       144\n",
            "           1     0.7572    0.5347    0.6268       245\n",
            "           2     0.4359    0.7526    0.5521       384\n",
            "           3     0.5665    0.5765    0.5714       170\n",
            "           4     0.6593    0.4983    0.5676       299\n",
            "           5     0.6106    0.5144    0.5584       381\n",
            "\n",
            "    accuracy                         0.5527      1623\n",
            "   macro avg     0.5895    0.5188    0.5331      1623\n",
            "weighted avg     0.5866    0.5527    0.5493      1623\n",
            "\n",
            "0.36459026113152504 0.8694053692499665 0.4820747252009329 0.5493369094607896\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.40155962109565735\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 13 loss average: 0.310\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8464    0.8644    0.8553       376\n",
            "           1     0.9409    0.9372    0.9390       764\n",
            "           2     0.8622    0.8750    0.8686      1080\n",
            "           3     0.8783    0.8865    0.8824       749\n",
            "           4     0.9032    0.8788    0.8908       520\n",
            "           5     0.8955    0.8851    0.8903      1210\n",
            "\n",
            "    accuracy                         0.8891      4699\n",
            "   macro avg     0.8877    0.8878    0.8877      4699\n",
            "weighted avg     0.8894    0.8891    0.8892      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5169    0.3194    0.3948       144\n",
            "           1     0.6377    0.6898    0.6627       245\n",
            "           2     0.4828    0.6198    0.5428       384\n",
            "           3     0.5121    0.6235    0.5623       170\n",
            "           4     0.6040    0.6120    0.6080       299\n",
            "           5     0.6241    0.4357    0.5131       381\n",
            "\n",
            "    accuracy                         0.5595      1623\n",
            "   macro avg     0.5629    0.5500    0.5473      1623\n",
            "weighted avg     0.5677    0.5595    0.5549      1623\n",
            "\n",
            "0.3095909534022212 0.8892143771227267 0.48991933823358697 0.5548594024325277\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.15041027963161469\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 14 loss average: 0.278\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9086    0.8723    0.8901       376\n",
            "           1     0.9344    0.9516    0.9429       764\n",
            "           2     0.8941    0.8991    0.8966      1080\n",
            "           3     0.8820    0.8985    0.8902       749\n",
            "           4     0.9120    0.9365    0.9241       520\n",
            "           5     0.9065    0.8818    0.8940      1210\n",
            "\n",
            "    accuracy                         0.9051      4699\n",
            "   macro avg     0.9063    0.9066    0.9063      4699\n",
            "weighted avg     0.9051    0.9051    0.9050      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4014    0.3958    0.3986       144\n",
            "           1     0.6590    0.7020    0.6798       245\n",
            "           2     0.4893    0.6536    0.5596       384\n",
            "           3     0.6402    0.6176    0.6287       170\n",
            "           4     0.6382    0.4247    0.5100       299\n",
            "           5     0.6134    0.5538    0.5821       381\n",
            "\n",
            "    accuracy                         0.5687      1623\n",
            "   macro avg     0.5736    0.5580    0.5598      1623\n",
            "weighted avg     0.5795    0.5687    0.5669      1623\n",
            "\n",
            "0.278326742661496 0.9049662529768572 0.4797450610066709 0.5668634551406583\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.22883160412311554\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 15 loss average: 0.234\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9122    0.9122    0.9122       376\n",
            "           1     0.9400    0.9437    0.9419       764\n",
            "           2     0.9173    0.9037    0.9104      1080\n",
            "           3     0.9026    0.9159    0.9092       749\n",
            "           4     0.9342    0.9288    0.9315       520\n",
            "           5     0.9078    0.9116    0.9097      1210\n",
            "\n",
            "    accuracy                         0.9176      4699\n",
            "   macro avg     0.9190    0.9193    0.9192      4699\n",
            "weighted avg     0.9177    0.9176    0.9176      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4595    0.2361    0.3119       144\n",
            "           1     0.6725    0.6286    0.6498       245\n",
            "           2     0.4225    0.7240    0.5336       384\n",
            "           3     0.5674    0.5941    0.5805       170\n",
            "           4     0.6396    0.4749    0.5451       299\n",
            "           5     0.6374    0.4383    0.5194       381\n",
            "\n",
            "    accuracy                         0.5397      1623\n",
            "   macro avg     0.5665    0.5160    0.5234      1623\n",
            "weighted avg     0.5691    0.5397    0.5352      1623\n",
            "\n",
            "0.23448860358136395 0.9176406023426976 0.47590780144644096 0.535172784105665\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.21508601307868958\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.55it/s]\n",
            "Epoch 16 loss average: 0.205\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9055    0.9176    0.9115       376\n",
            "           1     0.9624    0.9712    0.9668       764\n",
            "           2     0.9171    0.9111    0.9141      1080\n",
            "           3     0.9223    0.9346    0.9284       749\n",
            "           4     0.9456    0.9365    0.9411       520\n",
            "           5     0.9242    0.9165    0.9203      1210\n",
            "\n",
            "    accuracy                         0.9293      4699\n",
            "   macro avg     0.9295    0.9313    0.9304      4699\n",
            "weighted avg     0.9293    0.9293    0.9293      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4923    0.2222    0.3062       144\n",
            "           1     0.7157    0.5755    0.6380       245\n",
            "           2     0.4379    0.7344    0.5486       384\n",
            "           3     0.5778    0.6118    0.5943       170\n",
            "           4     0.6652    0.5184    0.5827       299\n",
            "           5     0.6217    0.4961    0.5518       381\n",
            "\n",
            "    accuracy                         0.5564      1623\n",
            "   macro avg     0.5851    0.5264    0.5369      1623\n",
            "weighted avg     0.5843    0.5564    0.5524      1623\n",
            "\n",
            "0.2047174397545556 0.9293145815282412 0.47904363528362537 0.5524264349891099\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.13016891479492188\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 17 loss average: 0.176\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9286    0.8989    0.9135       376\n",
            "           1     0.9646    0.9634    0.9640       764\n",
            "           2     0.9232    0.9352    0.9292      1080\n",
            "           3     0.9315    0.9266    0.9290       749\n",
            "           4     0.9411    0.9519    0.9465       520\n",
            "           5     0.9362    0.9339    0.9350      1210\n",
            "\n",
            "    accuracy                         0.9370      4699\n",
            "   macro avg     0.9375    0.9350    0.9362      4699\n",
            "weighted avg     0.9370    0.9370    0.9370      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3239    0.3958    0.3562       144\n",
            "           1     0.6157    0.7061    0.6578       245\n",
            "           2     0.5033    0.6042    0.5491       384\n",
            "           3     0.5118    0.6353    0.5669       170\n",
            "           4     0.6552    0.3813    0.4820       299\n",
            "           5     0.6031    0.5066    0.5506       381\n",
            "\n",
            "    accuracy                         0.5404      1623\n",
            "   macro avg     0.5355    0.5382    0.5271      1623\n",
            "weighted avg     0.5566    0.5404    0.5383      1623\n",
            "\n",
            "0.17584189116799584 0.9369823809660353 0.4569659782045797 0.5382737210338456\n",
            "Patience counter: 11\n",
            "Done! It took 7.1e+02 secs\n",
            "\n",
            "Current RUN: 5\n",
            "\n",
            "\n",
            "Best test loss\n",
            "0.981419313699007\n",
            "Best test f1 weighted\n",
            "0.6048628544466557\n",
            "Best epoch\n",
            "6\n",
            "\n",
            "\n",
            "Average across runs:\n",
            "Best epoch\n",
            "[7, 6, 7, 6, 6]\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0207274086773395\n",
            "Overall test f1 weighted\n",
            "[0.61206222 0.58359861 0.59268359 0.56184358 0.60486285]\n",
            "Best test f1 weighted\n",
            "0.5910101720700245\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ERC_IEMOCAP_CORNELL.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakinavohracs/EmotionDetection/blob/master/ERC_IEMOCAP_CORNELL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CAhzlpZSUe3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "b5e68af7-be49-441a-b353-d36308f18353"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "root_path = 'gdrive/My Drive/Colab Notebooks/Sakina Model/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lf1dFoIeVoJG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1ed341f8-99ed-486c-bd01-6d59445cae89"
      },
      "source": [
        "%cd /content/gdrive/My\\ Drive/Colab\\ Notebooks/Sakina\\ Model/TL-ERC/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NINDhn63WDwy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "\n",
        "DATASET_DIRECTORY_PATH = \"./datasets/\"\n",
        "GENERATIVE_WEIGHTS_DIRECTORY_PATH = \"./generative_weights/\"\n",
        "\n",
        "if not os.path.exists(DATASET_DIRECTORY_PATH):\n",
        "    os.makedirs(DATASET_DIRECTORY_PATH)\n",
        "\n",
        "\n",
        "if not os.path.exists(GENERATIVE_WEIGHTS_DIRECTORY_PATH):\n",
        "    os.makedirs(GENERATIVE_WEIGHTS_DIRECTORY_PATH)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-8_ZDpZEX4nC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "acbf1895-0749-460e-f1f6-13146d8f3106"
      },
      "source": [
        "%cd bert_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/bert_model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2R2ZXYTYAPV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "957ef5c4-0782-415f-80c7-e20b9355d05d"
      },
      "source": [
        "pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg9iv36QYG66",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "outputId": "7c0c3bbd-3c38-4965-b2ea-447661542345"
      },
      "source": [
        "pip install pytorch_pretrained_bert"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_pretrained_bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 15.6MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2019.12.20)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.6.0+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (4.41.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.14.37)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_pretrained_bert) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=0.4.1->pytorch_pretrained_bert) (0.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_pretrained_bert) (3.0.4)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.18.0,>=1.17.37 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (1.17.37)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_pretrained_bert) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.18.0,>=1.17.37->boto3->pytorch_pretrained_bert) (1.15.0)\n",
            "Installing collected packages: pytorch-pretrained-bert\n",
            "Successfully installed pytorch-pretrained-bert-0.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9ZTFX6LYWyy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ea6523a5-514e-4242-e201-c708f3af4511"
      },
      "source": [
        "!python train.py --load_checkpoint=../generative_weights/cornell_weights.pkl --data=iemocap"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 4.7e+01 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.7834867238998413\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 1 loss average: 1.783\n",
            "train\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2086    0.3972    0.2735      1080\n",
            "           3     0.0964    0.0254    0.0402       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2579    0.5182    0.3444      1210\n",
            "\n",
            "    accuracy                         0.2288      4699\n",
            "   macro avg     0.0938    0.1568    0.1097      4699\n",
            "weighted avg     0.1297    0.2288    0.1580      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3243    0.0312    0.0570       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2396    0.9974    0.3864       381\n",
            "\n",
            "    accuracy                         0.2415      1623\n",
            "   macro avg     0.0940    0.1714    0.0739      1623\n",
            "weighted avg     0.1330    0.2415    0.1042      1623\n",
            "\n",
            "1.7829623023668926 0.15795155168336011 0.10504397524954288 0.10418957731240708\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.6515558958053589\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 2 loss average: 1.697\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     1.0000    0.0013    0.0026       764\n",
            "           2     0.2783    0.3120    0.2942      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2624    0.7562    0.3896      1210\n",
            "\n",
            "    accuracy                         0.2667      4699\n",
            "   macro avg     0.2568    0.1783    0.1144      4699\n",
            "weighted avg     0.2941    0.2667    0.1684      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.6667    0.0082    0.0161       245\n",
            "           2     0.2542    0.3568    0.2969       384\n",
            "           3     1.0000    0.1059    0.1915       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3180    0.8871    0.4681       381\n",
            "\n",
            "    accuracy                         0.3050      1623\n",
            "   macro avg     0.3731    0.2263    0.1621      1623\n",
            "weighted avg     0.3402    0.3050    0.2026      1623\n",
            "\n",
            "1.697459968427817 0.16836693037862038 0.18332885947709965 0.20262549891392623\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.146328330039978\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 3 loss average: 1.402\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.4966    0.5772    0.5339       764\n",
            "           2     0.3696    0.3019    0.3323      1080\n",
            "           3     0.4399    0.2590    0.3261       749\n",
            "           4     0.4783    0.4019    0.4368       520\n",
            "           5     0.4344    0.7364    0.5465      1210\n",
            "\n",
            "    accuracy                         0.4386      4699\n",
            "   macro avg     0.3698    0.3794    0.3626      4699\n",
            "weighted avg     0.4006    0.4386    0.4042      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.7561    0.3796    0.5054       245\n",
            "           2     0.4853    0.6432    0.5532       384\n",
            "           3     0.3234    0.8941    0.4750       170\n",
            "           4     0.5854    0.7224    0.6467       299\n",
            "           5     0.4474    0.1785    0.2552       381\n",
            "\n",
            "    accuracy                         0.4781      1623\n",
            "   macro avg     0.4329    0.4696    0.4059      1623\n",
            "weighted avg     0.4757    0.4781    0.4360      1623\n",
            "\n",
            "1.4023644278446834 0.4042030092715445 0.36365979089368416 0.43597540540049784\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.4637460708618164\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.54it/s]\n",
            "Epoch 4 loss average: 1.114\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5614    0.0851    0.1478       376\n",
            "           1     0.6633    0.7762    0.7153       764\n",
            "           2     0.4805    0.4343    0.4562      1080\n",
            "           3     0.6971    0.4179    0.5225       749\n",
            "           4     0.5187    0.7731    0.6208       520\n",
            "           5     0.5271    0.6744    0.5917      1210\n",
            "\n",
            "    accuracy                         0.5586      4699\n",
            "   macro avg     0.5747    0.5268    0.5091      4699\n",
            "weighted avg     0.5675    0.5586    0.5374      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2389    0.3750    0.2919       144\n",
            "           1     0.3761    0.8980    0.5301       245\n",
            "           2     0.5935    0.3802    0.4635       384\n",
            "           3     0.6452    0.4706    0.5442       170\n",
            "           4     0.7500    0.0401    0.0762       299\n",
            "           5     0.5540    0.6194    0.5849       381\n",
            "\n",
            "    accuracy                         0.4609      1623\n",
            "   macro avg     0.5263    0.4639    0.4151      1623\n",
            "weighted avg     0.5542    0.4609    0.4239      1623\n",
            "\n",
            "1.1142314486205578 0.5373531491123953 0.3368141911911592 0.42392545333840553\n",
            "Patience counter: 1\n",
            "Epoch: 5, iter 0: loss = 0.9742234945297241\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 5 loss average: 0.959\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5753    0.2846    0.3808       376\n",
            "           1     0.6954    0.8455    0.7631       764\n",
            "           2     0.5966    0.5519    0.5734      1080\n",
            "           3     0.6753    0.5194    0.5872       749\n",
            "           4     0.6186    0.7423    0.6748       520\n",
            "           5     0.5884    0.6736    0.6281      1210\n",
            "\n",
            "    accuracy                         0.6255      4699\n",
            "   macro avg     0.6249    0.6029    0.6012      4699\n",
            "weighted avg     0.6238    0.6255    0.6163      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3158    0.0833    0.1319       144\n",
            "           1     0.7444    0.6776    0.7094       245\n",
            "           2     0.5212    0.7057    0.5996       384\n",
            "           3     0.8105    0.4529    0.5811       170\n",
            "           4     0.5871    0.7893    0.6733       299\n",
            "           5     0.6087    0.5512    0.5785       381\n",
            "\n",
            "    accuracy                         0.5989      1623\n",
            "   macro avg     0.5979    0.5433    0.5456      1623\n",
            "weighted avg     0.5996    0.5989    0.5814      1623\n",
            "\n",
            "0.9594085291028023 0.6163383119244792 0.45875556348724916 0.5813629188962001\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 1.0846093893051147\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 6 loss average: 0.820\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5921    0.4787    0.5294       376\n",
            "           1     0.8175    0.8207    0.8191       764\n",
            "           2     0.6486    0.6630    0.6557      1080\n",
            "           3     0.7603    0.5928    0.6662       749\n",
            "           4     0.6477    0.7212    0.6824       520\n",
            "           5     0.6561    0.7380    0.6947      1210\n",
            "\n",
            "    accuracy                         0.6884      4699\n",
            "   macro avg     0.6870    0.6691    0.6746      4699\n",
            "weighted avg     0.6912    0.6884    0.6868      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4667    0.2917    0.3590       144\n",
            "           1     0.7534    0.6857    0.7179       245\n",
            "           2     0.5109    0.6120    0.5569       384\n",
            "           3     0.7597    0.5765    0.6555       170\n",
            "           4     0.6793    0.5385    0.6007       299\n",
            "           5     0.5620    0.7139    0.6289       381\n",
            "\n",
            "    accuracy                         0.6014      1623\n",
            "   macro avg     0.6220    0.5697    0.5865      1623\n",
            "weighted avg     0.6126    0.6014    0.5990      1623\n",
            "\n",
            "0.8203624896705151 0.6868144708965278 0.47117668257976386 0.5989534370063377\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 0.5955107808113098\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.53it/s]\n",
            "Epoch 7 loss average: 0.713\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7575    0.5399    0.6304       376\n",
            "           1     0.8236    0.8861    0.8537       764\n",
            "           2     0.7203    0.6917    0.7057      1080\n",
            "           3     0.7647    0.6769    0.7181       749\n",
            "           4     0.6993    0.7962    0.7446       520\n",
            "           5     0.7008    0.7628    0.7305      1210\n",
            "\n",
            "    accuracy                         0.7387      4699\n",
            "   macro avg     0.7444    0.7256    0.7305      4699\n",
            "weighted avg     0.7398    0.7387    0.7364      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4891    0.3125    0.3814       144\n",
            "           1     0.7512    0.6531    0.6987       245\n",
            "           2     0.4514    0.7500    0.5636       384\n",
            "           3     0.6606    0.6412    0.6507       170\n",
            "           4     0.6494    0.5017    0.5660       299\n",
            "           5     0.6373    0.4751    0.5444       381\n",
            "\n",
            "    accuracy                         0.5749      1623\n",
            "   macro avg     0.6065    0.5556    0.5675      1623\n",
            "weighted avg     0.6020    0.5749    0.5729      1623\n",
            "\n",
            "0.7125544231384993 0.7364226473549095 0.5085895179091937 0.5728839473663527\n",
            "Patience counter: 2\n",
            "Epoch: 8, iter 0: loss = 0.8992918729782104\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 8 loss average: 0.620\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7666    0.6463    0.7013       376\n",
            "           1     0.8834    0.8730    0.8782       764\n",
            "           2     0.7457    0.7574    0.7515      1080\n",
            "           3     0.8090    0.7410    0.7735       749\n",
            "           4     0.7461    0.8192    0.7809       520\n",
            "           5     0.7447    0.7835    0.7636      1210\n",
            "\n",
            "    accuracy                         0.7783      4699\n",
            "   macro avg     0.7826    0.7701    0.7748      4699\n",
            "weighted avg     0.7796    0.7783    0.7780      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3885    0.4236    0.4053       144\n",
            "           1     0.5879    0.7918    0.6748       245\n",
            "           2     0.5282    0.6094    0.5659       384\n",
            "           3     0.5251    0.6765    0.5913       170\n",
            "           4     0.6486    0.4816    0.5528       299\n",
            "           5     0.6032    0.3990    0.4803       381\n",
            "\n",
            "    accuracy                         0.5545      1623\n",
            "   macro avg     0.5469    0.5636    0.5450      1623\n",
            "weighted avg     0.5643    0.5545    0.5482      1623\n",
            "\n",
            "0.6200949226816496 0.7779636977913562 0.4855062033455971 0.5482228616837684\n",
            "Patience counter: 3\n",
            "Epoch: 9, iter 0: loss = 0.5977683663368225\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 9 loss average: 0.508\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7781    0.7553    0.7665       376\n",
            "           1     0.8875    0.8979    0.8926       764\n",
            "           2     0.7996    0.8278    0.8135      1080\n",
            "           3     0.8319    0.7931    0.8120       749\n",
            "           4     0.8145    0.8019    0.8081       520\n",
            "           5     0.8044    0.8091    0.8068      1210\n",
            "\n",
            "    accuracy                         0.8202      4699\n",
            "   macro avg     0.8193    0.8142    0.8166      4699\n",
            "weighted avg     0.8202    0.8202    0.8200      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5068    0.2569    0.3410       144\n",
            "           1     0.6518    0.6571    0.6545       245\n",
            "           2     0.5112    0.5938    0.5494       384\n",
            "           3     0.4092    0.7294    0.5243       170\n",
            "           4     0.6423    0.5886    0.6143       299\n",
            "           5     0.5786    0.4252    0.4902       381\n",
            "\n",
            "    accuracy                         0.5471      1623\n",
            "   macro avg     0.5500    0.5418    0.5289      1623\n",
            "weighted avg     0.5613    0.5471    0.5422      1623\n",
            "\n",
            "0.5081402321035663 0.8200388536374271 0.5055319416791373 0.5421969644679505\n",
            "Patience counter: 4\n",
            "Epoch: 10, iter 0: loss = 0.47699081897735596\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 10 loss average: 0.453\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8099    0.7367    0.7716       376\n",
            "           1     0.8873    0.9175    0.9022       764\n",
            "           2     0.8415    0.8259    0.8336      1080\n",
            "           3     0.8428    0.8091    0.8256       749\n",
            "           4     0.8187    0.8596    0.8386       520\n",
            "           5     0.8164    0.8380    0.8271      1210\n",
            "\n",
            "    accuracy                         0.8378      4699\n",
            "   macro avg     0.8361    0.8311    0.8331      4699\n",
            "weighted avg     0.8377    0.8378    0.8374      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3571    0.5208    0.4237       144\n",
            "           1     0.6246    0.7265    0.6717       245\n",
            "           2     0.5326    0.6380    0.5806       384\n",
            "           3     0.5223    0.6882    0.5939       170\n",
            "           4     0.6867    0.3813    0.4903       299\n",
            "           5     0.6223    0.4541    0.5250       381\n",
            "\n",
            "    accuracy                         0.5558      1623\n",
            "   macro avg     0.5576    0.5682    0.5475      1623\n",
            "weighted avg     0.5793    0.5558    0.5521      1623\n",
            "\n",
            "0.452851637887458 0.837406570699741 0.4995348080326283 0.5521452543744407\n",
            "Patience counter: 5\n",
            "Epoch: 11, iter 0: loss = 0.3471848964691162\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 11 loss average: 0.386\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8343    0.7500    0.7899       376\n",
            "           1     0.9079    0.9293    0.9185       764\n",
            "           2     0.8677    0.8620    0.8648      1080\n",
            "           3     0.8562    0.8585    0.8573       749\n",
            "           4     0.8377    0.8635    0.8504       520\n",
            "           5     0.8556    0.8620    0.8588      1210\n",
            "\n",
            "    accuracy                         0.8636      4699\n",
            "   macro avg     0.8599    0.8542    0.8566      4699\n",
            "weighted avg     0.8633    0.8636    0.8632      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4701    0.3819    0.4215       144\n",
            "           1     0.5634    0.7796    0.6541       245\n",
            "           2     0.5130    0.6146    0.5592       384\n",
            "           3     0.5423    0.6412    0.5876       170\n",
            "           4     0.6699    0.4682    0.5512       299\n",
            "           5     0.6061    0.4724    0.5310       381\n",
            "\n",
            "    accuracy                         0.5613      1623\n",
            "   macro avg     0.5608    0.5597    0.5508      1623\n",
            "weighted avg     0.5706    0.5613    0.5562      1623\n",
            "\n",
            "0.3857889127296706 0.8632142863134714 0.48696359222142444 0.5561870224915713\n",
            "Patience counter: 6\n",
            "Epoch: 12, iter 0: loss = 0.24137914180755615\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 12 loss average: 0.330\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8443    0.8511    0.8477       376\n",
            "           1     0.9359    0.9359    0.9359       764\n",
            "           2     0.8851    0.8843    0.8847      1080\n",
            "           3     0.8817    0.8959    0.8887       749\n",
            "           4     0.8921    0.8904    0.8912       520\n",
            "           5     0.8906    0.8810    0.8857      1210\n",
            "\n",
            "    accuracy                         0.8917      4699\n",
            "   macro avg     0.8883    0.8897    0.8890      4699\n",
            "weighted avg     0.8917    0.8917    0.8917      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4565    0.2917    0.3559       144\n",
            "           1     0.7524    0.6327    0.6874       245\n",
            "           2     0.4619    0.6797    0.5501       384\n",
            "           3     0.5648    0.6412    0.6006       170\n",
            "           4     0.6403    0.5418    0.5870       299\n",
            "           5     0.5924    0.4882    0.5353       381\n",
            "\n",
            "    accuracy                         0.5638      1623\n",
            "   macro avg     0.5781    0.5459    0.5527      1623\n",
            "weighted avg     0.5796    0.5638    0.5622      1623\n",
            "\n",
            "0.3301244604711731 0.8916878816808369 0.4950866921006024 0.5621704349640122\n",
            "Patience counter: 7\n",
            "Epoch: 13, iter 0: loss = 0.26916125416755676\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 13 loss average: 0.290\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8892    0.8324    0.8599       376\n",
            "           1     0.9498    0.9411    0.9454       764\n",
            "           2     0.8875    0.8907    0.8891      1080\n",
            "           3     0.8837    0.9025    0.8930       749\n",
            "           4     0.8794    0.9115    0.8952       520\n",
            "           5     0.8977    0.8917    0.8947      1210\n",
            "\n",
            "    accuracy                         0.8987      4699\n",
            "   macro avg     0.8979    0.8950    0.8962      4699\n",
            "weighted avg     0.8989    0.8987    0.8987      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3824    0.3611    0.3714       144\n",
            "           1     0.7004    0.6776    0.6888       245\n",
            "           2     0.5135    0.5964    0.5518       384\n",
            "           3     0.5824    0.5824    0.5824       170\n",
            "           4     0.5918    0.5819    0.5868       299\n",
            "           5     0.5765    0.5144    0.5437       381\n",
            "\n",
            "    accuracy                         0.5644      1623\n",
            "   macro avg     0.5578    0.5523    0.5542      1623\n",
            "weighted avg     0.5665    0.5644    0.5642      1623\n",
            "\n",
            "0.28958908934146166 0.8986550402404607 0.491863086015616 0.5642314382851422\n",
            "Patience counter: 8\n",
            "Epoch: 14, iter 0: loss = 0.05602177977561951\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 14 loss average: 0.248\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8839    0.8910    0.8874       376\n",
            "           1     0.9425    0.9647    0.9534       764\n",
            "           2     0.9107    0.9065    0.9086      1080\n",
            "           3     0.9108    0.8999    0.9053       749\n",
            "           4     0.9065    0.9135    0.9100       520\n",
            "           5     0.9191    0.9107    0.9149      1210\n",
            "\n",
            "    accuracy                         0.9155      4699\n",
            "   macro avg     0.9122    0.9144    0.9133      4699\n",
            "weighted avg     0.9154    0.9155    0.9154      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5063    0.2778    0.3587       144\n",
            "           1     0.7553    0.5796    0.6559       245\n",
            "           2     0.4377    0.6953    0.5372       384\n",
            "           3     0.5285    0.6000    0.5620       170\n",
            "           4     0.6891    0.4448    0.5407       299\n",
            "           5     0.5500    0.5197    0.5344       381\n",
            "\n",
            "    accuracy                         0.5434      1623\n",
            "   macro avg     0.5778    0.5195    0.5315      1623\n",
            "weighted avg     0.5739    0.5434    0.5419      1623\n",
            "\n",
            "0.24802372829678157 0.9154384756874749 0.4663554921661401 0.5418661698749013\n",
            "Patience counter: 9\n",
            "Epoch: 15, iter 0: loss = 0.15604658424854279\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 15 loss average: 0.230\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9091    0.9043    0.9067       376\n",
            "           1     0.9592    0.9529    0.9560       764\n",
            "           2     0.9128    0.9204    0.9166      1080\n",
            "           3     0.9008    0.9092    0.9050       749\n",
            "           4     0.9359    0.9269    0.9314       520\n",
            "           5     0.9138    0.9107    0.9123      1210\n",
            "\n",
            "    accuracy                         0.9208      4699\n",
            "   macro avg     0.9219    0.9207    0.9213      4699\n",
            "weighted avg     0.9209    0.9208    0.9209      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4138    0.2500    0.3117       144\n",
            "           1     0.6070    0.7061    0.6528       245\n",
            "           2     0.5136    0.5911    0.5496       384\n",
            "           3     0.5362    0.6529    0.5889       170\n",
            "           4     0.6314    0.5786    0.6038       299\n",
            "           5     0.5671    0.4882    0.5247       381\n",
            "\n",
            "    accuracy                         0.5582      1623\n",
            "   macro avg     0.5448    0.5445    0.5386      1623\n",
            "weighted avg     0.5555    0.5582    0.5523      1623\n",
            "\n",
            "0.2302107447758317 0.9208677950421541 0.4977014035735533 0.5523384041159085\n",
            "Patience counter: 10\n",
            "Epoch: 16, iter 0: loss = 0.3259222209453583\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 16 loss average: 0.215\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9218    0.8777    0.8992       376\n",
            "           1     0.9680    0.9516    0.9597       764\n",
            "           2     0.9139    0.9241    0.9190      1080\n",
            "           3     0.9077    0.9319    0.9196       749\n",
            "           4     0.9076    0.9442    0.9255       520\n",
            "           5     0.9310    0.9140    0.9224      1210\n",
            "\n",
            "    accuracy                         0.9257      4699\n",
            "   macro avg     0.9250    0.9239    0.9242      4699\n",
            "weighted avg     0.9260    0.9257    0.9257      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3469    0.3542    0.3505       144\n",
            "           1     0.6493    0.7102    0.6784       245\n",
            "           2     0.5276    0.5234    0.5255       384\n",
            "           3     0.5025    0.6000    0.5469       170\n",
            "           4     0.6314    0.4983    0.5570       299\n",
            "           5     0.5258    0.5354    0.5306       381\n",
            "\n",
            "    accuracy                         0.5428      1623\n",
            "   macro avg     0.5306    0.5369    0.5315      1623\n",
            "weighted avg     0.5460    0.5428    0.5423      1623\n",
            "\n",
            "0.21456827312552681 0.925739391869422 0.475043005634683 0.5422834264489638\n",
            "Patience counter: 11\n",
            "Done! It took 6.6e+02 secs\n",
            "\n",
            "Current RUN: 1\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0204411335289478\n",
            "Best test f1 weighted\n",
            "0.5813629188962001\n",
            "Best epoch\n",
            "5\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.2 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8189870119094849\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 1 loss average: 1.784\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.1282    0.0399    0.0609       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2868    0.2417    0.2623      1080\n",
            "           3     0.1031    0.0614    0.0770       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2643    0.7000    0.3837      1210\n",
            "\n",
            "    accuracy                         0.2488      4699\n",
            "   macro avg     0.1304    0.1738    0.1306      4699\n",
            "weighted avg     0.1607    0.2488    0.1762      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.0000    0.0000    0.0000       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2349    1.0000    0.3804       381\n",
            "\n",
            "    accuracy                         0.2348      1623\n",
            "   macro avg     0.0391    0.1667    0.0634      1623\n",
            "weighted avg     0.0551    0.2348    0.0893      1623\n",
            "\n",
            "1.7835967366894085 0.17623061841885662 0.08759296982329812 0.08930596711217831\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.9026135206222534\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 2 loss average: 1.704\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.3009    0.4231    0.3517      1080\n",
            "           3     0.3045    0.1883    0.2327       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2823    0.6339    0.3906      1210\n",
            "\n",
            "    accuracy                         0.2905      4699\n",
            "   macro avg     0.1479    0.2075    0.1625      4699\n",
            "weighted avg     0.1904    0.2905    0.2185      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2908    0.8255    0.4301       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.4278    0.5984    0.4989       381\n",
            "\n",
            "    accuracy                         0.3358      1623\n",
            "   macro avg     0.1198    0.2373    0.1548      1623\n",
            "weighted avg     0.1692    0.3358    0.2189      1623\n",
            "\n",
            "1.7043562158942223 0.2185020131875355 0.1785897525146843 0.2188848082522283\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.5291748046875\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 3 loss average: 1.494\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0667    0.0027    0.0051       376\n",
            "           1     0.3773    0.1872    0.2502       764\n",
            "           2     0.3502    0.4352    0.3881      1080\n",
            "           3     0.2828    0.0374    0.0660       749\n",
            "           4     0.4401    0.4308    0.4354       520\n",
            "           5     0.3817    0.7430    0.5043      1210\n",
            "\n",
            "    accuracy                         0.3756      4699\n",
            "   macro avg     0.3165    0.3060    0.2749      4699\n",
            "weighted avg     0.3393    0.3756    0.3189      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4048    0.3542    0.3778       144\n",
            "           1     0.7103    0.3102    0.4318       245\n",
            "           2     0.4193    0.6562    0.5117       384\n",
            "           3     0.5000    0.0059    0.0116       170\n",
            "           4     0.6581    0.3411    0.4493       299\n",
            "           5     0.4288    0.7113    0.5350       381\n",
            "\n",
            "    accuracy                         0.4640      1623\n",
            "   macro avg     0.5202    0.3965    0.3862      1623\n",
            "weighted avg     0.5166    0.4640    0.4294      1623\n",
            "\n",
            "1.4943887044986088 0.3188690764418792 0.3417113751060665 0.4293652466699171\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.2621400356292725\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 4 loss average: 1.211\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4894    0.1223    0.1957       376\n",
            "           1     0.6586    0.5681    0.6100       764\n",
            "           2     0.4344    0.4657    0.4495      1080\n",
            "           3     0.7052    0.1629    0.2646       749\n",
            "           4     0.5211    0.6654    0.5845       520\n",
            "           5     0.4541    0.7322    0.5606      1210\n",
            "\n",
            "    accuracy                         0.4973      4699\n",
            "   macro avg     0.5438    0.4528    0.4442      4699\n",
            "weighted avg     0.5331    0.4973    0.4694      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2581    0.1111    0.1553       144\n",
            "           1     0.8559    0.3878    0.5337       245\n",
            "           2     0.4449    0.7786    0.5663       384\n",
            "           3     0.5744    0.6588    0.6137       170\n",
            "           4     0.5750    0.6923    0.6282       299\n",
            "           5     0.5919    0.3465    0.4371       381\n",
            "\n",
            "    accuracy                         0.5305      1623\n",
            "   macro avg     0.5500    0.4958    0.4891      1623\n",
            "weighted avg     0.5624    0.5305    0.5110      1623\n",
            "\n",
            "1.2111014872789383 0.46936225469553755 0.43549162859101453 0.510954789362342\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.0846656560897827\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 5 loss average: 0.986\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5702    0.5186    0.5432       376\n",
            "           1     0.7375    0.7723    0.7545       764\n",
            "           2     0.5755    0.5296    0.5516      1080\n",
            "           3     0.6934    0.3925    0.5013       749\n",
            "           4     0.6569    0.6885    0.6723       520\n",
            "           5     0.5439    0.7165    0.6184      1210\n",
            "\n",
            "    accuracy                         0.6120      4699\n",
            "   macro avg     0.6296    0.6030    0.6069      4699\n",
            "weighted avg     0.6211    0.6120    0.6064      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.0208    0.0408       144\n",
            "           1     0.7713    0.5918    0.6697       245\n",
            "           2     0.4217    0.7500    0.5398       384\n",
            "           3     0.8173    0.5000    0.6204       170\n",
            "           4     0.6420    0.5217    0.5756       299\n",
            "           5     0.5672    0.5984    0.5824       381\n",
            "\n",
            "    accuracy                         0.5576      1623\n",
            "   macro avg     0.7032    0.4971    0.5048      1623\n",
            "weighted avg     0.6419    0.5576    0.5402      1623\n",
            "\n",
            "0.9862564926346143 0.6064466153551552 0.44784368952688053 0.5401960088023076\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.8553125858306885\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 6 loss average: 0.862\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5880    0.4441    0.5061       376\n",
            "           1     0.8117    0.8351    0.8232       764\n",
            "           2     0.6290    0.6074    0.6180      1080\n",
            "           3     0.7523    0.5354    0.6256       749\n",
            "           4     0.5908    0.7192    0.6487       520\n",
            "           5     0.6204    0.7281    0.6700      1210\n",
            "\n",
            "    accuracy                         0.6633      4699\n",
            "   macro avg     0.6654    0.6449    0.6486      4699\n",
            "weighted avg     0.6686    0.6633    0.6604      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5283    0.3889    0.4480       144\n",
            "           1     0.5543    0.7918    0.6521       245\n",
            "           2     0.5988    0.5130    0.5526       384\n",
            "           3     0.5429    0.6706    0.6000       170\n",
            "           4     0.6618    0.4582    0.5415       299\n",
            "           5     0.5416    0.5984    0.5686       381\n",
            "\n",
            "    accuracy                         0.5705      1623\n",
            "   macro avg     0.5713    0.5702    0.5605      1623\n",
            "weighted avg     0.5781    0.5705    0.5650      1623\n",
            "\n",
            "0.8618705924600363 0.6603999597161421 0.47230395893247207 0.5650095989866357\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.6659131646156311\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 7 loss average: 0.756\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6873    0.6489    0.6676       376\n",
            "           1     0.8284    0.8469    0.8375       764\n",
            "           2     0.6812    0.6491    0.6648      1080\n",
            "           3     0.7538    0.6622    0.7050       749\n",
            "           4     0.7029    0.7462    0.7239       520\n",
            "           5     0.6654    0.7281    0.6953      1210\n",
            "\n",
            "    accuracy                         0.7144      4699\n",
            "   macro avg     0.7198    0.7136    0.7157      4699\n",
            "weighted avg     0.7155    0.7144    0.7139      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4950    0.3472    0.4082       144\n",
            "           1     0.6180    0.8122    0.7019       245\n",
            "           2     0.6224    0.3906    0.4800       384\n",
            "           3     0.5399    0.6765    0.6005       170\n",
            "           4     0.5291    0.7592    0.6236       299\n",
            "           5     0.5836    0.4856    0.5301       381\n",
            "\n",
            "    accuracy                         0.5705      1623\n",
            "   macro avg     0.5647    0.5786    0.5574      1623\n",
            "weighted avg     0.5755    0.5705    0.5580      1623\n",
            "\n",
            "0.7555355678002039 0.7139189317953616 0.42548487158210946 0.5579708121519582\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.8386040925979614\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 8 loss average: 0.661\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7308    0.6064    0.6628       376\n",
            "           1     0.8570    0.8783    0.8675       764\n",
            "           2     0.7312    0.7278    0.7295      1080\n",
            "           3     0.7622    0.7063    0.7332       749\n",
            "           4     0.7451    0.8038    0.7734       520\n",
            "           5     0.7174    0.7554    0.7359      1210\n",
            "\n",
            "    accuracy                         0.7546      4699\n",
            "   macro avg     0.7573    0.7463    0.7504      4699\n",
            "weighted avg     0.7545    0.7546    0.7537      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5194    0.4653    0.4908       144\n",
            "           1     0.6515    0.7020    0.6758       245\n",
            "           2     0.4961    0.6667    0.5689       384\n",
            "           3     0.6522    0.6176    0.6344       170\n",
            "           4     0.6682    0.4716    0.5529       299\n",
            "           5     0.6023    0.5407    0.5698       381\n",
            "\n",
            "    accuracy                         0.5835      1623\n",
            "   macro avg     0.5983    0.5773    0.5821      1623\n",
            "weighted avg     0.5946    0.5835    0.5823      1623\n",
            "\n",
            "0.6606211289763451 0.7536820085513322 0.5161506411762686 0.5822616448851504\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.5653786063194275\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 9 loss average: 0.556\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7690    0.6729    0.7177       376\n",
            "           1     0.8674    0.8822    0.8748       764\n",
            "           2     0.7931    0.7917    0.7924      1080\n",
            "           3     0.8032    0.7463    0.7737       749\n",
            "           4     0.7828    0.8596    0.8194       520\n",
            "           5     0.7676    0.7917    0.7795      1210\n",
            "\n",
            "    accuracy                         0.7972      4699\n",
            "   macro avg     0.7972    0.7907    0.7929      4699\n",
            "weighted avg     0.7972    0.7972    0.7965      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5254    0.4306    0.4733       144\n",
            "           1     0.6357    0.7551    0.6903       245\n",
            "           2     0.5019    0.6771    0.5765       384\n",
            "           3     0.5989    0.6412    0.6193       170\n",
            "           4     0.6782    0.4582    0.5469       299\n",
            "           5     0.6122    0.5013    0.5512       381\n",
            "\n",
            "    accuracy                         0.5816      1623\n",
            "   macro avg     0.5921    0.5772    0.5763      1623\n",
            "weighted avg     0.5927    0.5816    0.5776      1623\n",
            "\n",
            "0.5557425860315561 0.7965035835944221 0.5082056688019695 0.577619769530763\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.3496533930301666\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 10 loss average: 0.484\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7464    0.6809    0.7121       376\n",
            "           1     0.8927    0.9254    0.9087       764\n",
            "           2     0.8224    0.8278    0.8251      1080\n",
            "           3     0.8430    0.8318    0.8374       749\n",
            "           4     0.7747    0.8135    0.7936       520\n",
            "           5     0.8314    0.8190    0.8251      1210\n",
            "\n",
            "    accuracy                         0.8287      4699\n",
            "   macro avg     0.8184    0.8164    0.8170      4699\n",
            "weighted avg     0.8281    0.8287    0.8281      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4865    0.5000    0.4932       144\n",
            "           1     0.5449    0.7429    0.6287       245\n",
            "           2     0.5000    0.5807    0.5373       384\n",
            "           3     0.5805    0.5941    0.5872       170\n",
            "           4     0.7165    0.3043    0.4272       299\n",
            "           5     0.5609    0.5801    0.5703       381\n",
            "\n",
            "    accuracy                         0.5484      1623\n",
            "   macro avg     0.5649    0.5504    0.5407      1623\n",
            "weighted avg     0.5682    0.5484    0.5399      1623\n",
            "\n",
            "0.48380319743106764 0.8281411981366422 0.4792147396935292 0.5398892887144697\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.3748909831047058\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 11 loss average: 0.414\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8222    0.7872    0.8043       376\n",
            "           1     0.8885    0.9175    0.9028       764\n",
            "           2     0.8562    0.8380    0.8470      1080\n",
            "           3     0.8530    0.8598    0.8564       749\n",
            "           4     0.8482    0.8596    0.8539       520\n",
            "           5     0.8530    0.8537    0.8534      1210\n",
            "\n",
            "    accuracy                         0.8568      4699\n",
            "   macro avg     0.8535    0.8526    0.8530      4699\n",
            "weighted avg     0.8565    0.8568    0.8565      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4444    0.5278    0.4825       144\n",
            "           1     0.5668    0.7102    0.6304       245\n",
            "           2     0.5053    0.6172    0.5557       384\n",
            "           3     0.5930    0.6000    0.5965       170\n",
            "           4     0.6341    0.3478    0.4492       299\n",
            "           5     0.5971    0.5328    0.5631       381\n",
            "\n",
            "    accuracy                         0.5521      1623\n",
            "   macro avg     0.5568    0.5560    0.5463      1623\n",
            "weighted avg     0.5637    0.5521    0.5469      1623\n",
            "\n",
            "0.4140258068218827 0.8565452418592752 0.501389037917223 0.5468864824839508\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.3400386869907379\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 12 loss average: 0.367\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7925    0.7819    0.7871       376\n",
            "           1     0.9113    0.9280    0.9196       764\n",
            "           2     0.8705    0.8528    0.8616      1080\n",
            "           3     0.8770    0.8665    0.8717       749\n",
            "           4     0.8488    0.8635    0.8561       520\n",
            "           5     0.8553    0.8645    0.8598      1210\n",
            "\n",
            "    accuracy                         0.8657      4699\n",
            "   macro avg     0.8592    0.8595    0.8593      4699\n",
            "weighted avg     0.8656    0.8657    0.8656      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6176    0.2917    0.3962       144\n",
            "           1     0.6391    0.6939    0.6654       245\n",
            "           2     0.4610    0.6458    0.5380       384\n",
            "           3     0.5297    0.6294    0.5753       170\n",
            "           4     0.6445    0.4548    0.5333       299\n",
            "           5     0.5651    0.5013    0.5313       381\n",
            "\n",
            "    accuracy                         0.5508      1623\n",
            "   macro avg     0.5762    0.5362    0.5399      1623\n",
            "weighted avg     0.5672    0.5508    0.5461      1623\n",
            "\n",
            "0.36748461006209254 0.8656074484198014 0.4936170486878627 0.546107507061178\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.23800702393054962\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 13 loss average: 0.319\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8277    0.7793    0.8027       376\n",
            "           1     0.9259    0.9490    0.9373       764\n",
            "           2     0.9015    0.8648    0.8828      1080\n",
            "           3     0.8768    0.9025    0.8895       749\n",
            "           4     0.8448    0.9000    0.8715       520\n",
            "           5     0.8926    0.8860    0.8893      1210\n",
            "\n",
            "    accuracy                         0.8870      4699\n",
            "   macro avg     0.8782    0.8803    0.8788      4699\n",
            "weighted avg     0.8871    0.8870    0.8867      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5043    0.4097    0.4521       144\n",
            "           1     0.6276    0.6122    0.6198       245\n",
            "           2     0.4732    0.6198    0.5366       384\n",
            "           3     0.5926    0.5647    0.5783       170\n",
            "           4     0.6411    0.4482    0.5276       299\n",
            "           5     0.5471    0.5643    0.5556       381\n",
            "\n",
            "    accuracy                         0.5496      1623\n",
            "   macro avg     0.5643    0.5365    0.5450      1623\n",
            "weighted avg     0.5600    0.5496    0.5488      1623\n",
            "\n",
            "0.31891668572401005 0.8867310411142536 0.49732745160553327 0.5488311325332773\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.32753050327301025\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 14 loss average: 0.283\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8516    0.8697    0.8605       376\n",
            "           1     0.9347    0.9372    0.9359       764\n",
            "           2     0.9045    0.8861    0.8952      1080\n",
            "           3     0.8966    0.9146    0.9055       749\n",
            "           4     0.9010    0.8923    0.8966       520\n",
            "           5     0.8993    0.9008    0.9001      1210\n",
            "\n",
            "    accuracy                         0.9021      4699\n",
            "   macro avg     0.8980    0.9001    0.8990      4699\n",
            "weighted avg     0.9022    0.9021    0.9021      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4368    0.5278    0.4780       144\n",
            "           1     0.6549    0.6816    0.6680       245\n",
            "           2     0.4914    0.5964    0.5388       384\n",
            "           3     0.5828    0.5588    0.5706       170\n",
            "           4     0.6543    0.4114    0.5051       299\n",
            "           5     0.5517    0.5459    0.5488       381\n",
            "\n",
            "    accuracy                         0.5533      1623\n",
            "   macro avg     0.5620    0.5536    0.5516      1623\n",
            "weighted avg     0.5650    0.5533    0.5524      1623\n",
            "\n",
            "0.283062391448766 0.9021110423421592 0.4834208195947987 0.5523893739357432\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.1024448573589325\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 15 loss average: 0.250\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8919    0.8777    0.8847       376\n",
            "           1     0.9455    0.9542    0.9498       764\n",
            "           2     0.9079    0.9037    0.9058      1080\n",
            "           3     0.9215    0.9092    0.9153       749\n",
            "           4     0.8991    0.9250    0.9118       520\n",
            "           5     0.9148    0.9140    0.9144      1210\n",
            "\n",
            "    accuracy                         0.9157      4699\n",
            "   macro avg     0.9135    0.9140    0.9137      4699\n",
            "weighted avg     0.9157    0.9157    0.9157      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5135    0.3958    0.4471       144\n",
            "           1     0.5876    0.6571    0.6204       245\n",
            "           2     0.4425    0.6016    0.5099       384\n",
            "           3     0.6319    0.5353    0.5796       170\n",
            "           4     0.6757    0.4181    0.5165       299\n",
            "           5     0.5556    0.5643    0.5599       381\n",
            "\n",
            "    accuracy                         0.5422      1623\n",
            "   macro avg     0.5678    0.5287    0.5389      1623\n",
            "weighted avg     0.5600    0.5422    0.5413      1623\n",
            "\n",
            "0.24959659079710642 0.9156818877968311 0.4678807644981909 0.5412768865044182\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.12030025571584702\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 16 loss average: 0.215\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9069    0.9069    0.9069       376\n",
            "           1     0.9464    0.9712    0.9587       764\n",
            "           2     0.9244    0.9056    0.9149      1080\n",
            "           3     0.9346    0.9159    0.9252       749\n",
            "           4     0.9308    0.9308    0.9308       520\n",
            "           5     0.9144    0.9273    0.9208      1210\n",
            "\n",
            "    accuracy                         0.9264      4699\n",
            "   macro avg     0.9263    0.9263    0.9262      4699\n",
            "weighted avg     0.9263    0.9264    0.9263      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4714    0.4583    0.4648       144\n",
            "           1     0.5556    0.7755    0.6474       245\n",
            "           2     0.5417    0.5078    0.5242       384\n",
            "           3     0.5550    0.6235    0.5873       170\n",
            "           4     0.6400    0.4816    0.5496       299\n",
            "           5     0.5616    0.5381    0.5496       381\n",
            "\n",
            "    accuracy                         0.5582      1623\n",
            "   macro avg     0.5542    0.5641    0.5538      1623\n",
            "weighted avg     0.5617    0.5582    0.5548      1623\n",
            "\n",
            "0.21460060969305536 0.926279814676701 0.47145999643947284 0.5547685903901577\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.10364850610494614\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 17 loss average: 0.201\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9011    0.8963    0.8987       376\n",
            "           1     0.9571    0.9634    0.9602       764\n",
            "           2     0.9293    0.9130    0.9211      1080\n",
            "           3     0.9337    0.9399    0.9368       749\n",
            "           4     0.9358    0.9250    0.9304       520\n",
            "           5     0.9234    0.9364    0.9298      1210\n",
            "\n",
            "    accuracy                         0.9315      4699\n",
            "   macro avg     0.9301    0.9290    0.9295      4699\n",
            "weighted avg     0.9315    0.9315    0.9314      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4661    0.3819    0.4198       144\n",
            "           1     0.5791    0.7020    0.6347       245\n",
            "           2     0.4510    0.6589    0.5354       384\n",
            "           3     0.6026    0.5529    0.5767       170\n",
            "           4     0.6721    0.4114    0.5104       299\n",
            "           5     0.5974    0.4829    0.5341       381\n",
            "\n",
            "    accuracy                         0.5428      1623\n",
            "   macro avg     0.5614    0.5317    0.5352      1623\n",
            "weighted avg     0.5627    0.5428    0.5396      1623\n",
            "\n",
            "0.200962338441362 0.9314309026244155 0.47657274459564736 0.539557768578298\n",
            "Patience counter: 11\n",
            "Done! It took 7e+02 secs\n",
            "\n",
            "Current RUN: 2\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0564241372048855\n",
            "Best test f1 weighted\n",
            "0.5650095989866357\n",
            "Best epoch\n",
            "6\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.1 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.7833635807037354\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 1 loss average: 1.773\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.5000    0.0118    0.0230       764\n",
            "           2     0.3220    0.0528    0.0907      1080\n",
            "           3     0.1353    0.0240    0.0408       749\n",
            "           4     0.1807    0.0288    0.0498       520\n",
            "           5     0.2565    0.9091    0.4001      1210\n",
            "\n",
            "    accuracy                         0.2552      4699\n",
            "   macro avg     0.2324    0.1711    0.1007      4699\n",
            "weighted avg     0.2629    0.2552    0.1396      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3072    0.4688    0.3711       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.3028    0.8241    0.4429       381\n",
            "\n",
            "    accuracy                         0.3044      1623\n",
            "   macro avg     0.1017    0.2155    0.1357      1623\n",
            "weighted avg     0.1438    0.3044    0.1918      1623\n",
            "\n",
            "1.773218830426534 0.13963637181180089 0.15418607799283476 0.19177554661397367\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 2.106132984161377\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 2 loss average: 1.702\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1818    0.0026    0.0052       764\n",
            "           2     0.2727    0.4000    0.3243      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2790    0.7157    0.4015      1210\n",
            "\n",
            "    accuracy                         0.2767      4699\n",
            "   macro avg     0.1223    0.1864    0.1218      4699\n",
            "weighted avg     0.1641    0.2767    0.1788      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0769    0.0041    0.0078       245\n",
            "           2     0.2663    0.9167    0.4127       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.5104    0.3858    0.4395       381\n",
            "\n",
            "    accuracy                         0.3081      1623\n",
            "   macro avg     0.1423    0.2178    0.1433      1623\n",
            "weighted avg     0.1944    0.3081    0.2020      1623\n",
            "\n",
            "1.7015588507056236 0.17876326491678896 0.1672206621325749 0.20196925543071087\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.627952218055725\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 3 loss average: 1.500\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.3605    0.0694    0.1164       764\n",
            "           2     0.3546    0.4741    0.4057      1080\n",
            "           3     0.3532    0.1028    0.1593       749\n",
            "           4     0.6135    0.4365    0.5101       520\n",
            "           5     0.3623    0.7545    0.4895      1210\n",
            "\n",
            "    accuracy                         0.3792      4699\n",
            "   macro avg     0.3407    0.3062    0.2802      4699\n",
            "weighted avg     0.3576    0.3792    0.3201      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6667    0.0139    0.0272       144\n",
            "           1     0.8529    0.2367    0.3706       245\n",
            "           2     0.5435    0.6016    0.5711       384\n",
            "           3     0.4380    0.6647    0.5280       170\n",
            "           4     0.4859    0.8629    0.6217       299\n",
            "           5     0.5651    0.5013    0.5313       381\n",
            "\n",
            "    accuracy                         0.5256      1623\n",
            "   macro avg     0.5920    0.4802    0.4417      1623\n",
            "weighted avg     0.5845    0.5256    0.4880      1623\n",
            "\n",
            "1.4999912306666374 0.3200567431465597 0.385992289927607 0.48803668860342325\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.0809344053268433\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 4 loss average: 1.222\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.1968    0.2475       376\n",
            "           1     0.6487    0.5052    0.5681       764\n",
            "           2     0.5032    0.5046    0.5039      1080\n",
            "           3     0.6046    0.3164    0.4154       749\n",
            "           4     0.4900    0.6615    0.5630       520\n",
            "           5     0.5073    0.7149    0.5935      1210\n",
            "\n",
            "    accuracy                         0.5216      4699\n",
            "   macro avg     0.5145    0.4833    0.4819      4699\n",
            "weighted avg     0.5290    0.5216    0.5093      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.7637    0.5673    0.6511       245\n",
            "           2     0.5672    0.4948    0.5285       384\n",
            "           3     0.7295    0.5235    0.6096       170\n",
            "           4     0.5727    0.8428    0.6820       299\n",
            "           5     0.5147    0.7349    0.6054       381\n",
            "\n",
            "    accuracy                         0.5853      1623\n",
            "   macro avg     0.5246    0.5272    0.5128      1623\n",
            "weighted avg     0.5522    0.5853    0.5549      1623\n",
            "\n",
            "1.2223592239121597 0.5093285746954455 0.4286789428485985 0.5549384732021458\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 0.9823277592658997\n",
            "100%|███████████████████████████████████████████| 48/48 [00:31<00:00,  1.53it/s]\n",
            "Epoch 5 loss average: 1.042\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6395    0.1463    0.2381       376\n",
            "           1     0.7518    0.6859    0.7173       764\n",
            "           2     0.5645    0.5352    0.5494      1080\n",
            "           3     0.7014    0.4139    0.5206       749\n",
            "           4     0.5547    0.8385    0.6677       520\n",
            "           5     0.5571    0.7661    0.6451      1210\n",
            "\n",
            "    accuracy                         0.6023      4699\n",
            "   macro avg     0.6282    0.5643    0.5564      4699\n",
            "weighted avg     0.6198    0.6023    0.5849      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6667    0.1528    0.2486       144\n",
            "           1     0.7716    0.6204    0.6878       245\n",
            "           2     0.4607    0.5651    0.5076       384\n",
            "           3     0.6714    0.5529    0.6065       170\n",
            "           4     0.6797    0.5819    0.6270       299\n",
            "           5     0.5418    0.7480    0.6284       381\n",
            "\n",
            "    accuracy                         0.5816      1623\n",
            "   macro avg     0.6320    0.5369    0.5510      1623\n",
            "weighted avg     0.6074    0.5816    0.5725      1623\n",
            "\n",
            "1.0424456782639027 0.5849344160524862 0.44855614980638203 0.572543596885718\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.8469823598861694\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 6 loss average: 0.887\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7744    0.3378    0.4704       376\n",
            "           1     0.7599    0.8285    0.7927       764\n",
            "           2     0.6083    0.6213    0.6148      1080\n",
            "           3     0.7535    0.5100    0.6083       749\n",
            "           4     0.5959    0.7769    0.6745       520\n",
            "           5     0.6231    0.7281    0.6715      1210\n",
            "\n",
            "    accuracy                         0.6593      4699\n",
            "   macro avg     0.6858    0.6338    0.6387      4699\n",
            "weighted avg     0.6718    0.6593    0.6523      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2876    0.7708    0.4189       144\n",
            "           1     0.7630    0.6571    0.7061       245\n",
            "           2     0.4799    0.7474    0.5845       384\n",
            "           3     0.7734    0.5824    0.6644       170\n",
            "           4     0.7667    0.1538    0.2563       299\n",
            "           5     0.6750    0.4252    0.5217       381\n",
            "\n",
            "    accuracy                         0.5336      1623\n",
            "   macro avg     0.6243    0.5561    0.5253      1623\n",
            "weighted avg     0.6350    0.5336    0.5213      1623\n",
            "\n",
            "0.8866725073506435 0.6523233924362775 0.42186052350510006 0.5213414437284352\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 1.0319797992706299\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 7 loss average: 0.780\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5471    0.4947    0.5196       376\n",
            "           1     0.8089    0.8534    0.8306       764\n",
            "           2     0.6625    0.6907    0.6763      1080\n",
            "           3     0.7492    0.6262    0.6822       749\n",
            "           4     0.6826    0.6327    0.6567       520\n",
            "           5     0.6793    0.7405    0.7086      1210\n",
            "\n",
            "    accuracy                         0.6976      4699\n",
            "   macro avg     0.6883    0.6730    0.6790      4699\n",
            "weighted avg     0.6974    0.6976    0.6959      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4203    0.4028    0.4113       144\n",
            "           1     0.6835    0.6082    0.6436       245\n",
            "           2     0.5390    0.5573    0.5480       384\n",
            "           3     0.7712    0.5353    0.6319       170\n",
            "           4     0.7181    0.4515    0.5544       299\n",
            "           5     0.5177    0.7664    0.6180       381\n",
            "\n",
            "    accuracy                         0.5786      1623\n",
            "   macro avg     0.6083    0.5536    0.5679      1623\n",
            "weighted avg     0.6026    0.5786    0.5767      1623\n",
            "\n",
            "0.7798385440061489 0.6959284427243567 0.49414504314541485 0.5767193295784637\n",
            "Patience counter: 0\n",
            "Epoch: 8, iter 0: loss = 0.3627293109893799\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 8 loss average: 0.682\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6097    0.6356    0.6224       376\n",
            "           1     0.8629    0.8730    0.8679       764\n",
            "           2     0.7424    0.7231    0.7326      1080\n",
            "           3     0.7715    0.6582    0.7104       749\n",
            "           4     0.7329    0.7231    0.7280       520\n",
            "           5     0.7150    0.7860    0.7488      1210\n",
            "\n",
            "    accuracy                         0.7463      4699\n",
            "   macro avg     0.7391    0.7332    0.7350      4699\n",
            "weighted avg     0.7479    0.7463    0.7459      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7647    0.1806    0.2921       144\n",
            "           1     0.6234    0.7837    0.6944       245\n",
            "           2     0.5355    0.5885    0.5608       384\n",
            "           3     0.6207    0.6353    0.6279       170\n",
            "           4     0.6486    0.6789    0.6634       299\n",
            "           5     0.6156    0.6010    0.6082       381\n",
            "\n",
            "    accuracy                         0.6063      1623\n",
            "   macro avg     0.6347    0.5780    0.5745      1623\n",
            "weighted avg     0.6177    0.6063    0.5942      1623\n",
            "\n",
            "0.6815853404502074 0.7459164925762402 0.46868526680810574 0.5941936895562846\n",
            "Patience counter: 0\n",
            "Epoch: 9, iter 0: loss = 0.6954626441001892\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 9 loss average: 0.582\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7459    0.6011    0.6657       376\n",
            "           1     0.8662    0.8901    0.8780       764\n",
            "           2     0.7900    0.8009    0.7954      1080\n",
            "           3     0.8046    0.7423    0.7722       749\n",
            "           4     0.7400    0.8212    0.7785       520\n",
            "           5     0.7780    0.8025    0.7901      1210\n",
            "\n",
            "    accuracy                         0.7927      4699\n",
            "   macro avg     0.7875    0.7763    0.7800      4699\n",
            "weighted avg     0.7926    0.7927    0.7915      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5125    0.2847    0.3661       144\n",
            "           1     0.7749    0.6041    0.6789       245\n",
            "           2     0.4433    0.7031    0.5438       384\n",
            "           3     0.6730    0.6294    0.6505       170\n",
            "           4     0.7340    0.4615    0.5667       299\n",
            "           5     0.5758    0.5984    0.5869       381\n",
            "\n",
            "    accuracy                         0.5742      1623\n",
            "   macro avg     0.6189    0.5469    0.5655      1623\n",
            "weighted avg     0.6082    0.5742    0.5739      1623\n",
            "\n",
            "0.5817556499193112 0.7915108125098437 0.5155979958549635 0.5739347351661808\n",
            "Patience counter: 1\n",
            "Epoch: 10, iter 0: loss = 0.6424576640129089\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 10 loss average: 0.506\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7647    0.6915    0.7263       376\n",
            "           1     0.9056    0.8914    0.8984       764\n",
            "           2     0.8104    0.8231    0.8167      1080\n",
            "           3     0.8360    0.7757    0.8047       749\n",
            "           4     0.7900    0.8538    0.8207       520\n",
            "           5     0.7949    0.8231    0.8088      1210\n",
            "\n",
            "    accuracy                         0.8195      4699\n",
            "   macro avg     0.8169    0.8098    0.8126      4699\n",
            "weighted avg     0.8200    0.8195    0.8192      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7193    0.2847    0.4080       144\n",
            "           1     0.6132    0.7184    0.6617       245\n",
            "           2     0.4936    0.5026    0.4981       384\n",
            "           3     0.4701    0.7412    0.5753       170\n",
            "           4     0.7602    0.4983    0.6020       299\n",
            "           5     0.5495    0.6115    0.5789       381\n",
            "\n",
            "    accuracy                         0.5656      1623\n",
            "   macro avg     0.6010    0.5595    0.5540      1623\n",
            "weighted avg     0.5915    0.5656    0.5610      1623\n",
            "\n",
            "0.5055954720204076 0.8192434242679879 0.46044037166585944 0.5609825033009013\n",
            "Patience counter: 2\n",
            "Epoch: 11, iter 0: loss = 0.5246337056159973\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 11 loss average: 0.442\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7636    0.7473    0.7554       376\n",
            "           1     0.9092    0.9175    0.9134       764\n",
            "           2     0.8252    0.8435    0.8342      1080\n",
            "           3     0.8651    0.8051    0.8340       749\n",
            "           4     0.8038    0.8192    0.8114       520\n",
            "           5     0.8373    0.8504    0.8438      1210\n",
            "\n",
            "    accuracy                         0.8408      4699\n",
            "   macro avg     0.8340    0.8305    0.8320      4699\n",
            "weighted avg     0.8410    0.8408    0.8407      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5263    0.2778    0.3636       144\n",
            "           1     0.6535    0.6082    0.6300       245\n",
            "           2     0.4883    0.5417    0.5136       384\n",
            "           3     0.5481    0.6706    0.6032       170\n",
            "           4     0.6380    0.5953    0.6159       299\n",
            "           5     0.5567    0.5932    0.5743       381\n",
            "\n",
            "    accuracy                         0.5638      1623\n",
            "   macro avg     0.5685    0.5478    0.5501      1623\n",
            "weighted avg     0.5665    0.5638    0.5604      1623\n",
            "\n",
            "0.44174369346971315 0.8406948899723782 0.4986785205335665 0.5603532480591644\n",
            "Patience counter: 3\n",
            "Epoch: 12, iter 0: loss = 0.781413733959198\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 12 loss average: 0.388\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8000    0.7660    0.7826       376\n",
            "           1     0.9158    0.9254    0.9206       764\n",
            "           2     0.8651    0.8611    0.8631      1080\n",
            "           3     0.8481    0.8718    0.8598       749\n",
            "           4     0.8216    0.8500    0.8355       520\n",
            "           5     0.8733    0.8545    0.8638      1210\n",
            "\n",
            "    accuracy                         0.8627      4699\n",
            "   macro avg     0.8540    0.8548    0.8542      4699\n",
            "weighted avg     0.8627    0.8627    0.8626      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4783    0.3819    0.4247       144\n",
            "           1     0.6909    0.6204    0.6538       245\n",
            "           2     0.4704    0.6615    0.5498       384\n",
            "           3     0.6205    0.6059    0.6131       170\n",
            "           4     0.6626    0.5452    0.5982       299\n",
            "           5     0.5893    0.5197    0.5523       381\n",
            "\n",
            "    accuracy                         0.5699      1623\n",
            "   macro avg     0.5853    0.5558    0.5653      1623\n",
            "weighted avg     0.5834    0.5699    0.5705      1623\n",
            "\n",
            "0.3878777672847112 0.8626130225577194 0.4950200402041443 0.5705185287440506\n",
            "Patience counter: 4\n",
            "Epoch: 13, iter 0: loss = 0.40226662158966064\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 13 loss average: 0.350\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8205    0.8511    0.8355       376\n",
            "           1     0.9319    0.9319    0.9319       764\n",
            "           2     0.8719    0.8509    0.8613      1080\n",
            "           3     0.8784    0.8585    0.8683       749\n",
            "           4     0.8558    0.8558    0.8558       520\n",
            "           5     0.8676    0.8884    0.8779      1210\n",
            "\n",
            "    accuracy                         0.8755      4699\n",
            "   macro avg     0.8710    0.8728    0.8718      4699\n",
            "weighted avg     0.8757    0.8755    0.8755      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5190    0.2847    0.3677       144\n",
            "           1     0.6529    0.6449    0.6489       245\n",
            "           2     0.4444    0.7083    0.5462       384\n",
            "           3     0.6272    0.6235    0.6254       170\n",
            "           4     0.7553    0.4749    0.5832       299\n",
            "           5     0.6036    0.5276    0.5630       381\n",
            "\n",
            "    accuracy                         0.5669      1623\n",
            "   macro avg     0.6004    0.5440    0.5557      1623\n",
            "weighted avg     0.5963    0.5669    0.5649      1623\n",
            "\n",
            "0.34990216869240004 0.8755052963840999 0.494561215487525 0.5649104811136675\n",
            "Patience counter: 5\n",
            "Epoch: 14, iter 0: loss = 0.44202178716659546\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 14 loss average: 0.315\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8387    0.7606    0.7978       376\n",
            "           1     0.9392    0.9503    0.9447       764\n",
            "           2     0.8798    0.8880    0.8839      1080\n",
            "           3     0.8848    0.8611    0.8728       749\n",
            "           4     0.8463    0.9000    0.8723       520\n",
            "           5     0.8879    0.8901    0.8890      1210\n",
            "\n",
            "    accuracy                         0.8855      4699\n",
            "   macro avg     0.8794    0.8750    0.8767      4699\n",
            "weighted avg     0.8853    0.8855    0.8851      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4146    0.4722    0.4416       144\n",
            "           1     0.6278    0.6816    0.6536       245\n",
            "           2     0.4990    0.6797    0.5755       384\n",
            "           3     0.5746    0.6118    0.5926       170\n",
            "           4     0.6904    0.4548    0.5484       299\n",
            "           5     0.6062    0.4646    0.5260       381\n",
            "\n",
            "    accuracy                         0.5625      1623\n",
            "   macro avg     0.5688    0.5608    0.5563      1623\n",
            "weighted avg     0.5793    0.5625    0.5606      1623\n",
            "\n",
            "0.3150978308791916 0.8851440104396189 0.4952846342983214 0.5605903384729067\n",
            "Patience counter: 6\n",
            "Epoch: 15, iter 0: loss = 0.2735957205295563\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 15 loss average: 0.263\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8399    0.8511    0.8454       376\n",
            "           1     0.9480    0.9542    0.9511       764\n",
            "           2     0.9095    0.9028    0.9061      1080\n",
            "           3     0.9024    0.9132    0.9078       749\n",
            "           4     0.8927    0.8962    0.8944       520\n",
            "           5     0.9131    0.9033    0.9082      1210\n",
            "\n",
            "    accuracy                         0.9081      4699\n",
            "   macro avg     0.9009    0.9035    0.9022      4699\n",
            "weighted avg     0.9081    0.9081    0.9081      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7500    0.1667    0.2727       144\n",
            "           1     0.6582    0.6367    0.6473       245\n",
            "           2     0.4531    0.6042    0.5179       384\n",
            "           3     0.5118    0.6353    0.5669       170\n",
            "           4     0.6654    0.5652    0.6112       299\n",
            "           5     0.5305    0.5249    0.5277       381\n",
            "\n",
            "    accuracy                         0.5478      1623\n",
            "   macro avg     0.5948    0.5222    0.5240      1623\n",
            "weighted avg     0.5738    0.5478    0.5403      1623\n",
            "\n",
            "0.2628902238793671 0.9080776902391331 0.48760734670217054 0.5402986468717432\n",
            "Patience counter: 7\n",
            "Epoch: 16, iter 0: loss = 0.0707586258649826\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 16 loss average: 0.250\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8837    0.8085    0.8444       376\n",
            "           1     0.9455    0.9542    0.9498       764\n",
            "           2     0.9076    0.9185    0.9130      1080\n",
            "           3     0.9081    0.9105    0.9093       749\n",
            "           4     0.8807    0.9231    0.9014       520\n",
            "           5     0.9222    0.9107    0.9164      1210\n",
            "\n",
            "    accuracy                         0.9127      4699\n",
            "   macro avg     0.9080    0.9043    0.9057      4699\n",
            "weighted avg     0.9127    0.9127    0.9125      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4386    0.3472    0.3876       144\n",
            "           1     0.7032    0.6286    0.6638       245\n",
            "           2     0.4715    0.5807    0.5204       384\n",
            "           3     0.5231    0.6647    0.5855       170\n",
            "           4     0.6810    0.4783    0.5619       299\n",
            "           5     0.5294    0.5433    0.5363       381\n",
            "\n",
            "    accuracy                         0.5484      1623\n",
            "   macro avg     0.5578    0.5405    0.5426      1623\n",
            "weighted avg     0.5611    0.5484    0.5485      1623\n",
            "\n",
            "0.25007297098636627 0.912523573206141 0.49420928135067765 0.548453993590254\n",
            "Patience counter: 8\n",
            "Epoch: 17, iter 0: loss = 0.21798059344291687\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 17 loss average: 0.214\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9121    0.8830    0.8973       376\n",
            "           1     0.9606    0.9568    0.9587       764\n",
            "           2     0.9199    0.9250    0.9224      1080\n",
            "           3     0.9161    0.9186    0.9173       749\n",
            "           4     0.9264    0.9442    0.9352       520\n",
            "           5     0.9229    0.9207    0.9218      1210\n",
            "\n",
            "    accuracy                         0.9268      4699\n",
            "   macro avg     0.9263    0.9247    0.9255      4699\n",
            "weighted avg     0.9268    0.9268    0.9268      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4500    0.3750    0.4091       144\n",
            "           1     0.6325    0.6041    0.6180       245\n",
            "           2     0.4699    0.6094    0.5306       384\n",
            "           3     0.6065    0.5529    0.5785       170\n",
            "           4     0.7301    0.3980    0.5152       299\n",
            "           5     0.5077    0.6037    0.5516       381\n",
            "\n",
            "    accuracy                         0.5416      1623\n",
            "   macro avg     0.5661    0.5238    0.5338      1623\n",
            "weighted avg     0.5638    0.5416    0.5401      1623\n",
            "\n",
            "0.2144864754906545 0.9267596692009861 0.48318050187783756 0.5400958655583208\n",
            "Patience counter: 9\n",
            "Epoch: 18, iter 0: loss = 0.27444392442703247\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 18 loss average: 0.185\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8933    0.8910    0.8921       376\n",
            "           1     0.9646    0.9634    0.9640       764\n",
            "           2     0.9338    0.9278    0.9308      1080\n",
            "           3     0.9320    0.9332    0.9326       749\n",
            "           4     0.9256    0.9327    0.9291       520\n",
            "           5     0.9357    0.9388    0.9373      1210\n",
            "\n",
            "    accuracy                         0.9349      4699\n",
            "   macro avg     0.9308    0.9311    0.9310      4699\n",
            "weighted avg     0.9349    0.9349    0.9349      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5119    0.2986    0.3772       144\n",
            "           1     0.5719    0.6816    0.6220       245\n",
            "           2     0.4568    0.5781    0.5103       384\n",
            "           3     0.5934    0.6353    0.6136       170\n",
            "           4     0.6904    0.4548    0.5484       299\n",
            "           5     0.5497    0.5512    0.5505       381\n",
            "\n",
            "    accuracy                         0.5459      1623\n",
            "   macro avg     0.5624    0.5333    0.5370      1623\n",
            "weighted avg     0.5582    0.5459    0.5426      1623\n",
            "\n",
            "0.1845636176682698 0.934876932687242 0.49449441038627806 0.5426263156658024\n",
            "Patience counter: 10\n",
            "Epoch: 19, iter 0: loss = 0.0759856328368187\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 19 loss average: 0.164\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9117    0.9335    0.9225       376\n",
            "           1     0.9661    0.9699    0.9680       764\n",
            "           2     0.9321    0.9278    0.9299      1080\n",
            "           3     0.9361    0.9386    0.9373       749\n",
            "           4     0.9443    0.9462    0.9452       520\n",
            "           5     0.9408    0.9331    0.9369      1210\n",
            "\n",
            "    accuracy                         0.9402      4699\n",
            "   macro avg     0.9385    0.9415    0.9400      4699\n",
            "weighted avg     0.9402    0.9402    0.9402      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4040    0.2778    0.3292       144\n",
            "           1     0.7403    0.5469    0.6291       245\n",
            "           2     0.4755    0.6068    0.5332       384\n",
            "           3     0.5300    0.6765    0.5943       170\n",
            "           4     0.6113    0.5050    0.5531       299\n",
            "           5     0.5398    0.5512    0.5455       381\n",
            "\n",
            "    accuracy                         0.5441      1623\n",
            "   macro avg     0.5502    0.5274    0.5307      1623\n",
            "weighted avg     0.5550    0.5441    0.5425      1623\n",
            "\n",
            "0.1636764113791287 0.9401992782217278 0.49896887539794205 0.5425218776963469\n",
            "Patience counter: 11\n",
            "Done! It took 7.8e+02 secs\n",
            "\n",
            "Current RUN: 3\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0019919574260712\n",
            "Best test f1 weighted\n",
            "0.5941936895562846\n",
            "Best epoch\n",
            "8\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.2 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.849906325340271\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 1 loss average: 1.790\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1059    0.0890    0.0967       764\n",
            "           2     0.1628    0.0583    0.0859      1080\n",
            "           3     0.0826    0.0360    0.0502       749\n",
            "           4     0.0364    0.0077    0.0127       520\n",
            "           5     0.2465    0.6587    0.3588      1210\n",
            "\n",
            "    accuracy                         0.2041      4699\n",
            "   macro avg     0.1057    0.1416    0.1007      4699\n",
            "weighted avg     0.1353    0.2041    0.1373      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.0000    0.0000    0.0000       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2348    1.0000    0.3802       381\n",
            "\n",
            "    accuracy                         0.2348      1623\n",
            "   macro avg     0.0391    0.1667    0.0634      1623\n",
            "weighted avg     0.0551    0.2348    0.0893      1623\n",
            "\n",
            "1.790017008781433 0.13725497804596598 0.08752898664592536 0.0892614032563339\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 1.5641106367111206\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 2 loss average: 1.711\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2189    0.1204    0.1553      1080\n",
            "           3     0.2045    0.0240    0.0430       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2589    0.8595    0.3979      1210\n",
            "\n",
            "    accuracy                         0.2528      4699\n",
            "   macro avg     0.1137    0.1673    0.0994      4699\n",
            "weighted avg     0.1496    0.2528    0.1450      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.3643    0.1328    0.1947       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2549    0.9921    0.4056       381\n",
            "\n",
            "    accuracy                         0.2643      1623\n",
            "   macro avg     0.1032    0.1875    0.1000      1623\n",
            "weighted avg     0.1460    0.2643    0.1413      1623\n",
            "\n",
            "1.7112281993031502 0.14502168410090382 0.11954459611962791 0.1412654606753018\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.7045609951019287\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 3 loss average: 1.460\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2639    0.0505    0.0848       376\n",
            "           1     0.4687    0.4411    0.4545       764\n",
            "           2     0.3443    0.4370    0.3851      1080\n",
            "           3     0.2540    0.0427    0.0731       749\n",
            "           4     0.5015    0.3135    0.3858       520\n",
            "           5     0.3591    0.6190    0.4545      1210\n",
            "\n",
            "    accuracy                         0.3771      4699\n",
            "   macro avg     0.3652    0.3173    0.3063      4699\n",
            "weighted avg     0.3649    0.3771    0.3406      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4412    0.1042    0.1685       144\n",
            "           1     0.5452    0.7633    0.6361       245\n",
            "           2     0.6983    0.3255    0.4440       384\n",
            "           3     0.2522    0.5059    0.3366       170\n",
            "           4     0.5692    0.8528    0.6827       299\n",
            "           5     0.5899    0.4304    0.4977       381\n",
            "\n",
            "    accuracy                         0.5126      1623\n",
            "   macro avg     0.5160    0.4970    0.4609      1623\n",
            "weighted avg     0.5564    0.5126    0.4939      1623\n",
            "\n",
            "1.459664098918438 0.34058570333219235 0.3067359762123695 0.49390545415555503\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.1905713081359863\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 4 loss average: 1.145\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3613    0.1144    0.1737       376\n",
            "           1     0.7161    0.7526    0.7339       764\n",
            "           2     0.4723    0.4657    0.4690      1080\n",
            "           3     0.7340    0.3057    0.4317       749\n",
            "           4     0.4885    0.6942    0.5735       520\n",
            "           5     0.5244    0.7198    0.6068      1210\n",
            "\n",
            "    accuracy                         0.5495      4699\n",
            "   macro avg     0.5494    0.5088    0.4981      4699\n",
            "weighted avg     0.5600    0.5495    0.5295      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3443    0.1458    0.2049       144\n",
            "           1     0.6305    0.7592    0.6889       245\n",
            "           2     0.4970    0.6432    0.5607       384\n",
            "           3     0.8295    0.4294    0.5659       170\n",
            "           4     0.5145    0.8294    0.6351       299\n",
            "           5     0.6350    0.3333    0.4372       381\n",
            "\n",
            "    accuracy                         0.5558      1623\n",
            "   macro avg     0.5751    0.5234    0.5154      1623\n",
            "weighted avg     0.5741    0.5558    0.5337      1623\n",
            "\n",
            "1.1454929821193218 0.529523862471526 0.3826248786910786 0.533737000354194\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.2832692861557007\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 5 loss average: 0.984\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6576    0.3218    0.4321       376\n",
            "           1     0.7603    0.8220    0.7899       764\n",
            "           2     0.5630    0.5380    0.5502      1080\n",
            "           3     0.6831    0.4806    0.5643       749\n",
            "           4     0.5481    0.7558    0.6354       520\n",
            "           5     0.5796    0.6769    0.6245      1210\n",
            "\n",
            "    accuracy                         0.6176      4699\n",
            "   macro avg     0.6320    0.5992    0.5994      4699\n",
            "weighted avg     0.6244    0.6176    0.6105      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5385    0.2431    0.3349       144\n",
            "           1     0.7051    0.6735    0.6889       245\n",
            "           2     0.4538    0.6146    0.5221       384\n",
            "           3     0.7679    0.5059    0.6099       170\n",
            "           4     0.6827    0.4749    0.5602       299\n",
            "           5     0.5475    0.6955    0.6127       381\n",
            "\n",
            "    accuracy                         0.5724      1623\n",
            "   macro avg     0.6159    0.5346    0.5548      1623\n",
            "weighted avg     0.5963    0.5724    0.5682      1623\n",
            "\n",
            "0.9843557178974152 0.6105263743912444 0.47085451386747934 0.5681667244595853\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.6017783284187317\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 6 loss average: 0.837\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7072    0.4176    0.5251       376\n",
            "           1     0.8094    0.8560    0.8321       764\n",
            "           2     0.6454    0.6370    0.6412      1080\n",
            "           3     0.7367    0.5527    0.6316       749\n",
            "           4     0.6399    0.8269    0.7215       520\n",
            "           5     0.6340    0.7174    0.6731      1210\n",
            "\n",
            "    accuracy                         0.6833      4699\n",
            "   macro avg     0.6954    0.6679    0.6708      4699\n",
            "weighted avg     0.6880    0.6833    0.6785      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3654    0.5278    0.4318       144\n",
            "           1     0.6049    0.7061    0.6516       245\n",
            "           2     0.5332    0.6276    0.5766       384\n",
            "           3     0.6207    0.6353    0.6279       170\n",
            "           4     0.6746    0.2843    0.4000       299\n",
            "           5     0.5729    0.5669    0.5699       381\n",
            "\n",
            "    accuracy                         0.5539      1623\n",
            "   macro avg     0.5620    0.5580    0.5430      1623\n",
            "weighted avg     0.5737    0.5539    0.5463      1623\n",
            "\n",
            "0.836748947078983 0.6785105547613679 0.5007672260480827 0.546337128565019\n",
            "Patience counter: 0\n",
            "Epoch: 7, iter 0: loss = 0.9454296827316284\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 7 loss average: 0.722\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6726    0.6064    0.6378       376\n",
            "           1     0.8482    0.8704    0.8592       764\n",
            "           2     0.6910    0.7019    0.6964      1080\n",
            "           3     0.7893    0.6502    0.7130       749\n",
            "           4     0.7472    0.7731    0.7599       520\n",
            "           5     0.6896    0.7545    0.7206      1210\n",
            "\n",
            "    accuracy                         0.7348      4699\n",
            "   macro avg     0.7396    0.7261    0.7311      4699\n",
            "weighted avg     0.7366    0.7348    0.7341      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5053    0.3333    0.4017       144\n",
            "           1     0.6733    0.6898    0.6815       245\n",
            "           2     0.4478    0.6927    0.5440       384\n",
            "           3     0.6645    0.5941    0.6273       170\n",
            "           4     0.7205    0.3880    0.5043       299\n",
            "           5     0.5838    0.5669    0.5752       381\n",
            "\n",
            "    accuracy                         0.5644      1623\n",
            "   macro avg     0.5992    0.5441    0.5557      1623\n",
            "weighted avg     0.5918    0.5644    0.5609      1623\n",
            "\n",
            "0.7220245183755954 0.7340783643630084 0.4795756591062331 0.5608686563682271\n",
            "Patience counter: 1\n",
            "Epoch: 8, iter 0: loss = 0.62978196144104\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 8 loss average: 0.633\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7532    0.6170    0.6784       376\n",
            "           1     0.8531    0.9045    0.8780       764\n",
            "           2     0.7371    0.7426    0.7399      1080\n",
            "           3     0.7864    0.6782    0.7283       749\n",
            "           4     0.7491    0.8212    0.7835       520\n",
            "           5     0.7283    0.7686    0.7479      1210\n",
            "\n",
            "    accuracy                         0.7640      4699\n",
            "   macro avg     0.7679    0.7553    0.7593      4699\n",
            "weighted avg     0.7642    0.7640    0.7625      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4429    0.4306    0.4366       144\n",
            "           1     0.6245    0.6653    0.6443       245\n",
            "           2     0.4759    0.5911    0.5273       384\n",
            "           3     0.6643    0.5471    0.6000       170\n",
            "           4     0.7611    0.2876    0.4175       299\n",
            "           5     0.5407    0.6982    0.6094       381\n",
            "\n",
            "    accuracy                         0.5527      1623\n",
            "   macro avg     0.5849    0.5366    0.5392      1623\n",
            "weighted avg     0.5829    0.5527    0.5436      1623\n",
            "\n",
            "0.6333583494027456 0.7624552701083468 0.46426954196760656 0.5435636859842559\n",
            "Patience counter: 2\n",
            "Epoch: 9, iter 0: loss = 0.36751171946525574\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 9 loss average: 0.531\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7710    0.7074    0.7379       376\n",
            "           1     0.8929    0.9058    0.8993       764\n",
            "           2     0.7820    0.8037    0.7927      1080\n",
            "           3     0.8281    0.7397    0.7814       749\n",
            "           4     0.7947    0.8115    0.8030       520\n",
            "           5     0.7801    0.8182    0.7987      1210\n",
            "\n",
            "    accuracy                         0.8070      4699\n",
            "   macro avg     0.8081    0.7977    0.8022      4699\n",
            "weighted avg     0.8074    0.8070    0.8065      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4203    0.4028    0.4113       144\n",
            "           1     0.6779    0.5755    0.6225       245\n",
            "           2     0.4622    0.6536    0.5415       384\n",
            "           3     0.5600    0.6588    0.6054       170\n",
            "           4     0.7134    0.3746    0.4912       299\n",
            "           5     0.5411    0.5354    0.5383       381\n",
            "\n",
            "    accuracy                         0.5410      1623\n",
            "   macro avg     0.5625    0.5335    0.5350      1623\n",
            "weighted avg     0.5661    0.5410    0.5389      1623\n",
            "\n",
            "0.5314826481044292 0.8065284180513731 0.48549698869868746 0.538861027702451\n",
            "Patience counter: 3\n",
            "Epoch: 10, iter 0: loss = 0.3079233765602112\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 10 loss average: 0.465\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7956    0.7766    0.7860       376\n",
            "           1     0.8996    0.9149    0.9072       764\n",
            "           2     0.8330    0.7898    0.8108      1080\n",
            "           3     0.8311    0.8144    0.8227       749\n",
            "           4     0.8231    0.8500    0.8363       520\n",
            "           5     0.8032    0.8364    0.8194      1210\n",
            "\n",
            "    accuracy                         0.8317      4699\n",
            "   macro avg     0.8309    0.8304    0.8304      4699\n",
            "weighted avg     0.8318    0.8317    0.8314      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4356    0.4931    0.4625       144\n",
            "           1     0.5948    0.5633    0.5786       245\n",
            "           2     0.4819    0.6250    0.5442       384\n",
            "           3     0.5347    0.6353    0.5806       170\n",
            "           4     0.7188    0.3077    0.4309       299\n",
            "           5     0.5400    0.5669    0.5531       381\n",
            "\n",
            "    accuracy                         0.5330      1623\n",
            "   macro avg     0.5510    0.5319    0.5250      1623\n",
            "weighted avg     0.5576    0.5330    0.5272      1623\n",
            "\n",
            "0.46486176115771133 0.8314362441583157 0.4686031509322207 0.5271992782107106\n",
            "Patience counter: 4\n",
            "Epoch: 11, iter 0: loss = 0.3555942177772522\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 11 loss average: 0.408\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8005    0.8218    0.8110       376\n",
            "           1     0.8978    0.9424    0.9195       764\n",
            "           2     0.8514    0.8278    0.8394      1080\n",
            "           3     0.8672    0.8371    0.8519       749\n",
            "           4     0.8261    0.8404    0.8332       520\n",
            "           5     0.8594    0.8587    0.8590      1210\n",
            "\n",
            "    accuracy                         0.8568      4699\n",
            "   macro avg     0.8504    0.8547    0.8524      4699\n",
            "weighted avg     0.8566    0.8568    0.8565      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3854    0.2569    0.3083       144\n",
            "           1     0.6364    0.6000    0.6176       245\n",
            "           2     0.5253    0.5677    0.5457       384\n",
            "           3     0.5176    0.6059    0.5583       170\n",
            "           4     0.6299    0.5351    0.5787       299\n",
            "           5     0.5327    0.5984    0.5637       381\n",
            "\n",
            "    accuracy                         0.5502      1623\n",
            "   macro avg     0.5379    0.5273    0.5287      1623\n",
            "weighted avg     0.5499    0.5502    0.5471      1623\n",
            "\n",
            "0.4076509963100155 0.8565269475152774 0.47001302249276106 0.547100748415734\n",
            "Patience counter: 5\n",
            "Epoch: 12, iter 0: loss = 0.40905046463012695\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 12 loss average: 0.333\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8588    0.8085    0.8329       376\n",
            "           1     0.9288    0.9398    0.9343       764\n",
            "           2     0.8776    0.8759    0.8767      1080\n",
            "           3     0.8638    0.8718    0.8678       749\n",
            "           4     0.8685    0.9019    0.8849       520\n",
            "           5     0.8848    0.8760    0.8804      1210\n",
            "\n",
            "    accuracy                         0.8832      4699\n",
            "   macro avg     0.8804    0.8790    0.8795      4699\n",
            "weighted avg     0.8831    0.8832    0.8830      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4321    0.4861    0.4575       144\n",
            "           1     0.5730    0.6571    0.6122       245\n",
            "           2     0.5110    0.6042    0.5537       384\n",
            "           3     0.4937    0.6941    0.5770       170\n",
            "           4     0.6667    0.3946    0.4958       299\n",
            "           5     0.5710    0.4646    0.5123       381\n",
            "\n",
            "    accuracy                         0.5397      1623\n",
            "   macro avg     0.5412    0.5501    0.5347      1623\n",
            "weighted avg     0.5543    0.5397    0.5360      1623\n",
            "\n",
            "0.332903672552978 0.8830028120018794 0.4751423197049228 0.5360487736111001\n",
            "Patience counter: 6\n",
            "Epoch: 13, iter 0: loss = 0.3595699965953827\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 13 loss average: 0.294\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8594    0.8777    0.8684       376\n",
            "           1     0.9501    0.9463    0.9482       764\n",
            "           2     0.8909    0.8843    0.8875      1080\n",
            "           3     0.8858    0.8905    0.8881       749\n",
            "           4     0.9096    0.8904    0.8999       520\n",
            "           5     0.8918    0.8992    0.8955      1210\n",
            "\n",
            "    accuracy                         0.8993      4699\n",
            "   macro avg     0.8979    0.8981    0.8979      4699\n",
            "weighted avg     0.8995    0.8993    0.8994      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3810    0.3889    0.3849       144\n",
            "           1     0.5885    0.5837    0.5861       245\n",
            "           2     0.4685    0.6589    0.5476       384\n",
            "           3     0.5521    0.6235    0.5856       170\n",
            "           4     0.6154    0.4548    0.5231       299\n",
            "           5     0.6000    0.4409    0.5083       381\n",
            "\n",
            "    accuracy                         0.5311      1623\n",
            "   macro avg     0.5342    0.5251    0.5226      1623\n",
            "weighted avg     0.5455    0.5311    0.5292      1623\n",
            "\n",
            "0.29379433874661726 0.8993816977487582 0.4790112205121853 0.5292191423248854\n",
            "Patience counter: 7\n",
            "Epoch: 14, iter 0: loss = 0.2508365213871002\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 14 loss average: 0.264\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8984    0.8697    0.8838       376\n",
            "           1     0.9403    0.9490    0.9446       764\n",
            "           2     0.8801    0.8907    0.8854      1080\n",
            "           3     0.8922    0.8838    0.8880       749\n",
            "           4     0.9223    0.9135    0.9179       520\n",
            "           5     0.8946    0.8975    0.8960      1210\n",
            "\n",
            "    accuracy                         0.9017      4699\n",
            "   macro avg     0.9047    0.9007    0.9026      4699\n",
            "weighted avg     0.9017    0.9017    0.9016      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4051    0.2222    0.2870       144\n",
            "           1     0.6719    0.5265    0.5904       245\n",
            "           2     0.4532    0.6432    0.5318       384\n",
            "           3     0.5852    0.6059    0.5954       170\n",
            "           4     0.5898    0.5050    0.5441       299\n",
            "           5     0.5440    0.5354    0.5397       381\n",
            "\n",
            "    accuracy                         0.5336      1623\n",
            "   macro avg     0.5415    0.5064    0.5147      1623\n",
            "weighted avg     0.5423    0.5336    0.5297      1623\n",
            "\n",
            "0.26398663098613423 0.9016496966602006 0.4783069635074244 0.5296971284428562\n",
            "Patience counter: 8\n",
            "Epoch: 15, iter 0: loss = 0.23886291682720184\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 15 loss average: 0.232\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9118    0.8803    0.8958       376\n",
            "           1     0.9375    0.9620    0.9496       764\n",
            "           2     0.9136    0.9102    0.9119      1080\n",
            "           3     0.9116    0.8812    0.8961       749\n",
            "           4     0.9385    0.9385    0.9385       520\n",
            "           5     0.8994    0.9157    0.9075      1210\n",
            "\n",
            "    accuracy                         0.9162      4699\n",
            "   macro avg     0.9187    0.9146    0.9166      4699\n",
            "weighted avg     0.9161    0.9162    0.9160      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3611    0.4514    0.4012       144\n",
            "           1     0.6139    0.6490    0.6310       245\n",
            "           2     0.4967    0.5964    0.5420       384\n",
            "           3     0.5763    0.6000    0.5879       170\n",
            "           4     0.6497    0.3846    0.4832       299\n",
            "           5     0.5691    0.5512    0.5600       381\n",
            "\n",
            "    accuracy                         0.5422      1623\n",
            "   macro avg     0.5445    0.5388    0.5342      1623\n",
            "weighted avg     0.5559    0.5422    0.5411      1623\n",
            "\n",
            "0.23215512562698373 0.9160183027068203 0.4836216258346632 0.5411403626113171\n",
            "Patience counter: 9\n",
            "Epoch: 16, iter 0: loss = 0.1699342578649521\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 16 loss average: 0.208\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9034    0.9202    0.9117       376\n",
            "           1     0.9517    0.9542    0.9529       764\n",
            "           2     0.9211    0.9185    0.9198      1080\n",
            "           3     0.9038    0.9159    0.9098       749\n",
            "           4     0.9551    0.9404    0.9477       520\n",
            "           5     0.9243    0.9182    0.9212      1210\n",
            "\n",
            "    accuracy                         0.9264      4699\n",
            "   macro avg     0.9266    0.9279    0.9272      4699\n",
            "weighted avg     0.9265    0.9264    0.9264      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4028    0.4028    0.4028       144\n",
            "           1     0.6145    0.6571    0.6351       245\n",
            "           2     0.4841    0.5938    0.5333       384\n",
            "           3     0.4476    0.6529    0.5311       170\n",
            "           4     0.7421    0.3946    0.5153       299\n",
            "           5     0.5487    0.4882    0.5167       381\n",
            "\n",
            "    accuracy                         0.5311      1623\n",
            "   macro avg     0.5400    0.5316    0.5224      1623\n",
            "weighted avg     0.5554    0.5311    0.5296      1623\n",
            "\n",
            "0.2075700556548933 0.9264018401458934 0.4779365585125105 0.5296417302801087\n",
            "Patience counter: 10\n",
            "Epoch: 17, iter 0: loss = 0.16692069172859192\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 17 loss average: 0.187\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9115    0.9043    0.9079       376\n",
            "           1     0.9587    0.9712    0.9649       764\n",
            "           2     0.9282    0.9222    0.9252      1080\n",
            "           3     0.9159    0.9306    0.9232       749\n",
            "           4     0.9293    0.9346    0.9319       520\n",
            "           5     0.9423    0.9306    0.9364      1210\n",
            "\n",
            "    accuracy                         0.9336      4699\n",
            "   macro avg     0.9310    0.9322    0.9316      4699\n",
            "weighted avg     0.9336    0.9336    0.9336      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4273    0.3264    0.3701       144\n",
            "           1     0.6000    0.6245    0.6120       245\n",
            "           2     0.4794    0.5755    0.5231       384\n",
            "           3     0.4408    0.6353    0.5205       170\n",
            "           4     0.6474    0.4114    0.5031       299\n",
            "           5     0.5442    0.5171    0.5303       381\n",
            "\n",
            "    accuracy                         0.5231      1623\n",
            "   macro avg     0.5232    0.5150    0.5098      1623\n",
            "weighted avg     0.5351    0.5231    0.5207      1623\n",
            "\n",
            "0.18664003846546015 0.933573429681587 0.46100966736876037 0.5206590682253449\n",
            "Patience counter: 11\n",
            "Done! It took 7e+02 secs\n",
            "\n",
            "Current RUN: 4\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.073816917836666\n",
            "Best test f1 weighted\n",
            "0.546337128565019\n",
            "Best epoch\n",
            "6\n",
            "Configurations\n",
            "{'activation': 'Tanh',\n",
            " 'batch_size': 2,\n",
            " 'bidirectional': True,\n",
            " 'checkpoint': None,\n",
            " 'clip': 1.0,\n",
            " 'context_size': 256,\n",
            " 'conversation_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/conversation_length.pkl'),\n",
            " 'data': 'iemocap',\n",
            " 'data_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train'),\n",
            " 'dataset_dir': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap'),\n",
            " 'dropout': 0.0,\n",
            " 'embedding_size': 300,\n",
            " 'encoder_hidden_size': 768,\n",
            " 'eval_batch_size': 2,\n",
            " 'feedforward': 'FeedForward',\n",
            " 'id2word_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/id2word.pkl'),\n",
            " 'label_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/labels.pkl'),\n",
            " 'learning_rate': 0.0001,\n",
            " 'load_checkpoint': '../generative_weights/cornell_weights.pkl',\n",
            " 'minimum_improvement': 0.001,\n",
            " 'mode': 'train',\n",
            " 'model': 'bc_RNN',\n",
            " 'n_epoch': 500,\n",
            " 'num_bert_layers': 4,\n",
            " 'num_classes': 6,\n",
            " 'num_layers': 1,\n",
            " 'optimizer': <class 'torch.optim.adam.Adam'>,\n",
            " 'patience': 10,\n",
            " 'plot_every_epoch': 1,\n",
            " 'print_every': 100,\n",
            " 'rnn': <class 'torch.nn.modules.rnn.GRU'>,\n",
            " 'rnncell': <class 'layer.rnncells.StackedGRUCell'>,\n",
            " 'runs': 5,\n",
            " 'save_every_epoch': 1,\n",
            " 'sentence_length_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentence_length.pkl'),\n",
            " 'sentences_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/train/sentences.pkl'),\n",
            " 'train_emb': True,\n",
            " 'training_percentage': 1.0,\n",
            " 'word2id_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word2id.pkl'),\n",
            " 'word_emb_path': PosixPath('/content/gdrive/My Drive/Colab Notebooks/Sakina Model/TL-ERC/datasets/iemocap/word_emb.pkl')}\n",
            "Build Graph\n",
            "Parameter initiailization\n",
            "\tcontext_encoder.rnn.weight_hh_l0\n",
            "\tencoder.embeddings.word_embeddings.weight True\n",
            "\tencoder.embeddings.position_embeddings.weight True\n",
            "\tencoder.embeddings.token_type_embeddings.weight True\n",
            "\tencoder.embeddings.LayerNorm.weight True\n",
            "\tencoder.embeddings.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.dense.weight True\n",
            "\tencoder.encoder.layer.0.output.dense.bias True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.dense.weight True\n",
            "\tencoder.encoder.layer.1.output.dense.bias True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.dense.weight True\n",
            "\tencoder.encoder.layer.2.output.dense.bias True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias True\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight True\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight True\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.dense.weight True\n",
            "\tencoder.encoder.layer.3.output.dense.bias True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight True\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias True\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.dense.weight False\n",
            "\tencoder.encoder.layer.4.output.dense.bias False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.dense.weight False\n",
            "\tencoder.encoder.layer.5.output.dense.bias False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.dense.weight False\n",
            "\tencoder.encoder.layer.6.output.dense.bias False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.dense.weight False\n",
            "\tencoder.encoder.layer.7.output.dense.bias False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.dense.weight False\n",
            "\tencoder.encoder.layer.8.output.dense.bias False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.dense.weight False\n",
            "\tencoder.encoder.layer.9.output.dense.bias False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.dense.weight False\n",
            "\tencoder.encoder.layer.10.output.dense.bias False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias False\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight False\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight False\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.dense.weight False\n",
            "\tencoder.encoder.layer.11.output.dense.bias False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight False\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias False\n",
            "\tencoder.pooler.dense.weight True\n",
            "\tencoder.pooler.dense.bias True\n",
            "\tcontext_encoder.rnn.weight_ih_l0 True\n",
            "\tcontext_encoder.rnn.weight_hh_l0 True\n",
            "\tcontext_encoder.rnn.bias_ih_l0 True\n",
            "\tcontext_encoder.rnn.bias_hh_l0 True\n",
            "\tcontext2decoder.linears.0.weight True\n",
            "\tcontext2decoder.linears.0.bias True\n",
            "\tdecoder2output.linears.0.weight True\n",
            "\tdecoder2output.linears.0.bias True\n",
            "Model Parameters\n",
            "\tencoder.embeddings.word_embeddings.weight\t [30522, 768]\n",
            "\tencoder.embeddings.position_embeddings.weight\t [512, 768]\n",
            "\tencoder.embeddings.token_type_embeddings.weight\t [2, 768]\n",
            "\tencoder.embeddings.LayerNorm.weight\t [768]\n",
            "\tencoder.embeddings.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.0.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.0.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.0.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.0.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.0.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.1.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.1.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.1.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.1.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.1.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.2.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.2.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.2.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.2.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.2.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.3.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.3.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.3.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.3.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.3.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.4.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.4.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.4.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.4.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.4.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.5.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.5.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.5.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.5.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.5.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.6.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.6.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.6.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.6.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.6.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.7.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.7.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.7.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.7.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.7.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.8.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.8.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.8.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.8.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.8.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.9.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.9.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.9.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.9.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.9.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.10.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.10.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.10.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.10.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.10.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.query.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.key.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.self.value.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.weight\t [768, 768]\n",
            "\tencoder.encoder.layer.11.attention.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.attention.output.LayerNorm.bias\t [768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.weight\t [3072, 768]\n",
            "\tencoder.encoder.layer.11.intermediate.dense.bias\t [3072]\n",
            "\tencoder.encoder.layer.11.output.dense.weight\t [768, 3072]\n",
            "\tencoder.encoder.layer.11.output.dense.bias\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.weight\t [768]\n",
            "\tencoder.encoder.layer.11.output.LayerNorm.bias\t [768]\n",
            "\tencoder.pooler.dense.weight\t [768, 768]\n",
            "\tencoder.pooler.dense.bias\t [768]\n",
            "\tcontext_encoder.rnn.weight_ih_l0\t [768, 768]\n",
            "\tcontext_encoder.rnn.weight_hh_l0\t [768, 256]\n",
            "\tcontext_encoder.rnn.bias_ih_l0\t [768]\n",
            "\tcontext_encoder.rnn.bias_hh_l0\t [768]\n",
            "\tcontext2decoder.linears.0.weight\t [256, 256]\n",
            "\tcontext2decoder.linears.0.bias\t [256]\n",
            "\tdecoder2output.linears.0.weight\t [6, 256]\n",
            "\tdecoder2output.linears.0.bias\t [6]\n",
            "Load parameters from ../generative_weights/cornell_weights.pkl\n",
            "Filtered pretrained dict: dict_keys(['context_encoder.rnn.weight_hh_l0', 'context_encoder.rnn.bias_hh_l0', 'context2decoder.linears.0.weight', 'context2decoder.linears.0.bias'])\n",
            "Done! It took 8.3 secs\n",
            "\n",
            "Training Start!\n",
            "Epoch: 1, iter 0: loss = 1.8255722522735596\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.55it/s]\n",
            "Epoch 1 loss average: 1.760\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.0000    0.0000    0.0000       764\n",
            "           2     0.2615    0.0630    0.1015      1080\n",
            "           3     0.0161    0.0013    0.0025       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2706    0.9438    0.4205      1210\n",
            "\n",
            "    accuracy                         0.2577      4699\n",
            "   macro avg     0.0914    0.1680    0.0874      4699\n",
            "weighted avg     0.1323    0.2577    0.1320      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.2366    1.0000    0.3827       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.0000    0.0000    0.0000       381\n",
            "\n",
            "    accuracy                         0.2366      1623\n",
            "   macro avg     0.0394    0.1667    0.0638      1623\n",
            "weighted avg     0.0560    0.2366    0.0905      1623\n",
            "\n",
            "1.7595686341325443 0.13201169864568704 0.07909632291642447 0.09053709429197439\n",
            "Patience counter: 0\n",
            "Epoch: 2, iter 0: loss = 2.306900978088379\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 2 loss average: 1.746\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       376\n",
            "           1     0.1667    0.0432    0.0686       764\n",
            "           2     0.2629    0.5546    0.3568      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.0000    0.0000    0.0000       520\n",
            "           5     0.2663    0.4893    0.3449      1210\n",
            "\n",
            "    accuracy                         0.2605      4699\n",
            "   macro avg     0.1160    0.1812    0.1284      4699\n",
            "weighted avg     0.1561    0.2605    0.1820      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.0000    0.0000    0.0000       144\n",
            "           1     0.0000    0.0000    0.0000       245\n",
            "           2     0.5690    0.1719    0.2640       384\n",
            "           3     0.0000    0.0000    0.0000       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.2488    0.9843    0.3972       381\n",
            "\n",
            "    accuracy                         0.2717      1623\n",
            "   macro avg     0.1363    0.1927    0.1102      1623\n",
            "weighted avg     0.1930    0.2717    0.1557      1623\n",
            "\n",
            "1.7459649840990703 0.1819601781327201 0.10999945806159538 0.15571573357561327\n",
            "Patience counter: 0\n",
            "Epoch: 3, iter 0: loss = 1.540048360824585\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 3 loss average: 1.591\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3333    0.0027    0.0053       376\n",
            "           1     0.5499    0.3246    0.4082       764\n",
            "           2     0.3317    0.6194    0.4320      1080\n",
            "           3     0.0000    0.0000    0.0000       749\n",
            "           4     0.5333    0.0154    0.0299       520\n",
            "           5     0.3290    0.6017    0.4254      1210\n",
            "\n",
            "    accuracy                         0.3520      4699\n",
            "   macro avg     0.3462    0.2606    0.2168      4699\n",
            "weighted avg     0.3360    0.3520    0.2789      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.2677    0.2361    0.2509       144\n",
            "           1     0.6065    0.5347    0.5683       245\n",
            "           2     0.3830    0.6823    0.4906       384\n",
            "           3     0.9474    0.2118    0.3462       170\n",
            "           4     0.0000    0.0000    0.0000       299\n",
            "           5     0.4749    0.6955    0.5644       381\n",
            "\n",
            "    accuracy                         0.4486      1623\n",
            "   macro avg     0.4466    0.3934    0.3701      1623\n",
            "weighted avg     0.4166    0.4486    0.3929      1623\n",
            "\n",
            "1.5913562600811322 0.2789317175403301 0.3023136032809234 0.3928972235307062\n",
            "Patience counter: 0\n",
            "Epoch: 4, iter 0: loss = 1.162248134613037\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 4 loss average: 1.196\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4070    0.2793    0.3312       376\n",
            "           1     0.7008    0.7696    0.7336       764\n",
            "           2     0.4390    0.5861    0.5020      1080\n",
            "           3     0.7200    0.2403    0.3604       749\n",
            "           4     0.5530    0.2808    0.3724       520\n",
            "           5     0.4903    0.6669    0.5651      1210\n",
            "\n",
            "    accuracy                         0.5233      4699\n",
            "   macro avg     0.5517    0.4705    0.4775      4699\n",
            "weighted avg     0.5496    0.5233    0.5053      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7647    0.0903    0.1615       144\n",
            "           1     0.5681    0.8000    0.6644       245\n",
            "           2     0.6047    0.5339    0.5671       384\n",
            "           3     0.7054    0.5353    0.6087       170\n",
            "           4     0.5989    0.7191    0.6535       299\n",
            "           5     0.5599    0.6378    0.5963       381\n",
            "\n",
            "    accuracy                         0.5933      1623\n",
            "   macro avg     0.6336    0.5527    0.5419      1623\n",
            "weighted avg     0.6123    0.5933    0.5729      1623\n",
            "\n",
            "1.1958935223519802 0.5053327788545542 0.4065627516243095 0.5729295127304855\n",
            "Patience counter: 0\n",
            "Epoch: 5, iter 0: loss = 1.011043667793274\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 5 loss average: 0.969\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5661    0.2846    0.3788       376\n",
            "           1     0.7563    0.8285    0.7908       764\n",
            "           2     0.6012    0.5778    0.5892      1080\n",
            "           3     0.7613    0.4259    0.5462       749\n",
            "           4     0.5668    0.7423    0.6428       520\n",
            "           5     0.5792    0.7347    0.6477      1210\n",
            "\n",
            "    accuracy                         0.6295      4699\n",
            "   macro avg     0.6385    0.5990    0.5993      4699\n",
            "weighted avg     0.6396    0.6295    0.6193      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4758    0.4097    0.4403       144\n",
            "           1     0.7055    0.7918    0.7462       245\n",
            "           2     0.5598    0.6823    0.6150       384\n",
            "           3     0.7185    0.5706    0.6361       170\n",
            "           4     0.6407    0.6321    0.6364       299\n",
            "           5     0.6135    0.5249    0.5658       381\n",
            "\n",
            "    accuracy                         0.6168      1623\n",
            "   macro avg     0.6190    0.6019    0.6066      1623\n",
            "weighted avg     0.6185    0.6168    0.6139      1623\n",
            "\n",
            "0.9691118920842806 0.6192921290300635 0.47563431860137 0.6138892575524046\n",
            "Patience counter: 0\n",
            "Epoch: 6, iter 0: loss = 0.6887518167495728\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 6 loss average: 0.838\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6260    0.6011    0.6133       376\n",
            "           1     0.8142    0.8259    0.8200       764\n",
            "           2     0.6420    0.6657    0.6536      1080\n",
            "           3     0.7345    0.5394    0.6220       749\n",
            "           4     0.6545    0.6885    0.6710       520\n",
            "           5     0.6308    0.7017    0.6643      1210\n",
            "\n",
            "    accuracy                         0.6782      4699\n",
            "   macro avg     0.6837    0.6704    0.6741      4699\n",
            "weighted avg     0.6819    0.6782    0.6771      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6842    0.1806    0.2857       144\n",
            "           1     0.7386    0.7265    0.7325       245\n",
            "           2     0.4785    0.7526    0.5850       384\n",
            "           3     0.5936    0.6529    0.6218       170\n",
            "           4     0.6218    0.6488    0.6350       299\n",
            "           5     0.6058    0.3832    0.4695       381\n",
            "\n",
            "    accuracy                         0.5816      1623\n",
            "   macro avg     0.6204    0.5574    0.5549      1623\n",
            "weighted avg     0.6043    0.5816    0.5667      1623\n",
            "\n",
            "0.8384179063141346 0.6770961613226991 0.46617222140329617 0.5666691395394725\n",
            "Patience counter: 1\n",
            "Epoch: 7, iter 0: loss = 0.9768126010894775\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 7 loss average: 0.746\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7034    0.4415    0.5425       376\n",
            "           1     0.8406    0.8626    0.8514       764\n",
            "           2     0.7198    0.6852    0.7021      1080\n",
            "           3     0.7571    0.6409    0.6941       749\n",
            "           4     0.6223    0.8173    0.7066       520\n",
            "           5     0.6874    0.7579    0.7209      1210\n",
            "\n",
            "    accuracy                         0.7208      4699\n",
            "   macro avg     0.7218    0.7009    0.7029      4699\n",
            "weighted avg     0.7249    0.7208    0.7177      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5844    0.3125    0.4072       144\n",
            "           1     0.8133    0.5510    0.6569       245\n",
            "           2     0.4599    0.6875    0.5511       384\n",
            "           3     0.6554    0.5706    0.6101       170\n",
            "           4     0.7013    0.5418    0.6113       299\n",
            "           5     0.5480    0.6142    0.5792       381\n",
            "\n",
            "    accuracy                         0.5773      1623\n",
            "   macro avg     0.6271    0.5463    0.5693      1623\n",
            "weighted avg     0.6099    0.5773    0.5782      1623\n",
            "\n",
            "0.7460406987617413 0.7176729945389777 0.5053582521436155 0.5781923431500773\n",
            "Patience counter: 2\n",
            "Epoch: 8, iter 0: loss = 0.7428725361824036\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 8 loss average: 0.614\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7203    0.6782    0.6986       376\n",
            "           1     0.8721    0.8743    0.8732       764\n",
            "           2     0.7669    0.7676    0.7672      1080\n",
            "           3     0.8043    0.7023    0.7498       749\n",
            "           4     0.7509    0.8115    0.7800       520\n",
            "           5     0.7543    0.7992    0.7761      1210\n",
            "\n",
            "    accuracy                         0.7804      4699\n",
            "   macro avg     0.7781    0.7722    0.7742      4699\n",
            "weighted avg     0.7812    0.7804    0.7799      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5584    0.2986    0.3891       144\n",
            "           1     0.7733    0.5429    0.6379       245\n",
            "           2     0.4529    0.7135    0.5541       384\n",
            "           3     0.5736    0.6647    0.6158       170\n",
            "           4     0.6420    0.5518    0.5935       299\n",
            "           5     0.6000    0.4961    0.5431       381\n",
            "\n",
            "    accuracy                         0.5650      1623\n",
            "   macro avg     0.6000    0.5446    0.5556      1623\n",
            "weighted avg     0.5926    0.5650    0.5633      1623\n",
            "\n",
            "0.6137100936224064 0.7798946297161522 0.4847706243569055 0.5632561683670759\n",
            "Patience counter: 3\n",
            "Epoch: 9, iter 0: loss = 0.5564800500869751\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 9 loss average: 0.517\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7805    0.6809    0.7273       376\n",
            "           1     0.8824    0.8940    0.8882       764\n",
            "           2     0.8127    0.8278    0.8202      1080\n",
            "           3     0.8336    0.7623    0.7964       749\n",
            "           4     0.7676    0.8385    0.8015       520\n",
            "           5     0.7958    0.8182    0.8068      1210\n",
            "\n",
            "    accuracy                         0.8151      4699\n",
            "   macro avg     0.8121    0.8036    0.8067      4699\n",
            "weighted avg     0.8155    0.8151    0.8145      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4958    0.4097    0.4487       144\n",
            "           1     0.5569    0.7592    0.6425       245\n",
            "           2     0.5374    0.4870    0.5109       384\n",
            "           3     0.4471    0.7706    0.5659       170\n",
            "           4     0.6040    0.5050    0.5501       299\n",
            "           5     0.6093    0.4462    0.5152       381\n",
            "\n",
            "    accuracy                         0.5447      1623\n",
            "   macro avg     0.5417    0.5629    0.5389      1623\n",
            "weighted avg     0.5563    0.5447    0.5392      1623\n",
            "\n",
            "0.5169215512772402 0.8145018437444438 0.4643441350092089 0.5392255548129676\n",
            "Patience counter: 4\n",
            "Epoch: 10, iter 0: loss = 0.49136456847190857\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 10 loss average: 0.460\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7507    0.7287    0.7395       376\n",
            "           1     0.8972    0.9136    0.9053       764\n",
            "           2     0.8364    0.8333    0.8349      1080\n",
            "           3     0.8214    0.8104    0.8159       749\n",
            "           4     0.8119    0.8135    0.8127       520\n",
            "           5     0.8238    0.8306    0.8272      1210\n",
            "\n",
            "    accuracy                         0.8315      4699\n",
            "   macro avg     0.8236    0.8217    0.8226      4699\n",
            "weighted avg     0.8311    0.8315    0.8312      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5567    0.3750    0.4481       144\n",
            "           1     0.5856    0.6980    0.6369       245\n",
            "           2     0.5301    0.4818    0.5048       384\n",
            "           3     0.5957    0.6588    0.6257       170\n",
            "           4     0.6726    0.5084    0.5790       299\n",
            "           5     0.5329    0.6588    0.5892       381\n",
            "\n",
            "    accuracy                         0.5699      1623\n",
            "   macro avg     0.5789    0.5635    0.5640      1623\n",
            "weighted avg     0.5746    0.5699    0.5659      1623\n",
            "\n",
            "0.4595071052511533 0.8312273184423198 0.486128694852605 0.5658583298006353\n",
            "Patience counter: 5\n",
            "Epoch: 11, iter 0: loss = 0.5932343006134033\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 11 loss average: 0.403\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7956    0.7766    0.7860       376\n",
            "           1     0.9270    0.9306    0.9288       764\n",
            "           2     0.8655    0.8639    0.8647      1080\n",
            "           3     0.8589    0.8531    0.8560       749\n",
            "           4     0.8352    0.8577    0.8463       520\n",
            "           5     0.8635    0.8628    0.8632      1210\n",
            "\n",
            "    accuracy                         0.8651      4699\n",
            "   macro avg     0.8576    0.8575    0.8575      4699\n",
            "weighted avg     0.8650    0.8651    0.8650      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5660    0.2083    0.3046       144\n",
            "           1     0.6898    0.6082    0.6464       245\n",
            "           2     0.4770    0.6484    0.5497       384\n",
            "           3     0.6093    0.5412    0.5732       170\n",
            "           4     0.6714    0.4716    0.5540       299\n",
            "           5     0.5541    0.6850    0.6127       381\n",
            "\n",
            "    accuracy                         0.5681      1623\n",
            "   macro avg     0.5946    0.5271    0.5401      1623\n",
            "weighted avg     0.5848    0.5681    0.5606      1623\n",
            "\n",
            "0.40332790339986485 0.8650044617615433 0.4803437151024495 0.5605872470450398\n",
            "Patience counter: 6\n",
            "Epoch: 12, iter 0: loss = 0.25047990679740906\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 12 loss average: 0.352\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7968    0.8032    0.8000       376\n",
            "           1     0.9393    0.9319    0.9356       764\n",
            "           2     0.8777    0.8972    0.8874      1080\n",
            "           3     0.8691    0.8598    0.8644       749\n",
            "           4     0.8718    0.8635    0.8676       520\n",
            "           5     0.8719    0.8661    0.8690      1210\n",
            "\n",
            "    accuracy                         0.8776      4699\n",
            "   macro avg     0.8711    0.8703    0.8707      4699\n",
            "weighted avg     0.8777    0.8776    0.8776      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6596    0.2153    0.3246       144\n",
            "           1     0.7243    0.5469    0.6233       245\n",
            "           2     0.4523    0.5677    0.5035       384\n",
            "           3     0.5280    0.6647    0.5885       170\n",
            "           4     0.6767    0.5251    0.5913       299\n",
            "           5     0.5054    0.6142    0.5545       381\n",
            "\n",
            "    accuracy                         0.5465      1623\n",
            "   macro avg     0.5911    0.5223    0.5310      1623\n",
            "weighted avg     0.5735    0.5465    0.5428      1623\n",
            "\n",
            "0.3515200507827103 0.8776465706060624 0.45774149487232735 0.5427594965977718\n",
            "Patience counter: 7\n",
            "Epoch: 13, iter 0: loss = 0.3464895188808441\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 13 loss average: 0.303\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8538    0.7766    0.8134       376\n",
            "           1     0.9355    0.9306    0.9331       764\n",
            "           2     0.9064    0.9056    0.9060      1080\n",
            "           3     0.8960    0.8745    0.8851       749\n",
            "           4     0.8477    0.9096    0.8776       520\n",
            "           5     0.8902    0.9041    0.8971      1210\n",
            "\n",
            "    accuracy                         0.8944      4699\n",
            "   macro avg     0.8883    0.8835    0.8854      4699\n",
            "weighted avg     0.8946    0.8944    0.8942      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4576    0.3750    0.4122       144\n",
            "           1     0.7245    0.5796    0.6440       245\n",
            "           2     0.4835    0.5339    0.5074       384\n",
            "           3     0.5450    0.6412    0.5892       170\n",
            "           4     0.7297    0.4515    0.5579       299\n",
            "           5     0.4940    0.6483    0.5607       381\n",
            "\n",
            "    accuracy                         0.5496      1623\n",
            "   macro avg     0.5724    0.5382    0.5452      1623\n",
            "weighted avg     0.5718    0.5496    0.5500      1623\n",
            "\n",
            "0.3034050092101097 0.8942151157441247 0.4638483391810161 0.5499596431090766\n",
            "Patience counter: 8\n",
            "Epoch: 14, iter 0: loss = 0.21151793003082275\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.56it/s]\n",
            "Epoch 14 loss average: 0.273\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8404    0.8404    0.8404       376\n",
            "           1     0.9461    0.9411    0.9436       764\n",
            "           2     0.9140    0.9056    0.9098      1080\n",
            "           3     0.8954    0.9025    0.8989       749\n",
            "           4     0.8915    0.8846    0.8880       520\n",
            "           5     0.8985    0.9074    0.9030      1210\n",
            "\n",
            "    accuracy                         0.9038      4699\n",
            "   macro avg     0.8976    0.8969    0.8973      4699\n",
            "weighted avg     0.9039    0.9038    0.9038      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.5600    0.1944    0.2887       144\n",
            "           1     0.6858    0.6327    0.6582       245\n",
            "           2     0.4805    0.6406    0.5491       384\n",
            "           3     0.5519    0.5941    0.5722       170\n",
            "           4     0.5719    0.5585    0.5651       299\n",
            "           5     0.5750    0.5433    0.5587       381\n",
            "\n",
            "    accuracy                         0.5570      1623\n",
            "   macro avg     0.5709    0.5273    0.5320      1623\n",
            "weighted avg     0.5650    0.5570    0.5501      1623\n",
            "\n",
            "0.27347832747424644 0.9038300630824521 0.4808960227386232 0.5500934445454424\n",
            "Patience counter: 9\n",
            "Epoch: 15, iter 0: loss = 0.1668957620859146\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 15 loss average: 0.246\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8447    0.8245    0.8345       376\n",
            "           1     0.9506    0.9568    0.9537       764\n",
            "           2     0.9153    0.9111    0.9132      1080\n",
            "           3     0.9170    0.9146    0.9158       749\n",
            "           4     0.8698    0.8865    0.8781       520\n",
            "           5     0.9174    0.9182    0.9178      1210\n",
            "\n",
            "    accuracy                         0.9113      4699\n",
            "   macro avg     0.9025    0.9019    0.9022      4699\n",
            "weighted avg     0.9112    0.9113    0.9112      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.4690    0.4722    0.4706       144\n",
            "           1     0.5903    0.7469    0.6595       245\n",
            "           2     0.5161    0.5000    0.5079       384\n",
            "           3     0.5744    0.6588    0.6137       170\n",
            "           4     0.6321    0.4482    0.5245       299\n",
            "           5     0.5656    0.5774    0.5714       381\n",
            "\n",
            "    accuracy                         0.5601      1623\n",
            "   macro avg     0.5579    0.5673    0.5579      1623\n",
            "weighted avg     0.5622    0.5601    0.5565      1623\n",
            "\n",
            "0.24642952267701426 0.9111981897684426 0.47156048095162584 0.5565231271439818\n",
            "Patience counter: 10\n",
            "Epoch: 16, iter 0: loss = 0.2294023633003235\n",
            "100%|███████████████████████████████████████████| 48/48 [00:30<00:00,  1.57it/s]\n",
            "Epoch 16 loss average: 0.213\n",
            "train\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8632    0.8723    0.8677       376\n",
            "           1     0.9536    0.9686    0.9610       764\n",
            "           2     0.9233    0.9250    0.9241      1080\n",
            "           3     0.9170    0.9439    0.9303       749\n",
            "           4     0.9167    0.8885    0.9023       520\n",
            "           5     0.9410    0.9223    0.9316      1210\n",
            "\n",
            "    accuracy                         0.9262      4699\n",
            "   macro avg     0.9191    0.9201    0.9195      4699\n",
            "weighted avg     0.9262    0.9262    0.9261      4699\n",
            "\n",
            "test\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.3961    0.4236    0.4094       144\n",
            "           1     0.6639    0.6531    0.6584       245\n",
            "           2     0.5176    0.4219    0.4648       384\n",
            "           3     0.5086    0.6941    0.5871       170\n",
            "           4     0.4953    0.5251    0.5097       299\n",
            "           5     0.5383    0.5171    0.5274       381\n",
            "\n",
            "    accuracy                         0.5268      1623\n",
            "   macro avg     0.5200    0.5391    0.5262      1623\n",
            "weighted avg     0.5287    0.5268    0.5249      1623\n",
            "\n",
            "0.21322651308340332 0.9260988845504157 0.44958914031672254 0.5249175604577585\n",
            "Patience counter: 11\n",
            "Done! It took 6.6e+02 secs\n",
            "\n",
            "Current RUN: 5\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.028525572270155\n",
            "Best test f1 weighted\n",
            "0.6138892575524046\n",
            "Best epoch\n",
            "5\n",
            "\n",
            "\n",
            "Average across runs:\n",
            "Best epoch\n",
            "[5, 6, 8, 6, 5]\n",
            "\n",
            "\n",
            "Best test loss\n",
            "1.0362399436533452\n",
            "Overall test f1 weighted\n",
            "[0.58136292 0.5650096  0.59419369 0.54633713 0.61388926]\n",
            "Best test f1 weighted\n",
            "0.5801585187113087\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}